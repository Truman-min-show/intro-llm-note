{"cells":[{"cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"16BEDEF83DF24B65ABB0E52615CECDCB","scrolled":false,"notebookId":"68b0fffdb2fa13cb0f9ce08a"},"source":"'''1.2.3 预训练语言模型实践'''\n\nimport os\nos.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\nimport json\nfrom datasets import concatenate_datasets, load_dataset\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizerFast, BertConfig, BertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, pipeline\n\nfrom itertools import chain\n\n\nbookcorpus = load_dataset(\"bookcorpus\", \"plain_text\", split=\"train\")\nwiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n\n# 仅保留 'text' 列\nwiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])\n\ndataset = concatenate_datasets([bookcorpus, wiki])\n\n# 将数据集切分为 90% 训练集，10% 测试集\nd = dataset.train_test_split(test_size=0.1)\n\ndef dataset_to_text(dataset, output_filename=\"data.txt\"):\n    \"\"\"将数据集文本保存到磁盘的通用函数\"\"\"\n    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n        for t in dataset[\"text\"]:\n            print(t, file=f)\n\n# 将训练集保存为 train.txt\ndataset_to_text(d[\"train\"], \"train.txt\")\n\n# 将测试集保存为 test.txt\ndataset_to_text(d[\"test\"], \"test.txt\")\n\n\nspecial_tokens = [\n  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n]\n# 如果根据训练和测试两个集合训练词元分析器，则需要修改files\n# files = [\"train.txt\", \"test.txt\"]\n# 仅根据训练集合训练词元分析器\nfiles = [\"train.txt\"]\n# BERT中采用的默认词表大小为30522，可以随意修改\nvocab_size = 30_522\n# 最大序列长度，该值越小，训练速度越快\nmax_length = 512\n# 是否将长样本截断\ntruncate_longer_samples = False\n\n# 初始化WordPiece词元分析器\ntokenizer = BertWordPieceTokenizer()\n# 训练词元分析器\ntokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n# 允许截断达到最大512个词元\ntokenizer.enable_truncation(max_length=max_length)\n\nmodel_path = \"/home/mw/input/pretrained_bert50375037\"\n\n# 如果文件夹不存在，则先创建文件夹\nif not os.path.isdir(model_path):\n  os.mkdir(model_path)\n# 保存词元分析器模型\ntokenizer.save_model(model_path)\n# 将一些词元分析器中的配置保存到配置文件，包括特殊词元、转换为小写、最大序列长度等\nwith open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n  tokenizer_cfg = {\n      \"do_lower_case\": True,\n      \"unk_token\": \"[UNK]\",\n      \"sep_token\": \"[SEP]\",\n      \"pad_token\": \"[PAD]\",\n      \"cls_token\": \"[CLS]\",\n      \"mask_token\": \"[MASK]\",\n      \"model_max_length\": max_length,\n      \"max_len\": max_length,\n  }\n  json.dump(tokenizer_cfg, f)\n\n# 当词元分析器进行训练和配置时，将其装载到BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained(model_path)\n\ndef encode_with_truncation(examples):\n    \"\"\"使用词元分析对句子进行处理并截断的映射函数（Mapping function）\"\"\"\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n                     max_length=max_length, return_special_tokens_mask=True)\n\ndef encode_without_truncation(examples):\n    \"\"\"使用词元分析对句子进行处理且不截断的映射函数（Mapping function）\"\"\"\n    return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n\n# 编码函数将依赖于 truncate_longer_samples 变量\nencode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n\n# 对训练数据集进行分词处理\ntrain_dataset = d[\"train\"].map(encode, batched=True)\n# 对测试数据集进行分词处理\ntest_dataset = d[\"test\"].map(encode, batched=True)\n\nif truncate_longer_samples:\n    # 移除其他列，将 input_ids 和 attention_mask 设置为 PyTorch 张量\n    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\nelse:\n    # 移除其他列，将它们保留为 Python 列表\n    test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n    train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n\n\n# 主要数据处理函数，拼接数据集中的所有文本并生成最大序列长度的块\ndef group_texts(examples):\n    # 拼接所有文本\n    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # 舍弃了剩余部分，如果模型支持填充而不是舍弃，则可以根据需要自定义这部分\n    if total_length >= max_length:\n        total_length = (total_length // max_length) * max_length\n    # 按照最大长度分割成块\n    result = {\n        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n        for k, t in concatenated_examples.items()\n    }\n    return result\n\n# 请注意，使用 batched=True，此映射一次处理 1000 个文本\n# 因此，group_texts 会为这 1000 个文本组抛弃不足的部分\n# 可以在这里调整 batch_size，但较高的值可能会使预处理速度变慢\n#\n# 为了加速这一部分，使用了多进程处理\nif not truncate_longer_samples:\n    train_dataset = train_dataset.map(group_texts, batched=True,\n                                      desc=f\"Grouping texts in chunks of {max_length}\")\n    test_dataset = test_dataset.map(group_texts, batched=True,\n                                    desc=f\"Grouping texts in chunks of {max_length}\")\n    # 将它们从列表转换为 PyTorch 张量\n    train_dataset.set_format(\"torch\")\n    test_dataset.set_format(\"torch\")\n\n# 使用配置文件初始化模型\nmodel_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\nmodel = BertForMaskedLM(config=model_config)\n\n# 初始化数据整理器，随机屏蔽 20%（默认为 15%）的标记\n# 用于掩盖语言建模（MLM）任务\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n)\n\ntraining_args = TrainingArguments(\n    output_dir=model_path,          # 输出目录，用于保存模型检查点\n    evaluation_strategy=\"steps\",    # 每隔 `logging_steps` 步进行一次评估\n    overwrite_output_dir=True,\n    num_train_epochs=10,            # 训练时的轮数，可以根据需要进行调整\n    per_device_train_batch_size=10, # 训练批量大小，可以根据 GPU 内存容量将其设置得尽可能大\n    gradient_accumulation_steps=8,  # 在更新权重之前累积梯度\n    per_device_eval_batch_size=64,  # 评估批量大小\n    logging_steps=1000,             # 每隔 1000 步进行一次评估，记录并保存模型检查点\n    save_steps=1000,\n    # load_best_model_at_end=True,  # 是否在训练结束时加载最佳模型（根据损失）\n    # save_total_limit=3,           # 如果磁盘空间有限，则可以限制只保存 3 个模型权重\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n\n# 训练模型\ntrainer.train()\n\n# 加载模型检查点\nmodel = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-10000\"))\n# 加载词元分析器\ntokenizer = BertTokenizerFast.from_pretrained(model_path)\n\nfill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n\n# 进行预测\nexamples = [\n  \"Today's most trending hashtags on [MASK] is Donald Trump\",\n  \"The [MASK] was cloudy yesterday, but today it's rainy.\",\n]\nfor example in examples:\n  for prediction in fill_mask(example):\n    print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n  print(\"=\"*50)\n","outputs":[{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"},{"output_type":"stream","name":"stream","text":"The repository for bookcorpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bookcorpus.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N] : y"},{"output_type":"stream","name":"stderr","text":"Downloading data: 100%|██████████| 1.18G/1.18G [00:45<00:00, 26.0MB/s] \nGenerating train split: 100%|██████████| 74004228/74004228 [09:32<00:00, 129235.69 examples/s]\n"},{"output_type":"stream","name":"stream","text":"The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N] : y"},{"output_type":"stream","name":"stderr","text":"Downloading data:   2%|▏         | 1/41 [01:38<1:05:34, 98.36s/files]\nError while downloading from https://cas-bridge.xethub.hf-mirror.com/xet-bridge-us/621ffdd236468d709f18200b/2420a09156f55d8d8a6d9d3ec51559d4e6591abee0179a47768e7ba81c619725?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250829%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250829T014957Z&X-Amz-Expires=3600&X-Amz-Signature=b001dd1f42615a147cba24e383f9b7bfb4169b512e009a94c720dcf00d6dcedf&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27train-00001-of-00041.parquet%3B+filename%3D%22train-00001-of-00041.parquet%22%3B&x-id=GetObject&Expires=1756435797&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NjQzNTc5N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRkMjM2NDY4ZDcwOWYxODIwMGIvMjQyMGEwOTE1NmY1NWQ4ZDhhNmQ5ZDNlYzUxNTU5ZDRlNjU5MWFiZWUwMTc5YTQ3NzY4ZTdiYTgxYzYxOTcyNSoifV19&Signature=GxmISwEGD5afU7sxegFQPKeJnHGXBBWHTWqHwkpyeQmXw6gaEoj-aVfoeM8G5rI1i-h0yD4PeXltsujyCtb3Av6IkYPth%7E3xSHvoEI9u3P%7EzcLp40LWlktKJsdcb2ZiYGDz7GXIUODpDX3yZcldOQfssksKI3tYThSisEVbs%7EWQzIsuLky6d9e8hZpNW01ySkv47fpd8XL7OH3Ry2BaJCvBYZiat2TXCunsXN%7EqV7wjUZf27SU8nguBDF-p95SXiSXynwxcXnY7BQ6OHA3FgZOZUmojRYzeuFj9OEECH-u7nxpBsZf6QyzxfraPkw%7E4FgsCFgzh6dS7cUNDY0e2D0w__&Key-Pair-Id=K2L8F4GPSG1IFC: HTTPSConnectionPool(host='cas-bridge.xethub.hf-mirror.com', port=443): Read timed out.\nTrying to resume download...\n"}],"execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}