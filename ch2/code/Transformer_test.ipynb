{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "id": "822CA6AD42A34B57BF1FA07CD0BA602A",
    "jupyter": {},
    "notebookId": "68b069ddb2fa13cb0f8bd45f",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/papermill-2.3.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\n",
      "Collecting spacy\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/0a/0a/bb90e9aa0b3c527876627567d82517aabab08006ccf63796c33b0242254d/spacy-3.8.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/c3/55/12e842c70ff8828e34e543a2c7176dac4da006ca6901c9e8b43efab8bc6b/spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/e8/09/0e7afce0a422692506c85474a26fb3a03c1971b2b5f7e7745276c4b3de7f/murmurhash-1.0.13-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/48/cb/2207679e4b92701f78cf141e1ab4f81f55247dbe154eb426b842a0a993de/cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (218 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.9/218.9 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/1c/7d/ff19f74d15ee587905bafa3582883cfe2f72b574e6d691ee64dc690dc276/preshed-3.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (842 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m842.9/842.9 kB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/ae/7e/6b1bb6eba9c25a5911e1624c0da33ca007d7697b41ee11c059f3d23d8bbc/thinc-8.3.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/06/7c/34330a89da55610daa5f245ddce5aab81244321101614751e7537f125133/wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/35/a3/9eda9997a8bd011caed18fdaa5ce606714eb06d8dab587ed0522b3e92ab1/srsly-2.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/2a/87/abd57374044e1f627f0a905ac33c1a7daab35a3a815abfea4e1bafd3fdb1/weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.11/site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (24.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/c3/6b/068c2ea7a712bf805c62445bd9e9c06d7340358ef2824150eceac027444b/langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/5d/e9/5a5ffd9b286db82be70d677d0a91e4d58f7912bb8dd026ddeeb4abe70679/language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/da/77/6fbd4d9b923f3914c589d38a19dfc8fd45f54296aef75aba908a7d176871/blis-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/0c/00/3106b1854b45bd0474ced037dfe6b73b90fe68a68968cef47c23de3d43d2/confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/c8/b0/fbeee3000a51ebf7222016e2939b5c5ecf8000a19555d04a18f1e02521b8/numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/40/e7/6fea57b887f8e367c1e4a496ba03bfaf57824b766f777723ce1faf28834b/cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/36/89/c4eeefb956318047036e6bdc572b6112b2059d595e85961267a90aa40458/marisa_trie-1.3.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m127.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.17.2)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, numpy, murmurhash, marisa-trie, cloudpathlib, catalogue, srsly, preshed, language-data, blis, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.0\n",
      "    Uninstalling numpy-1.25.0:\n",
      "      Successfully uninstalled numpy-1.25.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "vllm 0.7.0 requires numpy<2.0.0, but you have numpy 2.3.2 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 murmurhash-1.0.13 numpy-2.3.2 preshed-3.0.10 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 wasabi-1.1.3 weasel-0.4.1\n",
      "\u001b[33mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/papermill-2.3.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\n",
      "Processing /home/mw/input/transformer50805080/en_core_web_sm-any-py3-none-any.whl\n",
      "Collecting spacy<3.8.0,>=3.7.2 (from en-core-web-sm==any)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/e0/ce/b5e6b02165881547ad251b0b172ebf496b9181a95833f94012af82d044df/spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (3.0.10)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/1a/19/cd73e3b5f22d5d9399f6f2931ab0fb985415f34030dcfead070181866761/thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.2/920.2 kB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.3.2)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==any)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/dc/23/eb01450dc284a7ea8ebc0e5296f1f8fdbe5299169f4c318f836b4284a119/blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy<3.8.0,>=3.7.2->en-core-web-sm==any)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/3a/d0/edc009c27b406c4f9cbc79274d6e46d634d139075492ad055e3d68445925/numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (2.17.2)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==any) (0.1.2)\n",
      "Installing collected packages: numpy, blis, thinc, spacy, en-core-web-sm\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.2\n",
      "    Uninstalling numpy-2.3.2:\n",
      "      Successfully uninstalled numpy-2.3.2\n",
      "  Attempting uninstall: blis\n",
      "    Found existing installation: blis 1.3.0\n",
      "    Uninstalling blis-1.3.0:\n",
      "      Successfully uninstalled blis-1.3.0\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.3.6\n",
      "    Uninstalling thinc-8.3.6:\n",
      "      Successfully uninstalled thinc-8.3.6\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.8.7\n",
      "    Uninstalling spacy-3.8.7:\n",
      "      Successfully uninstalled spacy-3.8.7\n",
      "Successfully installed blis-0.7.11 en-core-web-sm-3.7.1 numpy-1.26.4 spacy-3.7.5 thinc-8.2.5\n",
      "\u001b[33mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/papermill-2.3.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\n",
      "Processing /home/mw/input/transformer50805080/fr_core_news_sm-any-py3-none-any.whl\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /opt/conda/lib/python3.11/site-packages (from fr-core-news-sm==any) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (2.17.2)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==any) (0.1.2)\n",
      "Installing collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-3.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install  spacy  -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\n",
    "!pip install /home/mw/input/transformer50805080/en_core_web_sm-any-py3-none-any.whl -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\n",
    "!pip install /home/mw/input/transformer50805080/fr_core_news_sm-any-py3-none-any.whl -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "id": "62D1BA24BA8A44EAB94310458BF973EF",
    "jupyter": {},
    "notebookId": "68b069ddb2fa13cb0f8bd45f",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 外部库导入 ---\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "id": "020FC6F8506548D28E6978269E965E42",
    "jupyter": {},
    "notebookId": "68b069ddb2fa13cb0f8bd45f",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 新增单元格：自定义词汇表 ===\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, tokenizer, min_freq=1):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.min_freq = min_freq\n",
    "        # 初始化词汇表时，加入特殊标记\n",
    "        self.specials = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "        self.stoi = {token: i for i, token in enumerate(self.specials)}\n",
    "        self.itos = {i: token for i, token in enumerate(self.specials)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def build_vocab(self, sentence_list):\n",
    "        print(\"Building vocabulary...\")\n",
    "        counter = Counter()\n",
    "        # 使用tqdm显示进度条\n",
    "        for sentence in tqdm(sentence_list):\n",
    "            counter.update(self.tokenizer(sentence))\n",
    "        \n",
    "        # 过滤掉低频词\n",
    "        for word, count in counter.items():\n",
    "            if count >= self.min_freq and word not in self.stoi:\n",
    "                idx = len(self.stoi)\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "        print(\"Vocabulary built.\")\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        # 对于不在词汇表中的词，使用 <unk> 的索引\n",
    "        unk_idx = self.stoi['<unk>']\n",
    "        return [self.stoi.get(token, unk_idx) for token in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "id": "390C767995DD4A9FAAB2E2180942D868",
    "jupyter": {},
    "notebookId": "68b069ddb2fa13cb0f8bd45f",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 内容来自: sublayers.py ===\n",
    "\n",
    "'''1.1.2 注意力层'''\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        # 掩盖那些为了补全长度而增加的单元，使其通过Softmax计算后为0\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)\n",
    "        # 利用线性计算划分成h个头\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        # 矩阵转置\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        # 计算attention\n",
    "        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        # 连接多个头并输入最后的线性层\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "        return output\n",
    "'''1.1.3 前馈层'''\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # d_ff 默认设为 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "'''1.1.4 残差连接与层归一化'''\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.size = d_model\n",
    "        # 层归一化包含两个可学习参数\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps  # 避免除零\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        norm = self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "id": "4A6B7EA731014F6C9A9613B572BF81BD",
    "jupyter": {},
    "notebookId": "68b069ddb2fa13cb0f8bd45f",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 内容来自: layers.py ===\n",
    "\n",
    "'''1.1.5 编码器和解码器结构'''\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.attn(x, x, x, mask)\n",
    "        attn_output = self.dropout_1(attn_output)\n",
    "        x = x + attn_output\n",
    "        x = self.norm_1(x)\n",
    "        ff_output = self.ff(x)\n",
    "        ff_output = self.dropout_2(ff_output)\n",
    "        x = x + ff_output\n",
    "        x = self.norm_2(x)\n",
    "        return x\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        attn_output_1 = self.attn_1(x, x, x, trg_mask)\n",
    "        attn_output_1 = self.dropout_1(attn_output_1)\n",
    "        x = x + attn_output_1\n",
    "        x = self.norm_1(x)\n",
    "        attn_output_2 = self.attn_2(x, e_outputs, e_outputs, src_mask)\n",
    "        attn_output_2 = self.dropout_2(attn_output_2)\n",
    "        x = x + attn_output_2\n",
    "        x = self.norm_2(x)\n",
    "        ff_output = self.ff(x)\n",
    "        ff_output = self.dropout_3(ff_output)\n",
    "        x = x + ff_output\n",
    "        x = self.norm_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "id": "532C08ACCC4A4D05B091CE3D39EBB0B3",
    "jupyter": {},
    "notebookId": "68b069ddb2fa13cb0f8bd45f",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 内容来自: embed.py ===\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "'''1.1.1 嵌入表示层'''\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=80, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 根据pos和i创建一个常量PE矩阵\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "                if i + 1 < d_model:\n",
    "                    pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        # 使得单词嵌入表示相对大一些\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        # 增加位置常量到单词嵌入表示中\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:, :seq_len, :], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "id": "8AEF54A088EA4DED892E0265EEEFFF3A",
    "jupyter": {},
    "notebookId": "68b069ddb2fa13cb0f8bd45f",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 内容来自: tokenizer.py ===\n",
    "\n",
    "class tokenize(object):\n",
    "    def __init__(self, lang):\n",
    "        self.nlp = spacy.load(lang)\n",
    "    def tokenizer(self, sentence):\n",
    "        sentence = re.sub(\n",
    "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
    "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
    "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
    "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
    "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
    "        sentence = sentence.lower()\n",
    "        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "06BB9F6C4D7C48E0B0B9129A863EBA0B",
    "jupyter": {},
    "notebookId": "68b069ddb2fa13cb0f8bd45f",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 内容来自: batch.py (已修正设备不匹配问题) ===\n",
    "\n",
    "def nopeak_mask(size):\n",
    "    np_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
    "    np_mask = Variable(torch.from_numpy(np_mask == 0))\n",
    "    return np_mask\n",
    "\n",
    "def create_masks(src, trg, src_pad, trg_pad):\n",
    "    src_mask = (src != src_pad).unsqueeze(-2)\n",
    "    \n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != trg_pad).unsqueeze(-2)\n",
    "        size = trg.size(1)\n",
    "        np_mask = nopeak_mask(size)\n",
    "        \n",
    "        # --- 核心修改在这里 ---\n",
    "        # np_mask 默认在 CPU 上创建，必须将它移动到与 trg 相同的设备上才能进行计算\n",
    "        # 我使用 trg.device 来确保设备总是正确的 (例如 'cuda:0')\n",
    "        if np_mask.device != trg.device:\n",
    "             np_mask = np_mask.to(trg.device)\n",
    "        \n",
    "        trg_mask = trg_mask & np_mask\n",
    "    else:\n",
    "        trg_mask = None\n",
    "        \n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "id": "F7F0AB86C47E4F99A6772ED1F9E98FBA",
    "jupyter": {},
    "notebookId": "68b069ddb2fa13cb0f8bd45f",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 内容来自: models.py ===\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "'''1.1.5 编码器和解码器结构'''\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "id": "49DDF3BF1F814D27A7512D207445640D",
    "jupyter": {},
    "notebookId": "68b069ddb2fa13cb0f8bd45f",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 内容来自: process.py (已重写，不依赖torchtext) ===\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def read_data(src_file, trg_file):\n",
    "    try:\n",
    "        src_data = open(src_file, encoding='utf-8').read().strip().split('\\n')\n",
    "        trg_data = open(trg_file, encoding='utf-8').read().strip().split('\\n')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. File not found.\")\n",
    "        quit()\n",
    "    return src_data, trg_data\n",
    "\n",
    "def prepare_data(src_file, trg_file, src_lang, trg_lang, max_strlen=80):\n",
    "    # 1. 读取数据\n",
    "    src_data, trg_data = read_data(src_file, trg_file)\n",
    "    \n",
    "    # 2. 过滤长句\n",
    "    print(f\"Original data size: {len(src_data)}\")\n",
    "    filtered_data = [(s, t) for s, t in zip(src_data, trg_data) if len(s.split()) < max_strlen and len(t.split()) < max_strlen]\n",
    "    src_data, trg_data = zip(*filtered_data)\n",
    "    print(f\"Filtered data size: {len(src_data)}\")\n",
    "\n",
    "    # 3. 初始化分词器和词汇表\n",
    "    src_tokenizer = tokenize(src_lang)\n",
    "    trg_tokenizer = tokenize(trg_lang)\n",
    "    \n",
    "    SRC_VOCAB = Vocabulary(src_tokenizer.tokenizer)\n",
    "    TRG_VOCAB = Vocabulary(trg_tokenizer.tokenizer)\n",
    "    \n",
    "    SRC_VOCAB.build_vocab(src_data)\n",
    "    TRG_VOCAB.build_vocab(trg_data)\n",
    "\n",
    "    # 4. 数值化数据\n",
    "    print(\"Numericalizing data...\")\n",
    "    src_numerical = [SRC_VOCAB.numericalize(s) for s in tqdm(src_data)]\n",
    "    trg_numerical = [TRG_VOCAB.numericalize(t) for t in tqdm(trg_data)]\n",
    "\n",
    "    return src_numerical, trg_numerical, SRC_VOCAB, TRG_VOCAB\n",
    "\n",
    "def data_generator(src_numerical, trg_numerical, batch_size, src_pad_idx, trg_pad_idx):\n",
    "    # 将数据打包成 (源, 目标) 对并打乱\n",
    "    data_pairs = list(zip(src_numerical, trg_numerical))\n",
    "    random.shuffle(data_pairs)\n",
    "    \n",
    "    num_batches = len(data_pairs) // batch_size\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        batch_pairs = data_pairs[i * batch_size : (i + 1) * batch_size]\n",
    "        \n",
    "        # 按长度排序有助于减少填充\n",
    "        batch_pairs.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "        \n",
    "        src_list, trg_list_raw = zip(*batch_pairs)\n",
    "\n",
    "        # 为目标序列添加 <sos> 和 <eos>\n",
    "        sos_idx = TRG_VOCAB.stoi['<sos>']\n",
    "        eos_idx = TRG_VOCAB.stoi['<eos>']\n",
    "        trg_list = [[sos_idx] + trg + [eos_idx] for trg in trg_list_raw]\n",
    "        \n",
    "        # --- 手动填充 ---\n",
    "        max_src_len = max(len(s) for s in src_list)\n",
    "        max_trg_len = max(len(t) for t in trg_list)\n",
    "        \n",
    "        src_padded = [s + [src_pad_idx] * (max_src_len - len(s)) for s in src_list]\n",
    "        trg_padded = [t + [trg_pad_idx] * (max_trg_len - len(t)) for t in trg_list]\n",
    "        \n",
    "        # 转换为Tensor\n",
    "        src_tensor = torch.LongTensor(src_padded)\n",
    "        trg_tensor = torch.LongTensor(trg_padded)\n",
    "        \n",
    "        yield src_tensor, trg_tensor\n",
    "\n",
    "# 这个函数现在只用于翻译阶段\n",
    "def tokenize_en_sentence(src_sentence, SRC_VOCAB):\n",
    "    spacy_en = tokenize('en_core_web_sm')\n",
    "    src_tokens = spacy_en.tokenizer(src_sentence)\n",
    "    return [SRC_VOCAB.stoi.get(tok, SRC_VOCAB.stoi['<unk>']) for tok in src_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "5AF684E4F3F34E4881AE8553EBED9218",
    "jupyter": {},
    "notebookId": "68b069ddb2fa13cb0f8bd45f",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在使用的计算设备 (Using device): cuda\n",
      "Original data size: 154883\n",
      "Filtered data size: 154883\n",
      "Building vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154883/154883 [00:02<00:00, 62002.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary built.\n",
      "Building vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154883/154883 [00:03<00:00, 40486.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary built.\n",
      "Numericalizing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154883/154883 [00:02<00:00, 74318.76it/s]\n",
      "100%|██████████| 154883/154883 [00:02<00:00, 59345.05it/s]\n",
      "Epoch 1/2: 100%|██████████| 4840/4840 [03:21<00:00, 23.97it/s, avg_loss=4.173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Total time: 3m 22s, Final Avg Loss: 4.173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 4840/4840 [03:20<00:00, 24.19it/s, avg_loss=3.143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Total time: 3m 20s, Final Avg Loss: 3.143\n",
      "\n",
      "==================== 开始翻译测试 ====================\n",
      "\n",
      "原始 (en): 'Let me see.'\n",
      "翻译 (fr): <sos> laissez moi le faire . <eos>\n",
      "--------------------------------------------------\n",
      "\n",
      "原始 (en): 'Hello, how are you?'\n",
      "翻译 (fr): <sos> qu' est ce que tu es en train de faire ? <eos>\n",
      "--------------------------------------------------\n",
      "\n",
      "原始 (en): 'This is a test.'\n",
      "翻译 (fr): <sos> c' est un bon problème . <eos>\n",
      "--------------------------------------------------\n",
      "\n",
      "原始 (en): 'I love to learn new things.'\n",
      "翻译 (fr): <sos> j' aime les choses . <eos>\n",
      "--------------------------------------------------\n",
      "\n",
      "原始 (en): 'What is your name?'\n",
      "翻译 (fr): <sos> quel est votre nom ? <eos>\n",
      "--------------------------------------------------\n",
      "\n",
      "原始 (en): 'The weather is nice today.'\n",
      "翻译 (fr): <sos> le temps est un problème . <eos>\n",
      "--------------------------------------------------\n",
      "\n",
      "原始 (en): 'Where is the library?'\n",
      "翻译 (fr): <sos> où est la maison ? <eos>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# === 内容来自: main.py (已重写，并支持GPU和更多测试语句) ===\n",
    "\n",
    "# 1. 定义设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"正在使用的计算设备 (Using device): {device}\")\n",
    "\n",
    "# 2. 数据和模型参数\n",
    "src_file = '/home/mw/input/transformer50805080/english.txt'\n",
    "trg_file = '/home/mw/input/transformer50805080/french.txt'\n",
    "src_lang = 'en_core_web_sm'\n",
    "trg_lang = 'fr_core_news_sm'\n",
    "max_strlen = 80\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "d_model = 512\n",
    "heads = 8\n",
    "N = 6\n",
    "dropout = 0.1\n",
    "\n",
    "# 3. 准备数据、词汇表\n",
    "src_numerical, trg_numerical, SRC_VOCAB, TRG_VOCAB = prepare_data(\n",
    "    src_file, trg_file, src_lang, trg_lang, max_strlen\n",
    ")\n",
    "src_pad_idx = SRC_VOCAB.stoi['<pad>']\n",
    "trg_pad_idx = TRG_VOCAB.stoi['<pad>']\n",
    "src_vocab_size = len(SRC_VOCAB)\n",
    "trg_vocab_size = len(TRG_VOCAB)\n",
    "\n",
    "# 4. 初始化模型和优化器\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, d_model, N, heads, dropout)\n",
    "model.to(device) # <-- 将模型移动到设备\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# 5. 模型训练\n",
    "def train_model(epochs):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = len(src_numerical) // batch_size\n",
    "        \n",
    "        train_generator = data_generator(src_numerical, trg_numerical, batch_size, src_pad_idx, trg_pad_idx)\n",
    "        \n",
    "        progress_bar = tqdm(train_generator, total=num_batches, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for i, (src, trg) in enumerate(progress_bar):\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            trg_input = trg[:, :-1]\n",
    "            targets = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            src_mask, trg_mask = create_masks(src, trg_input, src_pad_idx, trg_pad_idx)\n",
    "            \n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), targets, ignore_index=trg_pad_idx)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            current_avg_loss = total_loss / (i + 1)\n",
    "            progress_bar.set_postfix({\"avg_loss\": f\"{current_avg_loss:.3f}\"})\n",
    "            \n",
    "        epoch_time = time.time() - start\n",
    "        final_epoch_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1} finished. Total time: {epoch_time//60:.0f}m {epoch_time%60:.0f}s, Final Avg Loss: {final_epoch_loss:.3f}\")\n",
    "        start = time.time()\n",
    "\n",
    "# 6. 模型测试\n",
    "def translate(src_sentence, max_len=80):\n",
    "    # 注意：此处不再需要 model.eval()，因为我在主循环中统一处理\n",
    "    src = torch.LongTensor(tokenize_en_sentence(src_sentence, SRC_VOCAB)).unsqueeze(0).to(device)\n",
    "    src_mask = (src != src_pad_idx).unsqueeze(-2)\n",
    "    \n",
    "    e_outputs = model.encoder(src, src_mask)\n",
    "    \n",
    "    outputs = torch.zeros(max_len).type_as(src.data)\n",
    "    outputs[0] = torch.LongTensor([TRG_VOCAB.stoi['<sos>']]).to(device)\n",
    "    \n",
    "    for i in range(1, max_len):\n",
    "        trg_mask = np.triu(np.ones((1, i, i)), k=1).astype('uint8')\n",
    "        trg_mask = Variable(torch.from_numpy(trg_mask) == 0).to(device)\n",
    "        \n",
    "        out = model.out(model.decoder(outputs[:i].unsqueeze(0), e_outputs, src_mask, trg_mask))\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        val, ix = out[:, -1].data.topk(1)\n",
    "        \n",
    "        outputs[i] = ix[0][0]\n",
    "        if ix[0][0] == TRG_VOCAB.stoi['<eos>']:\n",
    "            break\n",
    "            \n",
    "    return \" \".join([TRG_VOCAB.itos[ix.item()] for ix in outputs[:i+1]])\n",
    "\n",
    "    # 运行\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 训练模型\n",
    "    train_model(epochs)\n",
    "    \n",
    "    # 2. 创建一个包含多个测试语句的列表\n",
    "    test_sentences = [\n",
    "        'Let me see.',\n",
    "        'Hello, how are you?',\n",
    "        'This is a test.',\n",
    "        'I love to learn new things.',\n",
    "        'What is your name?',\n",
    "        'The weather is nice today.',\n",
    "        'Where is the library?'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*20 + \" 开始翻译测试 \" + \"=\"*20)\n",
    "    \n",
    "    # 3. 循环遍历并翻译每个语句\n",
    "    model.eval() # 确保模型处于评估模式\n",
    "    with torch.no_grad(): # 在评估时关闭梯度计算，节省资源\n",
    "        for sentence in test_sentences:\n",
    "            print(f\"\\n原始 (en): '{sentence}'\")\n",
    "            translation = translate(sentence)\n",
    "            print(f\"翻译 (fr): {translation}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47A252DBB5B34B40A8EF9EE4E91753D0",
    "jupyter": {},
    "notebookId": "68b069ddb2fa13cb0f8bd45f",
    "runtime": {
     "execution_status": null,
     "is_visible": false,
     "status": "default"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 实验总结  \n",
    "\n",
    "### 1. 实验背景与目标  \n",
    "\n",
    "复现书里一个基于 “Attention Is All You Need” 论文的 Transformer 模型，用于英法机器翻译。原始代码基于较早的 PyTorch 和 `torchtext` 版本，在当前的主流的深度学习环境中（PyTorch 2.0+）已无法直接运行。我尝试了在heywhale中使用白嫖的算力来运行，但没有权限自己配置镜像，只能使用官方的镜像，官方的镜像没有完全适配requirements中的包，尽管我使用了py=3.8.5，torch=1.6的镜像，但仍然会出现环境配置的问题随后就放弃了对原论文配置的完全复现，  \n",
    "\n",
    "**核心目标：** 在不依赖 `torchtext` 旧版 API 的前提下，重构数据处理和模型训练流程，使其能够在现代 PyTorch 框架下，并利用 GPU 进行高效训练。  \n",
    "\n",
    "### 2. 核心挑战与解决过程  \n",
    "\n",
    "在将旧代码向新框架迁移的过程中，我遇到并解决了一系列典型问题：  \n",
    "\n",
    "1.  **`torchtext` API 废弃问题**：  \n",
    "    * **挑战**：旧代码的核心依赖 `torchtext.data` 在新版本中已被移除，导致程序在导入库时就出现 `OSError` 链接错误。  \n",
    "    * **解决方案**：我放弃了使用兼容层 `torchtext.legacy` 的方案，选择彻底解耦对 `torchtext` 的依赖。通过手动实现了三个核心组件，增强了代码的自主性和可移植性：  \n",
    "        * **自定义词汇表 (`Vocabulary` Class)**：手动管理从词元到索引的映射，包括特殊标记（`<unk>`, `<pad>` 等）。  \n",
    "        * **自定义数据处理流**：重写了 `prepare_data` 函数，负责读取源文件、分词、过滤和数值化。  \n",
    "        * **自定义批次生成器 (`data_generator`)**：实现了动态填充（Padding）和批次迭代功能，替代了原有的 `Iterator`。  \n",
    "\n",
    "2.  **环境依赖与底层库冲突**：  \n",
    "    * **挑战**：在解决了代码层面的问题后，程序在数据预处理阶段出现内核无故重启。但服务器拥有 80GB 内存和 48GB 显存，排除了内存不足的可能。  \n",
    "    * **解决方案**：通过分析，我将问题定位在 PyTorch 和 `spaCy` 两个库对 GPU 资源的底层冲突上。最终通过**重新安装纯 CPU 版本的 `spaCy`**，强制其在 CPU 上执行轻量级的分词任务，将 GPU 资源完全留给 PyTorch 进行模型计算，从而根除了冲突，解决了内核崩溃问题。  \n",
    "\n",
    "3.  **GPU 适配与设备不匹配**：  \n",
    "    * **挑战**：在启用 GPU 训练后，出现了经典的 `RuntimeError: Expected all tensors to be on the same device...` 错误。  \n",
    "    * **解决方案**：我系统性地检查了代码，确保了：  \n",
    "        * 模型参数通过 `model.to(device)` 被正确地移动到 GPU。  \n",
    "        * 在训练和推理的每一步，所有输入张量（包括数据和掩码）都通过 `.to(device)` 被显式地迁移到与模型相同的设备上。  \n",
    "\n",
    "### 3. 实验结果分析  \n",
    "\n",
    "在模型成功运行后，我对少量测试语句进行了翻译评估。结果表明，在短短 2 个周期的训练后，模型表现符合预期，具体如下：  \n",
    "\n",
    "* **已学到的能力**：  \n",
    "    * **固定短语**：对于训练数据中频繁出现的固定短语（如 \"What is your name?\" -> \"quel est votre nom ?\"），模型能给出完全准确的翻译。  \n",
    "    * **基本句式**：模型掌握了基础的句子结构，如 `Where is the...` -> `Où est la...`。  \n",
    "\n",
    "* **存在的不足**：  \n",
    "    * **词汇掌握有限**：模型经常用一个学过的、但语义不符的词替换生僻词（如 `library` -> `maison`）。  \n",
    "    * **语义理解偏差**：对于需要联系上下文的句子，模型理解能力不足，导致翻译结果与原意大相径庭（如 `Hello, how are you?` -> `What are you doing?`）。  \n",
    "    * **信息丢失**：在处理稍长的句子时，会忽略形容词、动词等关键信息，造成翻译的过度简化（如 `I love to learn new things` -> `I like things`）。  \n",
    "\n",
    "### 4. 结论与展望  \n",
    "\n",
    "本次实验成功达成了核心目标：**我完整地复现了 Transformer 的代码，并将其从对旧版 `torchtext` 的依赖中解放出来，使其成为了一个能在现代 PyTorch 环境中稳定运行、并充分利用 GPU 加速的独立项目。**  \n",
    "\n",
    "整个调试过程不仅解决了代码层面的兼容性问题，还深入到了底层库的资源冲突层面，为未来处理类似的环境问题积累了宝贵的经验。  \n",
    "\n",
    "**未来可行的优化方向包括：**  \n",
    "1.  **进行充分训练**：将训练周期从 2 个大幅增加到 20 个或更多，以充分挖掘模型的潜力。  \n",
    "2.  **采用更优的解码策略**：将当前的贪心搜索（Greedy Search）替换为集束搜索（Beam Search），以生成更流畅、更全局最优的翻译结果。  \n",
    "3.  **扩大训练数据集**：使用更大规模、更高质量的平行语料库进行训练。  \n",
    "4.  **模型与超参数调优**：尝试调整模型的层数、头数、隐藏层维度以及优化器的学习率等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
