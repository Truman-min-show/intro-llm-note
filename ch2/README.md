##### 2.1-transformer架构：

###### 问题：

1. 为什么使用层归一化，是否可使用批归一化？批归一化将同一batch中的各个vector的对应feature做归一化好理解，但层归一化将每一个向量的不同feature进行归一化，这个该如何理解？
2. decoder中多头交叉注意力块获取的encoder输入是否可以从相对应某个块的输出获取，而不是所有的en\_input都是从encoder最后的输出来获取？
3. attent结构中的残差和归一化是否可以调换顺序，不用非得放在attention块和fc块之后？

###### 李宏毅的网课中获得的思考：

1. cnn和self-attention的区别在于cnn每次卷积时只能关注到卷积核内的几个像素而自注意力相当于把卷积核放到和图像一样的大小，每次都可以从所有的像素中获取信息，获得全图的感受野
2. rnn和self-attention是等价的，但attention的优势在于对每个输入的向量进行计算时与其位置编码相差很多的向量的信息也可以通过注意力分数很大程度地影响该向量的输出，而rnn则会因为位置的相差，在多层计算后可能会导致前面的向量的信息被忽略或是丢失。同时rnn是串行的，但attention可以并行，方便并行计算
3. attention通过计算注意力分数和各向量的value可以有效捕捉到长输入的关键信息

##### 2.2-预训练：

###### 问题：

1. GPT既然是生成式预训练架构为何使用的attention块不是掩码的？使用掩码可以在训练时避免前面的输出生成受到后面的影响。
2. transformer架构单独把decoder拿出来是GPT，单独把encoder拿出来是bert，为什么会有这样的效果？
3. 有监督微调使用混合了预训练任务损失和下游任务损失，这与RL和蒸馏时常使用的KL有什么关系？
4. 对bert预训练时由于token限制，超过512的直接截断，这应该会导致影响吧，如何优化？

##### 2.3-大模型的结构

###### tips：

openai发布gpt5前一周，也开源了gpt-oss，水平和o4-mini差不多，之后上线到了AWS。

###### 问题：

1. RMS归一化相比于高斯分布的归一化有什么好处？
2. 稀疏注意力机制中是否可以将全局注意力中的全局节点理解为将两个向量的信息链接的一个纽带，从而达到节省非稀疏注意力中不必要的计算时间和存储空间？

##### 2.4-MOE





