## 2.1-transformer架构：

### 问题：

1. 为什么使用层归一化，是否可使用批归一化？批归一化将同一batch中的各个vector的对应feature做归一化好理解，但层归一化将每一个向量的不同feature进行归一化，这个该如何理解？
    - 样本的独立性:不同长度的句子如果做批归一化会强制padding到相同长度，干扰BN结果，而LN的每次都是单独一个样本不受干扰
    - LN在训练和推理是一致的不受训练批次和推理样本的巨大差异影响

2. decoder中多头交叉注意力块获取的encoder输入是否可以从相对应某个块的输出获取，而不是所有的en_input都是从encoder最后的输出来获取？
    - ok的，这个有查到不少模型都做了对应的改进


3. attent结构中的残差和归一化是否可以调换顺序，不用非得放在attention块和fnn块之后？
    - GPT等之后的众多模型都采用了先归一化的架构

### 李宏毅的网课中获得的思考：

1. cnn和self-attention的区别在于cnn每次卷积时只能关注到卷积核内的几个像素而自注意力相当于把卷积核放到和图像一样的大小，每次都可以从所有的像素中获取信息，获得全图的感受野
2. rnn和self-attention是等价的，但attention的优势在于对每个输入的向量进行计算时与其位置编码相差很多的向量的信息也可以通过注意力分数很大程度地影响该向量的输出，而rnn则会因为位置的相差，在多层计算后可能会导致前面的向量的信息被忽略或是丢失。同时rnn是串行的，但attention可以并行，方便并行计算
3. attention通过计算注意力分数和各向量的value可以有效捕捉到长输入的关键信息

## 2.2-预训练：

### 问题：

1. transformer架构单独把decoder拿出来是GPT，单独把encoder拿出来是bert，为什么会有这样的效果？
    - bert是双向自注意力，预训练任务是掩码语言模型。双向上下文理解能力使得BERT非常擅长**自然语言理解（NLU）**任务，如文本分类、情感分析、命名实体识别等，这些任务都需要对已有文本进行深入分析
    - GPT的核心是带掩码的单向自注意力，预训练任务是标准的语言模型，即根据前面的词预测下一个词。从左到右的结构天然适合**自然语言生成（NLG）**任务。如文本续写、对话、摘要生成等

2. 有监督微调使用混合了预训练任务损失和下游任务损失，这与RL和蒸馏时常使用的KL散度有什么关系？
    - 它们都体现了一种“保持接近”的思想。SFT的混合损失希望模型在学习新任务的同时，其行为（由预训练任务衡量）不要离原始的预训练模型太远。RL和蒸馏中的KL散度则更直接地在概率分布层面上约束模型，使其输出分布不要离参考分布（旧策略或教师模型）太远。可以说，它们都是为了在学习新知识的同时，稳定地保留已有知识或能力的手段。

3. 对bert预训练时由于token限制，超过512的直接截断，这应该会导致影响吧，如何优化？
    - 使用各种方法增大token限制

## 2.3-大模型的结构

### tips：

openai发布gpt5前一周，也开源了gpt-oss，水平和o4-mini差不多，之后上线到了AWS。

### 问题：

1. RMS归一化相比于高斯分布的归一化有什么好处？
    - RMS相当于只做了把均方值缩放到1，省去了减去均值这一步，不用对向量进行中心化，而往往这个中心化没什么用，去除后还可以减少计算量

2. 稀疏注意力机制中是否可以将全局注意力中的全局节点理解为将两个向量的信息链接的一个纽带，从而达到节省标准注意力中不必要的计算时间和存储空间？
    - ok的，这种机制避免了所有token之间都进行计算的巨大开销，同时通过少数关键节点维持了全局信息的流动性，实现了效率和性能的平衡

## 2.4-MOE

### 问题：
1. 稠密专家模型也是要激活所有的参数，感觉也就比不使用MOE多了一个对不同专家的参数权重的加权输出。正常的FNN在训练时，每层的参数应该已经是对不同的token有不同的重点和取舍，再次进行加权输出的意义是什么？
    - FNN是静态的加权机制而MOE则是动态的
    - MOE为为不同的expert引入了很强的归纳偏置，而FNN的bias则相对通用
    - 稠密MoE是在FFN层内部引入了一种类似注意力的动态路由机制，只不过注意力作用于序列维度，而MoE的门控作用于并行的专家模块维度

2. 混合专家模型和集成学习的联系？
    - 集成学习是独立训练的，而MOE则是端到端
    - 主要目标不同：
        - 集成学习: 主要目标是通过模型多样性来降低方差，提高预测的鲁棒性和准确性。
        - MoE: 主要目标是在控制计算成本的前提下，极大地扩展模型的参数容量，以存储更多的知识和提升性能。它是一种模型架构，而不是一种模型组合方法。





