
# 学习《大语言模型：从理论到实践》的复现与思考

## 1\. 项目简介

本仓库是我于2025年暑假期间对《大语言模型：从理论到实践》一书进行系统性学习、代码复现与深度思考的完整记录。

### 1.1 学习目标

我的导师要求我在暑期内完成本书的学习与代码复现，并为开学后的学术交流做好准备。本次学习的核心目标并非简单地运行全部代码，而是：

1.  **构建体系化认知**：深入理解大语言模型从底层原理到前沿应用的全链路技术。
2.  **深化理论与实践的结合**：通过“跑通-读懂-思考-总结”四步法，将书本理论与配套代码进行交叉验证，掌握核心算法的实现细节。
3.  **培养批判性与创新性思维**：针对每个技术点，结合AIGC工具进行追问与探讨，形成自己独特的学术见解，为未来的博士研究寻找方向。
4.  **沉淀可复用的知识库**：将学习过程中的笔记、代码注释、实验结果和深度思考整理归档，方便随时回顾与查阅。

### 1.2 学习方法

我将采用为期18天的学习计划，结合实体书与[官方代码库](https://github.com/intro-llm/intro-llm-code)，分阶段、有侧重地进行学习。对于算力要求高的章节（如分布式训练），将以理论学习和代码研读为主；对于核心且可实践的章节（如指令微调、RAG），将进行完整的代码复现与分析。

## 2\. 学习计划与进度

本仓库将根据以下18天学习计划，逐日更新学习笔记与成果。

| **阶段** | **天数** | **核心章节** | **学习重点与产出** |
| :--- | :--- | :--- | :--- |
| **一、夯实基础** | 5天 (Day 1-5) | Ch2: 大语言模型基础\<br\>Ch3: 预训练数据 | Transformer原理、BERT预训练、数据清洗与去重策略。产出对Transformer及数据处理流程的深度分析笔记。 |
| **二、核心技术** | 7天 (Day 6-12) | Ch5: 指令微调\<br\>Ch6: 强化学习\<br\>Ch9: RAG | LoRA原理与实现、RLHF三阶段流程、基础及高级RAG系统。产出PEFT、模型对齐及RAG技术的代码复现与对比分析。 |
| **三、拓展视野** | 6天 (Day 13-18) | Ch4, 7, 8, 10, 11 | 分布式训练思想、多模态、Agent、推理优化与模型评估。产出对前沿应用和工程实践的综述性理解和关键代码解读。 |

## 3\. 仓库内容结构

本仓库将以章节为单位进行组织，每个章节文件夹下包含：

  - `README.md`: 该章节的学习笔记，包含：
      - **核心理论总结**：对关键概念的提炼与理解。
      - **代码复现与分析**：环境配置、核心代码解读、实验结果展示。
      - **我的思考与提问**：结合AIGC工具和网课进行的深度追问与思辨过程记录。
  - `code/`: 若有对原代码的修改、注释或补充实验，将存放于此。

## 4\. 如何查看本项目

1.  **从README开始**：建议首先阅读本README，了解我的学习全貌。
2.  **分章节浏览**：进入`ChapterNotes`（或直接按章节命名）目录，选择你感兴趣的章节进行查阅。
3.  **关注“我的思考”**：每个章节笔记的最后一部分是我学习深度的体现，欢迎交流与探讨。

期待这次系统性的学习能为我即将开始的博士生涯奠定坚实的基础。
