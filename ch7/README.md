## 7.1 多模态大语言模型基础

### 思考：

这一节就是简单介绍了一下几个经典的多模态大语言模型，并说明了其和多模态大模型的不同。我个人总被多模态大语言模型的效果震惊，尤其是在今年的四月GPT更新了其图片生成功能，现实做到了可以“用嘴P图”。吉卜力风格的AI生成图片瞬间火爆全网，一个AI制作的时代仿佛就要来临，我曾经也和室友在吃饭时也讨论过这一点。



## 7.2 大语言模型和多模态融合架构

### 问题：

1. 对于图像，每个像素是一个vector，使用CNN比较好理解。如果是在音频上做卷积，每个vector是什么？进行卷积的意义在何处，宏观上进行了什么了操作？

   * 每个“时间点”通常是一个标量样本，对原始波形做一维卷积（1D conv），卷积核沿时间轴滑动，直接在时间域上混合相邻样本。
   * 就像图像中的边缘/纹理，卷积能检测短时（局部）模式 —— 如音节边界、音高跃迁、瞬态（敲击声）或周期性谐波结构。浅层卷积学到低级特征（高频瞬态、基频周期），深层组合这些得到更抽象的概念（说话人的音色、声学事件类别）

2. 对于多模态的LLM，类似于AnyGPT这样的模型，不同模态信息的token是如何被统一语义的，这和将不同模态的模型融合拼接起来的架构的模型相比，优势在于什么地方？

   * 多模态 LLM 的“统一语义”通常通过 模态专用编码器 → 投影到共享 embedding 空间 → 在统一 Transformer/LLM 中交互 来实现。相比把各模态模型简单拼接，统一方法在跨模态推理、能力迁移、prompt 接口和细粒度对齐上有明显优势，但代价是数据/算力和训练复杂度更高。

## 7.3 多模态LLM的训练

### 问题：

这一节中提到要完成视觉语义关联可以使用负样本生成的方法来优化，又提到负样本生成是对比学习中的关键部分。在我的直观理解中负样本生成有点类似于RLHF中那个rejected的样本，如果再配合正样本的话不久成了一个accept-reject对这样就可以做PPO或者其他强化学习的算法了。从数据来源上看这些"rejected"或是负样本都是人为创造或是使用奖励模型挑选出来的”不想要的结果“那么负样本学习和RLHF之前的异同点是什么？

相同点：两者都利用了 “正/负对比” 的思想，把 不想要的结果（负样本/rejected） 作为学习信号。
不同点：
  * 对比学习 → 表征学习任务，负样本是语义错配，目标是 embedding 对齐。
  * RLHF → 生成建模任务，rejected 样本是人类不喜欢的输出，目标是策略优化。
  * 可以说 RLHF 是在生成建模场景下，把“负样本”提升为一种“奖励信号”，而不仅仅是 embedding 空间的约束。
