## 11.3 大模型评估的方法

### 问题：

bleu和rouge两个评估指标评估的重点有何区别，同样是给出一定数量的参考文字，为何翻译的评估要靠bleu而摘要的评估则要使用rouge？

1. BLEU → 强调精确率（precision）：看模型生成的片段有多少出现在参考里，适合要求逐词/短语对齐的翻译。

   * Precision（精确率）：说的话里有多少是真正“命中要点”的？（生成端质量）

2. ROUGE → 强调召回率（recall）：看参考中的关键信息有多少被生成覆盖，适合关注信息覆盖的摘要。

   * Recall（召回率）：参考里的要点，说了多少？（覆盖率）

3. 翻译注重“我说的对不对”（precision），摘要注重“要点被没被覆盖”（recall）。

### tips：

这一节里提供的麦克尼马尔检验的示例我实际运行的结果和书本中有出入，即使环境包也完全按照requirements中的配置也是会不同

## 11.4 LLM评估实践

### tips：

这一节主要介绍了一些知名的大模型基准测试和各领域的一些认可度高的评估数据集。评估的一大重点就是防止数据泄露，即防止用于测评的数据集中有被用于训练的问题。还介绍了我也经常查看的LM Arena，这个”大模型竞技场“可以说是我使用AIGC的风向标，一般哪个在text中的code和math等最佳我就使用哪个。

