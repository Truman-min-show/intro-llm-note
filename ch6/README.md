## 6.1强化学习的概念

### 思考：

我在大创项目中对摘要模型的优化实际上就是强化学习的一种，与PPO不同，是采用了DPO的方法，不用训练奖励模型，将答案偏好内嵌在训练过程中

### tips：

最近我的邻居的猫不知怎么练会了开门的技巧，经常乱跑出来。随后我的邻居有一次忘记带钥匙了，她就不断敲门锁附近的地方，让自己的猫不断尝试，直到从里面开门，随后她大喜道："我没白养你"，想必她之后定会给宠物猫奖励一番。随后我又看见她忘带钥匙就反复敲门，总能听见门里传来的猫叫声和门把手被拨动的声音……这何尝不是一种强化学习？

## 6.2策略梯度

### tips：

这一节有大量的计算公式，学起来比较费劲，B站上有西湖大学的网课，这个网课又有点太深了，需要结合来看

### 思考：

相比于REINFORCE需要等到整条轨迹结束才能更新，TD方法引入了Bootstrapping机制，从而可以在每个时间步都更新参数，提高了效率

## 6.3推理模型的强化学习

### 问题：

1. deepseek是首个将推理过程显式输出的大模型，这可以方便用户查看ai助手是如何思考的，若用户对回答不满意，也方便了再提出意见。像deepseek这样先生成推理和思考的过程然后再输出回答的方式相比于gpt的o系列有哪些优势，明明都是推理模型？

   * 把“思考过程”展示给用户，相当于打开了模型的“思维黑盒”，提升了用户对回答的透明感和可信度
   * 用户发现DeepSeek的最终答案不符合预期时，您可以回溯它的思考过程，精确定位到出错或不完善的推理步骤
   * 形成CoT（Chain of Thought）本身就是一种对模型思维的约束和规范，可以减少模型在处理复杂问题时“一步到位”式的胡乱猜测或逻辑跳跃，强迫它遵循一个更严谨的分析路径，从而提升了最终答案的可靠性和准确性

2. aha moment真正的原理是什么，是延长思维链造成的结果吗？或者说这个推理的过程和输出的回答有什么本质的区别，是否可以理解为相当于两次回答，变成两个prompt：“先分析用户提问的问题，思考该问题的解决方案和用户更深层的要求”然后是“结合上次回答中的分析，去解决用户之前的问题”

   * “Aha moment”的本质，是模型通过一个结构化的、中间步骤明确的推理过程，解决了单步直接推理无法解决的复杂问题时，所展现出的“智能涌现”现象。 它并非某种神秘的魔法，而是 **延长并优化思维链（Chain of Thought）所带来的必然结果** 。
   * 可以将这个过程概念化为两个串联的Prompt：

     * Prompt 1 (内部生成): “针对用户‘<原始问题>’，请不要直接回答。首先，深入分析这个问题，将其分解，并制定一个详细的、分步骤的解决方案（即推理过程）。”
     * Prompt 2 (内部生成): “现在，严格遵循你刚才生成的解决方案‘<推理过程>’，一步步地执行它，并最终形成一个清晰、准确的答案来回应用户。”

## 6.4 RLHF

### 思考：

有时候在使用GPT时就会突然对同一个对话生成两个答案，然后再由你选择，这估计就是open AI构建人类偏好数据集的过程

## 6.6 RL的未来趋势

### 思考：

* 读到最后这一页时，突然想到了 AlphaGo 与 AlphaGo Zero 的训练差异。前者依赖大量人类棋谱作为起点，相当于让机器继承了人类数千年的棋艺知识；而后者完全抛弃了人类经验，通过自我博弈便在极短时间内超越了前者。这与“The Bitter Lesson”所揭示的真相高度契合：依赖人类知识注入的方式会受到认知局限，而规模化的算力与简单的搜索/学习规则结合，反而能迭代出更优解。从这个角度看，未来的强化学习模型或许也将逐渐减少对人工规则与偏好的依赖，更倾向于在大规模的自我博弈和环境交互中进化自身。
* 但这同时带来了新的思考：如果机器能够完全跳脱人类的知识体系，是否意味着它最终形成的“智能”将与人类理解的智能存在根本差异？这样的模型可能会得出人类完全想不到的解法——就像 Zero 中那些违背传统定式却极其高效的落子。我们或许要接受一个现实：AI 的进化路径未必与人类知识共线，而是可能出现“异质智能”。这既是令人兴奋的突破，也可能带来理解与控制上的挑战。
