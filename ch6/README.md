## 6.1强化学习的概念

### 思考：

我在大创项目中对摘要模型的优化实际上就是强化学习的一种，与PPO不同，是采用了DPO的方法，不用训练奖励模型，将答案偏好内嵌在训练过程中

### tips：

最近我的邻居的猫不知怎么练会了开门的技巧，经常乱跑出来。随后我的邻居有一次忘记带钥匙了，她就不断敲门锁附近的地方，让自己的猫不断尝试，直到从里面开门，随后她大喜道："我没白养你"，想必她之后定会给宠物猫奖励一番。随后我又看见她忘带钥匙就反复敲门，总能听见门里传来的猫叫声和门把手被拨动的声音……这何尝不是一种强化学习？

## 6.2策略梯度

### tips：

这一节有大量的计算公式，学起来比较费劲，B站上有西湖大学的网课，这个网课又有点太深了，需要结合来看

### 思考：

相比于REINFORCE需要等到整条轨迹结束才能更新，TD方法引入了Bootstrapping机制，从而可以在每个时间步都更新参数，提高了效率

## 6.3推理模型的强化学习

### 问题：

1. deepseek是首个将推理过程显式输出的大模型，这可以方便用户查看ai助手是如何思考的，若用户对回答不满意，也方便了再提出意见。像deepseek这样先生成推理和思考的过程然后再输出回答的方式相比于gpt的o系列有哪些优势，明明都是推理模型？
	- 把“思考过程”展示给用户，相当于打开了模型的“思维黑盒”，提升了用户对回答的透明感和可信度
	- 用户发现DeepSeek的最终答案不符合预期时，您可以回溯它的思考过程，精确定位到出错或不完善的推理步骤
	- 形成CoT（Chain of Thought）本身就是一种对模型思维的约束和规范，可以减少模型在处理复杂问题时“一步到位”式的胡乱猜测或逻辑跳跃，强迫它遵循一个更严谨的分析路径，从而提升了最终答案的可靠性和准确性

2. aha moment真正的原理是什么，是延长思维链造成的结果吗？或者说这个推理的过程和输出的回答有什么本质的区别，是否可以理解为相当于两次回答，变成两个prompt：“先分析用户提问的问题，思考该问题的解决方案和用户更深层的要求”然后是“结合上次回答中的分析，去解决用户之前的问题”

	- “Aha moment”的本质，是模型通过一个结构化的、中间步骤明确的推理过程，解决了单步直接推理无法解决的复杂问题时，所展现出的“智能涌现”现象。 它并非某种神秘的魔法，而是 **延长并优化思维链（Chain of Thought）所带来的必然结果** 。
	- 可以将这个过程概念化为两个串联的Prompt：
		- Prompt 1 (内部生成): “针对用户‘<原始问题>’，请不要直接回答。首先，深入分析这个问题，将其分解，并制定一个详细的、分步骤的解决方案（即推理过程）。”
		- Prompt 2 (内部生成): “现在，严格遵循你刚才生成的解决方案‘<推理过程>’，一步步地执行它，并最终形成一个清晰、准确的答案来回应用户。”

## 6.4 RLHF

### 思考：

有时候在使用GPT时就会突然对同一个对话生成两个答案，然后再由你选择，这估计就是open AI构建人类偏好数据集的过程



