{"cells":[{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"933AF9226A584C068AE9D5C2D513C39A","notebookId":"68b509aeb2fa13cb0f3cc343","trusted":true},"source":"import os\nos.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\nprint(\"HF_ENDPOINT:\", os.getenv(\"HF_ENDPOINT\"))\nfrom transformers import AutoTokenizer\nfrom collections import defaultdict","outputs":[{"output_type":"stream","name":"stdout","text":"HF_ENDPOINT: https://hf-mirror.com\n"},{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"}],"execution_count":1},{"cell_type":"code","metadata":{"jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"BE0FDDE7AD214D4DA7F4F54495506AF5","notebookId":"68b509aeb2fa13cb0f3cc343","trusted":true},"source":"# === 内容来自: main.py ===\n\n'''1.2.4 词元切分'''\n\n# 示例语料库\ncorpus = [\n    \"This is the HuggingFace Course.\",\n    \"This chapter is about tokenization.\",\n    \"This section shows several tokenizer algorithms.\",\n    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n]\n# 使用GPT-2词元分析器将输入分解为单词\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nword_freqs = defaultdict(int)\n# 统计文本中的词频\nfor text in corpus:\n    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n    new_words = [word for word, offset in words_with_offsets]\n    for word in new_words:\n        word_freqs[word] += 1\n# 计算基础词典，这里使用数据库中的所有字符\nalphabet = []\nfor word in word_freqs.keys():\n    for letter in word:\n        if letter not in alphabet:\n            alphabet.append(letter)\nalphabet.sort()\n# 在字典的开头增加特殊词元，GPT-2中仅有一个特殊词元\"<|endoftext|>\"，用来表示文本结束\nvocab = [\"<|endoftext|>\"] + alphabet.copy()\n# 将单词切分为字符\nsplits = {word: [c for c in word] for word in word_freqs.keys()}\n# 计算词元对的频率\ndef compute_pair_freqs(splits):\n    pair_freqs = defaultdict(int)\n    for word, freq in word_freqs.items():\n        split = splits[word]\n        if len(split) == 1:\n            continue\n        for i in range(len(split) - 1):\n            pair = (split[i], split[i + 1])\n            pair_freqs[pair] += freq\n    return pair_freqs\n# 合并词元对\ndef merge_pair(a, b, splits):\n    for word in word_freqs:\n        split = splits[word]\n        if len(split) == 1:\n            continue\n        i = 0\n        while i < len(split) - 1:\n            if split[i] == a and split[i + 1] == b:\n                split = split[:i] + [a + b] + split[i + 2:]\n            else:\n                i += 1\n        splits[word] = split\n    return splits\n# 迭代训练，直到字典大小达到目标\nvocab_size = 50\nmerges = {}\nwhile len(vocab) < vocab_size:\n    pair_freqs = compute_pair_freqs(splits)\n    best_pair = \"\"\n    max_freq = None\n    for pair, freq in pair_freqs.items():\n        if max_freq is None or max_freq < freq:\n            best_pair = pair\n            max_freq = freq\n    splits = merge_pair(*best_pair, splits)\n    merges[best_pair] = best_pair[0] + best_pair[1]\n    vocab.append(best_pair[0] + best_pair[1])\n# 训练完成后，tokenize函数用于对给定文本进行词元切分\ndef tokenize(text):\n    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n    splits = [[l for l in word] for word in pre_tokenized_text]\n    for pair, merge in merges.items():\n        for idx, split in enumerate(splits):\n            i = 0\n            while i < len(split) - 1:\n                if split[i] == pair[0] and split[i + 1] == pair[1]:\n                    split = split[:i] + [merge] + split[i + 2:]\n                else:\n                    i += 1\n            splits[idx] = split\n    return sum(splits, [])\n# 测试tokenizer\nprint(tokenize(\"This is not a token.\"))","outputs":[{"output_type":"stream","name":"stdout","text":"['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']\n"}],"execution_count":2}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":5}