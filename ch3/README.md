## Datasets-数据集处理

### 问题

1. 我曾记得程大伟老师在深度学习的课程上介绍过：完全由人类创建的内容已经全部被大模型用于训练了，全体人类产生信息的速度无法跟上大模型训练数据量增长而消耗的速度。在之后都是由AI生成的数据再提供给AI进行训练，当本身就是AIGC的生成的结果是否会对数据集产生干扰？
	- 容易导致 **模型坍塌** ，当模型开始以自身或同类模型的输出作为主要训练数据源时，会引发一个危险的恶性循环，数据分布会越来越窄，造成多样性丧失，并且会把本来就是AIGC生成的幻觉放大。

2. 对于各种类型的数据集，将其转化为可用于训练的tensor是最终的目标，而在如何进行转化这一步之前还需要对其进行一系列的预处理。对当前的我来说，自己创建一个数据并不现实，而是应该合理利用已有的、被广泛采纳的数据集。而将这些数据集使用并转为tensor的操作是否已有被集成在torch或者transformer中？我是不是不需要太关系这些数据具体该如何处理成可训练的张量？
	- datasets库和Tokenizer库已有高度集成的API工具链，几行代码直接一步到位。

### 思考

最显著可以增强大模型的性能的方法有两种方式：优化模型结构（例如推理模型，增加了思维链来解决复杂的问题）和“大即是好”的暴力美学（bigger is better），不断地像军备竞赛一样增大模型参数，然后再尽可能地获取更多的信息喂给大模型，让其更多地学会、或者说是模仿人类的思维和思考的方式。显而易见的是数据的处理是大模型训练中重要的前置步骤也是工程量最大的部分。

