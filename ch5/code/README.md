### LoRA-test：

trainable params: 2,359,296 || all params: 1,231,940,608 || trainable%: 0.1915

可以直观地看到使用LoRA微调训练的mt0-large模型的参数量仅为全量的0.2%，极大地减少了计算量

