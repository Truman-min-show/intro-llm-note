[2025-09-05 05:55:39,003] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-05 05:55:40,584] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 05:55:41,309] [WARNING] [runner.py:220:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0: setting --include=localhost:0
[2025-09-05 05:55:41,309] [INFO] [runner.py:610:main] cmd = /opt/conda/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29501 --enable_each_rank_log=None main.py --model_name_or_path ./models/Qwen3-0.6B --data_path ./data/MyDataset/CVPR2024Summary.json --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --num_train_epochs 10 --learning_rate 5e-6 --max_seq_len 512 --output_dir ./outputs/qwen0.6b_step1 --seed 42 --dtype bf16 --gradient_checkpointing --deepspeed --data_output_path ./data/cache
[2025-09-05 05:55:47,605] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-05 05:55:49,180] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 05:55:49,910] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.19.3-1+cuda12.3
[2025-09-05 05:55:49,910] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.19.3-1
[2025-09-05 05:55:49,910] [INFO] [launch.py:139:main] 0 NCCL_VERSION=2.19.3-1
[2025-09-05 05:55:49,910] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2025-09-05 05:55:49,910] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.19.3-1+cuda12.3
[2025-09-05 05:55:49,910] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2025-09-05 05:55:49,910] [INFO] [launch.py:139:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.19.3-1
[2025-09-05 05:55:49,910] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0]}
[2025-09-05 05:55:49,911] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-09-05 05:55:49,911] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-09-05 05:55:49,911] [INFO] [launch.py:164:main] dist_world_size=1
[2025-09-05 05:55:49,911] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-09-05 05:55:49,911] [INFO] [launch.py:256:main] process 8965 spawned with command: ['/opt/conda/bin/python3.11', '-u', 'main.py', '--local_rank=0', '--model_name_or_path', './models/Qwen3-0.6B', '--data_path', './data/MyDataset/CVPR2024Summary.json', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--num_train_epochs', '10', '--learning_rate', '5e-6', '--max_seq_len', '512', '--output_dir', './outputs/qwen0.6b_step1', '--seed', '42', '--dtype', 'bf16', '--gradient_checkpointing', '--deepspeed', '--data_output_path', './data/cache']
[2025-09-05 05:55:51,976] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-09-05 05:55:53,214] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-09-05 05:55:53,939] [INFO] [comm.py:821:init_distributed] cdb=None
ds_config: {'train_batch_size': 32, 'train_micro_batch_size_per_gpu': 8, 'steps_per_print': 10, 'zero_optimization': {'stage': 0, 'overlap_comm': True, 'offload_param': {'device': 'none'}, 'offload_optimizer': {'device': 'none'}, 'stage3_param_persistence_threshold': 10000.0, 'stage3_max_live_parameters': 30000000.0, 'stage3_prefetch_bucket_size': 30000000.0, 'memory_efficient_linear': False}, 'bfloat16': {'enabled': True}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'wall_clock_breakdown': False, 'hybrid_engine': {'enabled': False, 'max_out_tokens': 512, 'inference_tp_size': 1, 'release_inference_cache': False, 'pin_parameters': True, 'tp_gather_partition_size': 8}, 'tensorboard': {'enabled': False, 'output_path': 'step1_tensorboard/ds_tensorboard_logs/', 'job_name': 'step1_model_tensorboard'}}
[rank0]:[W905 05:55:54.693202775 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
create model ...
created model ...
preparing data ...
Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:  26%|██▌       | 52/200 [00:00<00:00, 500.07 examples/s]Map:  52%|█████▏    | 104/200 [00:00<00:00, 507.24 examples/s]Map:  81%|████████  | 162/200 [00:00<00:00, 535.34 examples/s]Map: 100%|██████████| 200/200 [00:00<00:00, 495.51 examples/s]
Using /home/mw/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/mw/.cache/torch_extensions/py311_cu124/fused_adam/build.ninja...
/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.06798934936523438 seconds
[2025-09-05 05:55:56,269] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.5, git-hash=unknown, git-branch=unknown
[2025-09-05 05:55:56,269] [INFO] [comm.py:846:init_distributed] Distributed backend already initialized
[2025-09-05 05:55:56,269] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 1
[2025-09-05 05:55:56,479] [INFO] [engine.py:1356:_configure_distributed_model] ********** distributed groups summary **********
	 self.dp_world_size=1
	 self.mp_world_size=1
	 self.seq_dp_world_size=1
	 self.sequence_parallel_size=1
***********************************************
[2025-09-05 05:55:56,514] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-09-05 05:55:56,514] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-09-05 05:55:56,515] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-09-05 05:55:56,520] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-09-05 05:55:56,520] [INFO] [logging.py:107:log_dist] [Rank 0] Creating BF16 optimizer
[2025-09-05 05:55:56,681] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2025-09-05 05:55:56,681] [INFO] [utils.py:782:see_memory_usage] MA 1.11 GB         Max_MA 1.11 GB         CA 1.13 GB         Max_CA 1 GB 
[2025-09-05 05:55:56,681] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 16.05 GB, percent = 5.2%
[2025-09-05 05:55:56,839] [INFO] [utils.py:781:see_memory_usage] before initializing group 0
[2025-09-05 05:55:56,839] [INFO] [utils.py:782:see_memory_usage] MA 1.11 GB         Max_MA 1.11 GB         CA 1.13 GB         Max_CA 1 GB 
[2025-09-05 05:55:56,840] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 16.05 GB, percent = 5.2%
[2025-09-05 05:55:57,019] [INFO] [utils.py:781:see_memory_usage] after initializing group 0
[2025-09-05 05:55:57,020] [INFO] [utils.py:782:see_memory_usage] MA 5.55 GB         Max_MA 5.55 GB         CA 7.8 GB         Max_CA 8 GB 
[2025-09-05 05:55:57,020] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 16.05 GB, percent = 5.2%
[2025-09-05 05:55:57,196] [INFO] [utils.py:781:see_memory_usage] before initializing group 1
[2025-09-05 05:55:57,196] [INFO] [utils.py:782:see_memory_usage] MA 5.55 GB         Max_MA 5.55 GB         CA 7.8 GB         Max_CA 8 GB 
[2025-09-05 05:55:57,197] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 16.05 GB, percent = 5.2%
[2025-09-05 05:55:57,362] [INFO] [utils.py:781:see_memory_usage] after initializing group 1
[2025-09-05 05:55:57,363] [INFO] [utils.py:782:see_memory_usage] MA 5.55 GB         Max_MA 5.55 GB         CA 7.8 GB         Max_CA 8 GB 
[2025-09-05 05:55:57,363] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 16.05 GB, percent = 5.2%
[2025-09-05 05:55:57,519] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2025-09-05 05:55:57,520] [INFO] [utils.py:782:see_memory_usage] MA 5.55 GB         Max_MA 5.55 GB         CA 7.8 GB         Max_CA 8 GB 
[2025-09-05 05:55:57,520] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 16.05 GB, percent = 5.2%
[2025-09-05 05:55:57,520] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = BF16_Optimizer
[2025-09-05 05:55:57,520] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-09-05 05:55:57,520] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7eff95fc5190>
[2025-09-05 05:55:57,520] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-06, 5e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2025-09-05 05:55:57,521] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True
[2025-09-05 05:55:57,521] [INFO] [config.py:954:print] DeepSpeedEngine configuration:
[2025-09-05 05:55:57,521] [INFO] [config.py:958:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-09-05 05:55:57,521] [INFO] [config.py:958:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-09-05 05:55:57,521] [INFO] [config.py:958:print]   amp_enabled .................. False
[2025-09-05 05:55:57,521] [INFO] [config.py:958:print]   amp_params ................... False
[2025-09-05 05:55:57,521] [INFO] [config.py:958:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-09-05 05:55:57,521] [INFO] [config.py:958:print]   bfloat16_config .............. enabled=True immediate_grad_update=False check_grad_overflow=False
[2025-09-05 05:55:57,521] [INFO] [config.py:958:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}
[2025-09-05 05:55:57,521] [INFO] [config.py:958:print]   checkpoint_parallel_write_pipeline  False
[2025-09-05 05:55:57,521] [INFO] [config.py:958:print]   checkpoint_tag_validation_enabled  True
[2025-09-05 05:55:57,521] [INFO] [config.py:958:print]   checkpoint_tag_validation_fail  False
[2025-09-05 05:55:57,521] [INFO] [config.py:958:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7eff940cacd0>
[2025-09-05 05:55:57,521] [INFO] [config.py:958:print]   communication_data_type ...... None
[2025-09-05 05:55:57,521] [INFO] [config.py:958:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False keep_int_input_tensors=True keep_all_input_tensors=False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   curriculum_enabled_legacy .... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   curriculum_params_legacy ..... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   data_efficiency_enabled ...... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   dataloader_drop_last ......... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   disable_allgather ............ False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   dump_state ................... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   eigenvalue_enabled ........... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   eigenvalue_gas_boundary_resolution  1
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   eigenvalue_layer_num ......... 0
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   eigenvalue_max_iter .......... 100
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   eigenvalue_stability ......... 1e-06
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   eigenvalue_tol ............... 0.01
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   eigenvalue_verbose ........... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   elasticity_enabled ........... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   float16_config ............... enabled=False auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   global_rank .................. 0
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   grad_accum_dtype ............. None
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   gradient_accumulation_steps .. 4
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   gradient_clipping ............ 1.0
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   gradient_predivide_factor .... 1.0
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   graph_harvesting ............. False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   load_universal_checkpoint .... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   memory_breakdown ............. False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   mics_hierarchial_params_gather  False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   mics_shard_size .............. -1
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='step1_tensorboard/ds_tensorboard_logs/', job_name='step1_model_tensorboard') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   optimizer_legacy_fusion ...... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   optimizer_name ............... None
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   optimizer_params ............. None
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   pld_enabled .................. False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   pld_params ................... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   prescale_gradients ........... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   scheduler_name ............... None
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   scheduler_params ............. None
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   seq_parallel_communication_data_type  torch.float32
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   sparse_attention ............. None
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   sparse_gradients_enabled ..... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   steps_per_print .............. 10
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   timers_config ................ enabled=True synchronized=True
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   torch_autocast_dtype ......... None
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   torch_autocast_enabled ....... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   torch_autocast_lower_precision_safe_modules  None
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   train_batch_size ............. 32
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   train_micro_batch_size_per_gpu  8
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   use_data_before_expert_parallel_  False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   use_node_local_storage ....... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   wall_clock_breakdown ......... False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   weight_quantization_config ... None
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   world_size ................... 1
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   zero_allow_untested_optimizer  False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) zenflow=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   zero_enabled ................. False
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   zero_force_ds_cpu_optimizer .. True
[2025-09-05 05:55:57,522] [INFO] [config.py:958:print]   zero_optimization_stage ...... 0
[2025-09-05 05:55:57,522] [INFO] [config.py:944:print_user_config]   json = {
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 8, 
    "steps_per_print": 10, 
    "zero_optimization": {
        "stage": 0, 
        "overlap_comm": true, 
        "offload_param": {
            "device": "none"
        }, 
        "offload_optimizer": {
            "device": "none"
        }, 
        "stage3_param_persistence_threshold": 1.000000e+04, 
        "stage3_max_live_parameters": 3.000000e+07, 
        "stage3_prefetch_bucket_size": 3.000000e+07, 
        "memory_efficient_linear": false
    }, 
    "bfloat16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "hybrid_engine": {
        "enabled": false, 
        "max_out_tokens": 512, 
        "inference_tp_size": 1, 
        "release_inference_cache": false, 
        "pin_parameters": true, 
        "tp_gather_partition_size": 8
    }, 
    "tensorboard": {
        "enabled": false, 
        "output_path": "step1_tensorboard/ds_tensorboard_logs/", 
        "job_name": "step1_model_tensorboard"
    }
}
***** Running training *****
***** Evaluating perplexity, Epoch 0/10 *****
ppl: 21.03133201599121, loss: 3.046013355255127
Beginning of Epoch 1/10, Total Micro Batches 23
Model Parameters: 0.596 B, Latency: 0.31s, TFLOPs: 52.55, Samples/sec: 25.75, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.31s, TFLOPs: 51.84, Samples/sec: 25.40, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 49.50, Samples/sec: 24.26, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.34s, TFLOPs: 47.37, Samples/sec: 23.22, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.28s, TFLOPs: 58.53, Samples/sec: 28.68, Time/seq 0.03s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.29s, TFLOPs: 55.97, Samples/sec: 27.43, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 49.64, Samples/sec: 24.33, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 50.32, Samples/sec: 24.66, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.02, Samples/sec: 26.47, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.99, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.34s, TFLOPs: 48.73, Samples/sec: 23.88, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 50.99, Samples/sec: 24.99, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.99, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.99, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.99, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.00, Samples/sec: 24.99, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.02, Samples/sec: 26.96, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.75, Samples/sec: 23.89, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.94, Samples/sec: 26.92, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.04, Samples/sec: 25.01, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.01, Samples/sec: 26.96, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.34s, TFLOPs: 48.71, Samples/sec: 23.87, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.16s, TFLOPs: 98.94, Samples/sec: 48.49, Time/seq 0.02s, Batch Size: 8, Sequence Length: 512
***** Evaluating perplexity, Epoch 1/10 *****
ppl: 16.25992774963379, loss: 2.788703680038452
Beginning of Epoch 2/10, Total Micro Batches 23
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.01, Samples/sec: 25.00, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.34s, TFLOPs: 48.72, Samples/sec: 23.88, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.74, Samples/sec: 23.88, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.03, Samples/sec: 26.97, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.00, Samples/sec: 24.99, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.00, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.77, Samples/sec: 23.90, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.00, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.03, Samples/sec: 25.01, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.05, Samples/sec: 26.98, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.74, Samples/sec: 23.89, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.00, Samples/sec: 26.96, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.00, Samples/sec: 24.99, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.02, Samples/sec: 26.96, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.98, Samples/sec: 26.94, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.96, Samples/sec: 26.94, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
[2025-09-05 05:56:10,663] [INFO] [logging.py:107:log_dist] [Rank 0] step=10, skipped=0, lr=[4.665063509461098e-06, 4.665063509461098e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2025-09-05 05:56:10,674] [INFO] [timer.py:264:stop] epoch=1/micro_step=17/global_step=10, RunningAvgSamplesPerSec=26.094141428771746, CurrSamplesPerSec=26.45892180901619, MemAllocated=12.32GB, MaxMemAllocated=19.51GB
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.01, Samples/sec: 25.00, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.77, Samples/sec: 23.90, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.98, Samples/sec: 26.94, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.03, Samples/sec: 26.97, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 50.97, Samples/sec: 24.98, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.76, Samples/sec: 23.90, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.16s, TFLOPs: 98.95, Samples/sec: 48.49, Time/seq 0.02s, Batch Size: 8, Sequence Length: 512
***** Evaluating perplexity, Epoch 2/10 *****
ppl: 14.313844680786133, loss: 2.661227226257324
Beginning of Epoch 3/10, Total Micro Batches 23
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.02, Samples/sec: 26.97, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.36s, TFLOPs: 45.58, Samples/sec: 22.34, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.75, Samples/sec: 23.89, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.98, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.94, Samples/sec: 26.93, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.00, Samples/sec: 24.99, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.78, Samples/sec: 23.90, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.94, Samples/sec: 26.92, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.98, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 50.97, Samples/sec: 24.98, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.73, Samples/sec: 23.88, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.97, Samples/sec: 26.94, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.96, Samples/sec: 26.93, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.03, Samples/sec: 25.01, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.96, Samples/sec: 26.93, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.97, Samples/sec: 26.94, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.97, Samples/sec: 26.94, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.36s, TFLOPs: 45.58, Samples/sec: 22.34, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.99, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.00, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.90, Samples/sec: 26.91, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.36s, TFLOPs: 45.63, Samples/sec: 22.36, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.16s, TFLOPs: 98.98, Samples/sec: 48.51, Time/seq 0.02s, Batch Size: 8, Sequence Length: 512
***** Evaluating perplexity, Epoch 3/10 *****
ppl: 13.105905532836914, loss: 2.5730628967285156
Beginning of Epoch 4/10, Total Micro Batches 23
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.99, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.74, Samples/sec: 23.88, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.36s, TFLOPs: 45.63, Samples/sec: 22.36, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.02, Samples/sec: 26.96, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.97, Samples/sec: 26.94, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.01, Samples/sec: 26.96, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.36s, TFLOPs: 45.60, Samples/sec: 22.35, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.98, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.02, Samples/sec: 26.97, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.98, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
[2025-09-05 05:56:23,637] [INFO] [logging.py:107:log_dist] [Rank 0] step=20, skipped=0, lr=[3.7500000000000005e-06, 3.7500000000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2025-09-05 05:56:23,648] [INFO] [timer.py:264:stop] epoch=3/micro_step=11/global_step=20, RunningAvgSamplesPerSec=26.064030679254405, CurrSamplesPerSec=25.657540201631182, MemAllocated=12.32GB, MaxMemAllocated=19.51GB
Model Parameters: 0.596 B, Latency: 0.36s, TFLOPs: 45.60, Samples/sec: 22.35, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.95, Samples/sec: 26.93, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.98, Samples/sec: 26.94, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.06, Samples/sec: 26.98, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 50.99, Samples/sec: 24.99, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.00, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.00, Samples/sec: 26.96, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.74, Samples/sec: 23.89, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.06, Samples/sec: 25.03, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.04, Samples/sec: 26.97, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.95, Samples/sec: 26.93, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.82, Samples/sec: 23.92, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.19s, TFLOPs: 86.96, Samples/sec: 42.61, Time/seq 0.02s, Batch Size: 8, Sequence Length: 512
***** Evaluating perplexity, Epoch 4/10 *****
ppl: 12.582889556884766, loss: 2.5323379039764404
Beginning of Epoch 5/10, Total Micro Batches 23
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.03, Samples/sec: 26.97, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.74, Samples/sec: 23.88, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.79, Samples/sec: 23.91, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.00, Samples/sec: 24.99, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.98, Samples/sec: 26.94, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.02, Samples/sec: 26.96, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.73, Samples/sec: 23.88, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.07, Samples/sec: 25.03, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.00, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.96, Samples/sec: 26.94, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.74, Samples/sec: 23.89, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.02, Samples/sec: 25.00, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.01, Samples/sec: 26.96, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.96, Samples/sec: 26.94, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.98, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.03, Samples/sec: 25.01, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.98, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.76, Samples/sec: 23.89, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.02, Samples/sec: 26.96, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.01, Samples/sec: 25.00, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.98, Samples/sec: 26.94, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.77, Samples/sec: 23.90, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.16s, TFLOPs: 99.01, Samples/sec: 48.52, Time/seq 0.02s, Batch Size: 8, Sequence Length: 512
***** Evaluating perplexity, Epoch 5/10 *****
ppl: 12.399747848510742, loss: 2.5176761150360107
Beginning of Epoch 6/10, Total Micro Batches 23
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.07, Samples/sec: 25.03, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.76, Samples/sec: 23.90, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.78, Samples/sec: 23.90, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.92, Samples/sec: 26.91, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
[2025-09-05 05:56:36,523] [INFO] [logging.py:107:log_dist] [Rank 0] step=30, skipped=0, lr=[2.5e-06, 2.5e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2025-09-05 05:56:36,534] [INFO] [timer.py:264:stop] epoch=5/micro_step=5/global_step=30, RunningAvgSamplesPerSec=26.11539068088054, CurrSamplesPerSec=24.891710905458485, MemAllocated=12.32GB, MaxMemAllocated=19.51GB
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 50.99, Samples/sec: 24.99, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.97, Samples/sec: 26.94, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.75, Samples/sec: 23.89, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.00, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.02, Samples/sec: 25.00, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.00, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.76, Samples/sec: 23.90, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.00, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.02, Samples/sec: 25.00, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.02, Samples/sec: 26.97, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.95, Samples/sec: 26.93, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.01, Samples/sec: 26.96, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.04, Samples/sec: 25.01, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.77, Samples/sec: 23.90, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.99, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.99, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.01, Samples/sec: 25.00, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.76, Samples/sec: 23.90, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.16s, TFLOPs: 98.96, Samples/sec: 48.50, Time/seq 0.02s, Batch Size: 8, Sequence Length: 512
***** Evaluating perplexity, Epoch 6/10 *****
ppl: 12.272584915161133, loss: 2.5073678493499756
Beginning of Epoch 7/10, Total Micro Batches 23
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.99, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.36s, TFLOPs: 45.62, Samples/sec: 22.36, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.75, Samples/sec: 23.89, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.96, Samples/sec: 26.94, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.99, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.02, Samples/sec: 25.00, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.73, Samples/sec: 23.88, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.99, Samples/sec: 26.95, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 54.95, Samples/sec: 26.93, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.01, Samples/sec: 25.00, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.90, Samples/sec: 23.96, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.15, Samples/sec: 27.03, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.20, Samples/sec: 27.05, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.10, Samples/sec: 25.05, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.17, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.15, Samples/sec: 27.03, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.15, Samples/sec: 27.03, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.36s, TFLOPs: 45.78, Samples/sec: 22.43, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.16, Samples/sec: 27.03, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.16, Samples/sec: 27.03, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.17, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
[2025-09-05 05:56:49,308] [INFO] [logging.py:107:log_dist] [Rank 0] step=40, skipped=0, lr=[1.2500000000000007e-06, 1.2500000000000007e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2025-09-05 05:56:49,319] [INFO] [timer.py:264:stop] epoch=6/micro_step=22/global_step=40, RunningAvgSamplesPerSec=26.07140725572805, CurrSamplesPerSec=25.73448604255033, MemAllocated=12.32GB, MaxMemAllocated=19.51GB
Model Parameters: 0.596 B, Latency: 0.36s, TFLOPs: 45.72, Samples/sec: 22.41, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.16s, TFLOPs: 99.21, Samples/sec: 48.62, Time/seq 0.02s, Batch Size: 8, Sequence Length: 512
***** Evaluating perplexity, Epoch 7/10 *****
ppl: 12.213483810424805, loss: 2.5025405883789062
Beginning of Epoch 8/10, Total Micro Batches 23
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.24, Samples/sec: 27.07, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.94, Samples/sec: 23.98, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.36s, TFLOPs: 45.74, Samples/sec: 22.41, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.18, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.19, Samples/sec: 27.05, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.13, Samples/sec: 27.02, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.36s, TFLOPs: 45.78, Samples/sec: 22.44, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.19, Samples/sec: 27.05, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.17, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.18, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.36s, TFLOPs: 45.75, Samples/sec: 22.42, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.20, Samples/sec: 27.05, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.21, Samples/sec: 27.06, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.13, Samples/sec: 27.02, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.21, Samples/sec: 25.10, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.19, Samples/sec: 27.05, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.17, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.95, Samples/sec: 23.99, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.18, Samples/sec: 25.08, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.21, Samples/sec: 27.05, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.17, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.93, Samples/sec: 23.98, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.19s, TFLOPs: 86.87, Samples/sec: 42.57, Time/seq 0.02s, Batch Size: 8, Sequence Length: 512
***** Evaluating perplexity, Epoch 8/10 *****
ppl: 12.211627006530762, loss: 2.5023884773254395
Beginning of Epoch 9/10, Total Micro Batches 23
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.24, Samples/sec: 27.07, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.92, Samples/sec: 23.97, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.93, Samples/sec: 23.98, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.19, Samples/sec: 25.08, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.19, Samples/sec: 27.05, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.13, Samples/sec: 27.02, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.94, Samples/sec: 23.98, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.16, Samples/sec: 25.07, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.14, Samples/sec: 27.02, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.18, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.91, Samples/sec: 23.97, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.21, Samples/sec: 25.10, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.20, Samples/sec: 27.05, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.12, Samples/sec: 27.01, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.20, Samples/sec: 27.05, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
[2025-09-05 05:57:02,156] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=0, lr=[3.3493649053890325e-07, 3.3493649053890325e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2025-09-05 05:57:02,167] [INFO] [timer.py:264:stop] epoch=8/micro_step=16/global_step=50, RunningAvgSamplesPerSec=26.117631603105945, CurrSamplesPerSec=26.545591539855433, MemAllocated=12.32GB, MaxMemAllocated=19.51GB
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.15, Samples/sec: 25.07, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.17, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.96, Samples/sec: 23.99, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.15, Samples/sec: 27.03, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.18, Samples/sec: 25.08, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.15, Samples/sec: 27.03, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.92, Samples/sec: 23.97, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.17s, TFLOPs: 97.66, Samples/sec: 47.86, Time/seq 0.02s, Batch Size: 8, Sequence Length: 512
***** Evaluating perplexity, Epoch 9/10 *****
ppl: 12.220597267150879, loss: 2.5031228065490723
Beginning of Epoch 10/10, Total Micro Batches 23
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.19, Samples/sec: 25.09, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.92, Samples/sec: 23.97, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.93, Samples/sec: 23.98, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.19, Samples/sec: 27.05, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.19, Samples/sec: 25.09, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.17, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.96, Samples/sec: 23.99, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.16, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.19, Samples/sec: 25.09, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.17, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.92, Samples/sec: 23.97, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.21, Samples/sec: 27.06, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.15, Samples/sec: 25.07, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.18, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.18, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.17, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.21, Samples/sec: 25.10, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.96, Samples/sec: 24.00, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.17, Samples/sec: 27.04, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.30s, TFLOPs: 55.19, Samples/sec: 27.05, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.32s, TFLOPs: 51.20, Samples/sec: 25.09, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.33s, TFLOPs: 48.93, Samples/sec: 23.98, Time/seq 0.04s, Batch Size: 8, Sequence Length: 512
Model Parameters: 0.596 B, Latency: 0.16s, TFLOPs: 99.16, Samples/sec: 48.60, Time/seq 0.02s, Batch Size: 8, Sequence Length: 512
***** Evaluating perplexity, Epoch 10/10 *****
ppl: 12.219796180725098, loss: 2.5030572414398193
saving the final model ...
[rank0]:[W905 05:57:15.179821358 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[2025-09-05 05:57:16,920] [INFO] [launch.py:351:main] Process 8965 exits successfully.
