[
    {
        "prompt": "summary: Efficient generation of 3D digital humans is important\nin several industries, including virtual reality, social media,\nand cinematic production. 3D generative adversarial net-\nworks (GANs) have demonstrated state-of-the-art (SOTA)\nquality and diversity for generated assets. Current 3D GAN\narchitectures, however, typically rely on volume representa-\ntions, which are slow to render, thereby hampering the GAN\ntraining and requiring multi-view-inconsistent 2D upsam-\nplers. Here, we introduce Gaussian Shell Maps (GSMs) as\na framework that connects SOTA generator network archi-\ntectures with emerging 3D Gaussian rendering primitives\nusing an articulable multi shell–based scaffold. In this set-\nting, a CNN generates a 3D texture stack with features that\nare mapped to the shells. The latter represent inflated and\ndeflated versions of a template surface of a digital human\nin a canonical body pose. Instead of rasterizing the shells\ndirectly, we sample 3D Gaussians on the shells whose at-\ntributes are encoded in the texture features. These Gaus-\nsians are efficiently and differentiably rendered. The ability\nto articulate the shells is important during GAN training\nand, at inference time, to deform a body into arbitrary user-\ndefined poses. Our efficient rendering scheme bypasses the\nneed for view-inconsistent upsamplers and achieves high-\nquality multi-view consistent renderings at a native resolu-\ntion of 512 × 512 pixels. We demonstrate that GSMs suc-\ncessfully generate 3D humans when trained on single-view\ndatasets, including SHHQ and DeepFashion.\nProject Page: rameenabdal.github.io/GaussianShellMaps\n1. Introduction\nThe ability to generate articulable three-dimensional dig-\nital humans augments traditional asset creation and anima-\ntion workflows, which are laborious and costly. Such gener-\native artificial intelligence–fueled workflows are crucial in\nseveral applications, including communication, cinematic\nproduction, and interactive gaming, among others.\n3D Generative Adversarial Networks (GANs) have\nemerged as the state-of-the-art (SOTA) platform in this do-\nmain, enabling the generation of diverse 3D assets at inter-\nactive framerates [6, 8, 15, 23, 41, 71, 76, 76]. Most ex-\nisting 3D GANs build on variants of volumetric scene rep-\nresentations combined with neural volume rendering [65].\nHowever, volume rendering is relatively slow and compro-\nmises the training of a GAN, which requires tens of millions\nof forward rendering passes to converge [11]. Mesh–based\nrepresentations building on fast differentiable rasterization\nhave been proposed to overcome this limitation [19, 71],\n* Equal Contribution\n† Work done as a visiting student researcher at Stanford University\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n9441",
        "chosen": "Gaussian Shell Maps (GSMs) are introduced to enhance 3D human generation efficiency. This framework combines generator network architectures with 3D Gaussian rendering primitives using an articulable multi-shell scaffold. A CNN generates a 3D texture stack, mapping features to shells representing inflated/deflated versions of a template surface. Instead of direct rasterization, 3D Gaussians are sampled on the shells, with attributes encoded in texture features. This method enables efficient, differentiable rendering, bypassing the need for view-inconsistent upsamplers and achieving high-quality multi-view consistent renderings at 512 × 512 pixels. GSMs successfully generate 3D humans when trained on single-view datasets like SHHQ and DeepFashion.",
        "rejected": "Efficient generation of 3D digital humans using Gaussian Shell Maps that connect generator networks with 3D Gaussian rendering primitives. This approach allows for fast, differentiable rendering and supports multi-view consistent outputs at high resolutions without requiring upsamplers."
    },
    {
        "prompt": "summary: Quantifying the degree of similarity between images is a\nkey copyright issue for image-based machine learning. In\nlegal doctrine however, determining the degree of similarity\nbetween works requires subjective analysis, and fact-ﬁnders\n(judges and juries) can demonstrate considerable variabil-\nity in these subjective judgement calls.\nImages that are\nstructurally similar can be deemed dissimilar, whereas im-\nages of completely different scenes can be deemed similar\nenough to support a claim of copying. We seek to deﬁne and\ncompute a notion of ‘conceptual similarity’ among images\nthat captures high-level relations even among images that\ndo not share repeated elements or visually similar compo-\nnents. The idea is to use a base multi-modal model to gen-\nerate ‘explanations’ (captions) of visual data at increasing\nlevels of complexity. Then, similarity can be measured by\nthe length of the caption needed to discriminate between\nthe two images: Two highly dissimilar images can be dis-\ncriminated early in their description, whereas conceptually\ndissimilar ones will need more detail to be distinguished.\nWe operationalize this deﬁnition and show that it correlates\nwith subjective (averaged human evaluation) assessment,\nand beats existing baselines on both image-to-image and\ntext-to-text similarity benchmarks. Beyond just providing a\nnumber, our method also offers interpretability by pointing\nto the speciﬁc level of granularity of the description where\nthe source data are differentiated.\n1. Introduction\nConsider the two images in Fig. 1. One could say they are\nsimilar: both portray small red Italian cars. Another could\nsay they are different: One is a sports car in an open space,\nthe other a tiny city car in an alley. Is there an objective way\nof measuring the similarity among images? In some cases,\nsimilarity judgments can be inﬂuenced by shared concepts:\nin Fig. 2, two images share compelling stylistic and concep-\ntual similarity, but it is difﬁcult to identify speciﬁc visual\nelements they share. Yet the two were found to be legally\n“substantially similar” [37]. Can we deﬁne an objective no-\ntion of ‘conceptual similarity’ among data?\nThere have been many attempts at deﬁning an objective\nnotion of similarity based on the number of bits of infor-\nmation that the two samples share [11, 25, 40], but funda-\nmentally they do not capture concepts (Appendix A), which\nare human constructs. Since humans must play a role in\ndeﬁning similarity among concepts, one way to achieve ob-\njectivity is by averaging a large number of human assess-\nments. This is what large-scale neural network models do\n[34]. However, contrastive-trained models measure similar-\nity based on how easily the data can be distinguished, and\nany two non-identical samples can be distinguished by ran-\ndom features of no conceptual relevance [22].\nRather than focusing on ﬁnding the features that best dis-\ntinguish the two images, we focus on ﬁnding the ones that\nbest describe them. Then, we can measure semantic sim-\nilarity based on how well descriptions of one ﬁt the other.\nIf two samples are similar, a short description should apply\nequally well to both. If the samples are very different, the\ndescription of one will be a poor ﬁt for the other. The more\ncomplex the description needed to distinguish the images,\nthe higher their conceptual similarity. The key idea is to fo-\ncus on optimally describing individual samples, rather than\nadversarially discriminating them.\nSpeciﬁcally, referring to Fig. 1, we generate multiple de-\nscriptions of each sample in increasing level of complexity,\nsorted by their coding length. Then, we measure the dif-\nference of the likelihood of each image conditioned on ei-\nther descriptions as a function of complexity. That traces\na curve that measures distance as a function of complexity.\nAny two images can be distinguished by their description\nat some point (the more you look, the more differences you\nsee). Therefore, similarity should always be relative to a de-\nscription length. However, when a single number is needed\nfor comparison, we show that the AUC of the distance func-\ntion is well aligned with human similarity assessments.\nOur proposed method, which we call Complexity-\nConstrained Descriptive Autoencoding, or CC:DAE, is\nrooted in the idea of the Kolmogorov Structure Function,\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n11062",
        "chosen": "A novel method called Complexity-Constrained Descriptive Autoencoding (CC:DAE) is proposed to define and compute 'conceptual similarity' among images. This method uses a base multi-modal model to generate 'explanations' (captions) of visual data at increasing complexity levels. Similarity is measured by the caption length needed to discriminate between two images. CC:DAE correlates with subjective human assessments and outperforms existing baselines on image-to-image and text-to-text similarity benchmarks. It also offers interpretability by indicating the description granularity level where source data differentiation occurs.",
        "rejected": "This work introduces a method to quantify 'conceptual similarity' among images by generating explanations of increasing complexity. The length of these descriptions needed to differentiate between two images defines their similarity, providing an interpretable metric aligned with human assessments."
    },
    {
        "prompt": "summary: Vision-Language Models (VLMs) such as CLIP are\ntrained on large amounts of image-text pairs, resulting\nin remarkable generalization across several data distribu-\ntions.\nHowever, in several cases, their expensive train-\ning and data collection/curation costs do not justify the\nend application. This motivates a vendor-client paradigm,\nwhere a vendor trains a large-scale VLM and grants only\ninput-output access to clients on a pay-per-query basis in\na black-box setting. The client aims to minimize inference\ncost by distilling the VLM to a student model using the\nlimited available task-specific data, and further deploying\nthis student model in the downstream application. While\nnaive distillation largely improves the In-Domain (ID) ac-\ncuracy of the student, it fails to transfer the superior out-\nof-distribution (OOD) generalization of the VLM teacher\nusing the limited available labeled images.\nTo mitigate\nthis, we propose Vision-Language to Vision - Align, Dis-\ntill, Predict (VL2V-ADiP), which first aligns the vision and\nlanguage modalities of the teacher model with the vision\nmodality of a pre-trained student model, and further distills\nthe aligned VLM representations to the student. This max-\nimally retains the pre-trained features of the student, while\nalso incorporating the rich representations of the VLM im-\nage encoder and the superior generalization of the text em-\nbeddings. The proposed approach achieves state-of-the-art\nresults on the standard Domain Generalization benchmarks\nin a black-box teacher setting as well as a white-box setting\nwhere the weights of the VLM are accessible. Project page:\nhttp://val.cds.iisc.ac.in/VL2V-ADiP/\n1. Introduction\nWhile the initial success of Deep Learning was predomi-\nnantly driven by training specialized models for each task\nor dataset [26, 29], recent research on foundation models\n[24, 34, 47, 65] eliminates the need for this by training\ngeneric models jointly over several modalities using large-\n*Equal Contribution.\nCorrespondence to Sravanti Addepalli <sravantia@iisc.ac.in>, Ashish\nRamayee Asokan <ashish.ramayee@gmail.com>\nFigure 1. Schematic diagram showing class and domain distri-\nbutions in the shared text/ image embedding space of a VLM:\nVLMs learn highly specialized image representations that are not\ndomain invariant. Thus, a linear classifier (red decision bound-\nary) that is trained over the vision encoder using limited training\ndata cannot generalize well to the target domain (shown in pur-\nple). On the other hand, generic text embeddings such as “A photo\nof a class” represent the core concept of a class by virtue of their\ntraining method and vast training data. Thus, they generalize ef-\nfectively across domains, and a zero-shot classifier (green decision\nboundary) aligns better with the true distribution of classes.\nscale data. The use of both image and language modal-\nities in large-scale Vision-Language Models (VLMs) en-\nables their use in several applications including zero-shot\nclassification, where the embedding of an image is com-\npared with text embeddings for “a photo of a {class}” cor-\nresponding to every class during inference and the class\nwith highest similarity is predicted. For example, the LiT\n(private) model [65] has been trained on 4 billion image-text\npairs scraped from the web, and it achieves 85.2% zero-shot\naccuracy on ImageNet. VLMs also demonstrate extraordi-\nnary performance across several distributions, owing to the\nvast diversity of distributions seen during their training [47].\nThe remarkable OOD generalization of VLMs makes\nthem suitable for use in several applications, either directly\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n23922",
        "chosen": "VL2V-ADiP is introduced to improve domain generalization in image classification by leveraging Vision-Language Models (VLMs). This method first aligns the vision and language modalities of a VLM teacher with a pre-trained student model's vision modality, then distills the aligned VLM representations to the student. This approach retains the student's pre-trained features while incorporating the rich representations of the VLM image encoder and superior generalization of text embeddings. VL2V-ADiP achieves state-of-the-art results on standard Domain Generalization benchmarks in both black-box and white-box teacher settings.",
        "rejected": "Vision-Language Models trained on large datasets achieve remarkable generalization but can be costly. A proposed distillation method called VL2V-ADiP aligns and transfers knowledge from a VLM teacher to a student model, maintaining pre-trained features while incorporating rich representations for improved out-of-distribution performance."
    },
    {
        "prompt": "summary: Scale-ambiguity in 3D scene dimensions leads to\nmagnitude-ambiguity of volumetric densities in neural ra-\ndiance ﬁelds, i.e., the densities double when scene size is\nhalved, and vice versa. We call this property alpha invari-\nance. For NeRFs to better maintain alpha invariance, we\nrecommend 1) parameterizing both distance and volume\ndensities in log space, and 2) a discretization-agnostic ini-\ntialization strategy to guarantee high ray transmittance. We\nrevisit a few popular radiance ﬁeld models and ﬁnd that\nthese systems use various heuristics to deal with issues aris-\ning from scene scaling. We test their behaviors and show\nour recipe to be more robust. Visit our project page at\nhttps://pals.ttic.edu/p/alpha-invariance.\n1. Introduction\n3D computer graphics and vision are fundamentally scale-\nambiguous. Lengths of objects and scenes are often unitless\nand measure up to a constant ratio. This is ﬁne for routines\nsuch as projection, triangulation, and camera motion estima-\ntion. A common practice is to normalize the scene dimension\nor the size of a reference object to an arbitrary number.\nVolume rendering [17] used by Neural Radiance Fields\n(NeRFs) [20] presents a complication due to explicit inte-\ngration over space. Since the distance scaling is arbitrary,\nthe volumetric density function σ(x) must compensate for\nthe multiplicative factor to render the same ﬁnal RGB color.\nIn other words, if the scene size expands by a factor k, it is\nsufﬁcient for the learned σ to shrink by 1/k to be invariant\nto the change. The same applies to integration over cones\nor more general spatial data structures [1, 3]. Note that in\naddition to scene dimensions, the magnitude of σ is also\ninﬂuenced by the number of samples per ray and ultimately\nthe sharpness of change in local opacity.\nThere is no single “correct” size for a scene setup. A\n* Equal contribution.\nFigure 1. A discretized view of volume rendering. Top: a ray is cut\ninto intervals, each with a density σi ≥0 and interval length di.\nBottom: illustration of the weight given to the 3rd interval, com-\nputed through alpha compositing. The rendered color is obtained\nby weighting all the interval colors with their wis. If we scale each\ndi by a constant k, scaling σi by 1\nk renders the identical color.\nrobust algorithm should be able to perform consistently\nacross different scalings. We investigate how this notion\nof invariance manifests itself in practice, discuss how the hy-\nperparameter decisions affect it, and propose a solution that\nensures robustness to distance scaling across the NeRF meth-\nods. We revisit and experiment with a few popular NeRF\narchitectures: Vanilla NeRF [20], TensoRF [6], DVGO [30],\nPlenoxels [8], and Nerfacto [31], and ﬁnd that many systems\nuse tailored heuristics that work well at a particular scene\nsize. We analyze the impact of some critical hyperparameters\nwhich are often overlooked or not highlighted in the original\npapers.\nIn our testing of these models we identify two main fail-\nure modes. When the scene is scaled down (short ray interval\nd), some models struggle to produce large enough σ values\nfor solid geometry. When the scene is scaled up (long inter-\nval length), the σ at initialization is often too large, resulting\nin cloudiness that traps the optimization at bad local optima.\nWe therefore propose 1) parameterizing both distance and\nvolume densities in log space for easier multiplicative scal-\ning; 2) a closed-form formula for σ value initialization that\nguarantees high ray transmittance and scene transparency.\nWe show that these two ingredients can robustly handle vari-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n20396",
        "chosen": "This paper addresses alpha invariance in Neural Radiance Fields (NeRFs), where scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of volumetric densities. To maintain alpha invariance, the authors recommend parameterizing both distance and volume densities in log space and using a discretization-agnostic initialization strategy for high ray transmittance. They revisit popular radiance field models, finding varied heuristics for handling scene scaling issues. Testing shows their approach is more robust across different NeRF methods and scene scalings.",
        "rejected": "Addressing scale-ambiguity in neural radiance fields, this paper recommends parameterizing distance and volume densities in log space for robustness. An initialization strategy ensuring high ray transmittance is also proposed, enhancing consistency across different scene scalings."
    },
    {
        "prompt": "summary: Unsupervised (US) video anomaly detection (VAD) in\nsurveillance applications is gaining more popularity re-\ncently due to its practical real-world applications.\nAs\nsurveillance videos are privacy sensitive and the availabil-\nity of large-scale video data may enable better US-VAD sys-\ntems, collaborative learning can be highly rewarding in this\nsetting. However, due to the extremely challenging nature\nof the US-VAD task, where learning is carried out without\nany annotations, privacy-preserving collaborative learning\nof US-VAD systems has not been studied yet. In this pa-\nper, we propose a new baseline for anomaly detection ca-\npable of localizing anomalous events in complex surveil-\nlance videos in a fully unsupervised fashion without any la-\nbels on a privacy-preserving participant-based distributed\ntraining configuration. Additionally, we propose three new\nevaluation protocols to benchmark anomaly detection ap-\nproaches on various scenarios of collaborations and data\navailability. Based on these protocols, we modify existing\nVAD datasets to extensively evaluate our approach as well\nas existing US SOTA methods on two large-scale datasets\nincluding UCF-Crime and XD-Violence. All proposed eval-\nuation protocols, dataset splits, and codes are available\nhere: https://github.com/AnasEmad11/CLAP.\n1. Introduction\nRecent years have seen a surge in federated learning based\nmethods, where the goal is to enable collaborative training\nof machine learning models without transferring any train-\ning data to a central server. This direction of research in ma-\nchine learning is of notable importance as it enables learn-\ning with multiple participants that can contribute data with-\nout compromising privacy. Several researchers have studied\nfederated learning for different applications such as medical\ndiagnosis [1, 3, 6, 22], network security [10, 19, 21, 30], and\nlarge-scale classification models [4, 14, 42].\nAnomaly detection in surveillance videos, being one\n...\nCentral\nParameter Sharing\nServer\nServer\nConventional Centralized Training\nOurs: Collaborative Training\n...\n...\n...\nFigure 1. a) Conventional central training requires all training data\nto be on the server to carry out the training. This setting cannot en-\nsure privacy, thus hindering collaborations between different enti-\nties holding large-scale surveillance data. b) Our proposed unsu-\npervised video anomaly detection technique does not require the\ntransfer of training data between the server and participants, thus\nensuring complete privacy.\nof the large-scale applications of computer vision, may\ngreatly benefit from autonomous collaborative training al-\ngorithms.\nVAD in surveillance videos is privacy sensi-\ntive and may involve data belonging to several organiza-\ntions/establishments. This may result in hectic bureaucratic\nprocesses to obtain data from each establishment for cen-\ntralized training. For example, the police department of a\ncity may not be willing to share street surveillance videos\ndue to privacy concerns of the general public, or a daycare\nfacility may have to obtain the consent of all parents to be\nallowed to share its CCTV footage. Such restrictions may\nhinder the possibility of obtaining large-scale data to train\nefficient anomaly detectors making a central training requir-\ning all training data the least preferred option in the real-\nworld scenarios. Unfortunately, to the best of our knowl-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n12416",
        "chosen": "CLAP is proposed as a new baseline for unsupervised video anomaly detection in surveillance, capable of localizing anomalies without labels in a privacy-preserving distributed training setup. The authors also introduce three new evaluation protocols to benchmark anomaly detection approaches across various collaboration scenarios and data availabilities. They modify existing VAD datasets to extensively evaluate their approach and existing US SOTA methods on large-scale datasets like UCF-Crime and XD-Violence. All evaluation protocols, dataset splits, and codes are made available.",
        "rejected": "An unsupervised video anomaly detection technique suitable for privacy-preserving collaborative learning is introduced. It localizes anomalies in surveillance videos without labels and proposes new evaluation protocols tested on large-scale datasets including UCF-Crime and XD-Violence."
    },
    {
        "prompt": "summary: This paper addresses complex challenges in histopatho-\nlogical image analysis through three key contributions.\nFirstly, it introduces a fast patch selection method, FPS,\nfor whole-slide image (WSI) analysis, significantly reduc-\ning computational cost while maintaining accuracy. Sec-\nondly, it presents PathDino, a lightweight histopathol-\nogy feature extractor with a minimal configuration of five\nTransformer blocks and only ≈9 million parameters,\nmarkedly fewer than alternatives. Thirdly, it introduces a\nrotation-agnostic representation learning paradigm using\nself-supervised learning, effectively mitigating overfitting.\nWe also show that our compact model outperforms existing\nstate-of-the-art histopathology-specific vision transformers\non 12 diverse datasets, including both internal datasets\nspanning four sites (breast, liver, skin, and colorectal) and\nseven public datasets (PANDA, CAMELYON16, BRACS,\nDigestPath, Kather, PanNuke, and WSSS4LUAD). Notably,\neven with a training dataset of ≈6 million histopathol-\nogy patches from The Cancer Genome Atlas (TCGA), our\napproach demonstrates an average 8.5% improvement in\npatch-level majority vote performance. These contributions\nprovide a robust framework for enhancing image analysis\nin digital pathology, rigorously validated through extensive\nevaluation. 1\n1. Introduction\nThe advent of whole slide image (WSI) scanning in dig-\nital pathology has revolutionized the research in compu-\ntational pathology [1–3]. While digital pathology enables\nboth researchers and clinicians to enjoy the ease of access\nto the WSIs, processing and storing these gigapixel images\nare still quite challenging.\nMotivation: Large image size and scarce or lack of patch-\nlevel labels (annotations) pose two main challenges in\nWSI analysis [4]. As a result, most state-of-the-art meth-\n*Corresponding author\n1The project page:\nhttps://KimiaLabMayo.github.io/\nPathDino-Page/\nFigure 1. HistoRotate. A 360◦rotation augmentation for\ntraining models on histopathology images. Unlike training on nat-\nural images where the rotation may change the context of the vi-\nsual data, rotating a histopathology image improves the learning\nprocess for discriminative embedding learning.\nods adopt Multi-instance Learning (MIL) with weak su-\npervision [5–13]. While these approaches may eliminate\nthe need for pixel-level annotations, MIL significantly in-\ncreases computational loads and potentially lowers the qual-\nity of results compared to fully supervised approaches.\nWhile some attempts have been made to select representa-\ntive patches [5,6,14,15], many such methods remain com-\nputationally intensive, leaving the desire for efficient, accu-\nrate solutions an unmet need.\nThe field of image analysis in digital pathology has\npredominantly adopted deep models designed for natural\nimage analysis without further customization to the field\n[16–19]. While showing good performance on natural im-\nage analysis, pre-trained deep models may not fully ex-\nploit the unique characteristics of histopathology images.\nFurthermore, most current training recipes for histopatho-\nlogical embedding learning adopt conventional training and\ncommon augmentation techniques for natural images [18].\nHowever, histopathology images have arguably very differ-\nent features compared to natural images and even radiology\nimages. This gap motivated us to design an improved train-\ning approach for histopathology images.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n11683",
        "chosen": "This paper presents three key contributions for histopathological image analysis: a fast patch selection method (FPS) for whole-slide image (WSI) analysis, PathDino (a lightweight histopathology feature extractor with five Transformer blocks and ≈9 million parameters), and a rotation-agnostic representation learning paradigm using self-supervised learning. The compact model outperforms existing state-of-the-art histopathology-specific vision transformers on 12 diverse datasets, including internal and public datasets. It shows an average 8.5% improvement in patch-level majority vote performance with a training dataset of ≈6 million histopathology patches from TCGA.",
        "rejected": "A framework for histopathological image analysis through a fast patch selection method (FPS), PathDino feature extractor, and rotation-agnostic representation learning reduces computational costs and overfitting. The compact model outperforms state-of-the-art alternatives on various datasets, demonstrating significant improvements in accuracy."
    },
    {
        "prompt": "summary: Image and video analysis requires not only accurate ob-\nject detection but also the understanding of relationships\namong detected objects. Common solutions to relation mod-\neling typically resort to stand-alone object detectors fol-\nlowed by non-differentiable post-processing techniques. Re-\ncently introduced detection transformers (DETR) perform\nend-to-end object detection based on a bipartite matching\nloss. Such methods, however, lack the ability to jointly detect\nobjects and resolve object associations. In this paper, we\nbuild on the DETR approach and extend it to the joint de-\ntection of objects and their relationships by introducing an\napproximated bipartite matching. While our method can gen-\neralize to an arbitrary number of objects, we here focus on\nthe modeling of object pairs and their relations. In particular,\nwe apply our method PairDETR to the problem of detect-\ning human bodies and faces, and associating them for the\nsame person. Our approach not only eliminates the need for\nhand-designed post-processing but also achieves excellent\nresults for body-face associations. We evaluate PairDETR\non the challenging CrowdHuman and CityPersons datasets\nand demonstrate a large improvement over the state of the\nart. Our training code and pre-trained models are available\nat https://github.com/mts-ai/pairdetr\n1. Introduction\nThe detection of objects and their relationships is one of the\nkey challenges in computer vision. It involves the prediction\nof bounding boxes for given object categories and assigning\npairs of detected objects to particular relations.\nFor example, the detection of human faces and bodies, as\nwell as the association of them for the same person, is essen-\ntial for applications including human-computer interaction\n(gestures detection), gaming (in virtual reality), fitness and\nsport (from virtual coaching to automated match tracking),\nmass market (virtual try-on), and many others.\nPerson detection is a well-studied problem in computer\nvision. However, common approaches often focus on de-\ntecting human bodies, faces, and heads in isolation, without\nconsidering relationships among them. Given face and body\ndetections, typical systems resolve object relations based\non ad-hoc post-processing techniques, such as non-maxima\nsuppression followed by the matching of overlapping object\nbounding boxes.\nIn this work, we formulate the detection and association\nof object pairs as a graph prediction problem. While apply-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n423",
        "chosen": "PairDETR extends the DETR approach to jointly detect objects and resolve their associations by introducing an approximated bipartite matching. The method focuses on modeling object pairs and their relations, particularly for detecting human bodies and faces and associating them for the same person. PairDETR eliminates the need for hand-designed post-processing and achieves excellent results for body-face associations. It is evaluated on CrowdHuman and CityPersons datasets, demonstrating significant improvement over the state of the art.",
        "rejected": "Building upon DETR, PairDETR extends object detection to include relationship modeling via approximated bipartite matching. This end-to-end approach eliminates the need for post-processing steps and achieves superior results for detecting and associating human bodies and faces."
    },
    {
        "prompt": "summary: The reflective nature of the human eye is an underappre-\nciated source of information about what the world around\nus looks like. By imaging the eyes of a moving person, we\ncapture multiple views of a scene outside the camera’s di-\nrect line of sight through the reflections in the eyes. In this\npaper, we reconstruct a radiance field beyond the camera’s\nline of sight using portrait images containing eye reflec-\ntions. This task is challenging due to 1) the difficulty of ac-\ncurately estimating eye poses and 2) the entangled appear-\nance of the iris textures and the scene reflections. To address\nthese, our method jointly optimizes the cornea poses, the ra-\ndiance field depicting the scene, and the observer’s eye iris\ntexture. We further present a regularization prior on the iris\ntexture to improve scene reconstruction quality. Through\nvarious experiments on synthetic and real-world captures\nfeaturing people with varied eye colors, and lighting con-\nditions, we demonstrate the feasibility of our approach to\nrecover the radiance field using cornea reflections.\n*Equal contribution\n1. Introduction\nThe human eye is a remarkable organ that enables vi-\nsion and holds valuable information about the surrounding\nworld. While we typically use our own eyes as two lenses\nto focus light onto the photosensitive cells composing our\nretina, we would also capture the light reflected from the\ncornea if we look at someone else’s eyes. When we use\na camera to image the eyes of another, we effectively turn\ntheir eyes into a pair of mirrors in the overall imaging sys-\ntem. Since the light that reflects off the observer’s eyes\nshare the same source as the light that reaches their retina,\nour camera can form images containing information about\nthe surrounding world the observer sees.\nPrior studies have explored recovering a panoramic im-\nage of the world the observer sees and simple 3D struc-\ntures like boxes that the observer is looking at from man-\nually specified correspondences from a single image of two\neyes [27, 28]. Follow-up works have further explored ap-\nplications such as personal identification [10, 29], detecting\ngrasp posture [53], focused object estimation [40], illumina-\ntion estimation [48], and relighting [26]. Given the recent\nadvancements in 3D vision and graphics, we wonder: Can\nwe do more than reconstruct a single panoramic environ-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n4864",
        "chosen": "This paper reconstructs a radiance field beyond the camera’s line of sight using portrait images containing eye reflections. The method jointly optimizes cornea poses, the scene’s radiance field, and the observer’s eye iris texture, with a regularization prior on the iris texture for improved scene reconstruction quality. Experiments on synthetic and real-world captures show the approach’s feasibility in recovering the radiance field using cornea reflections, regardless of eye color and lighting conditions.",
        "rejected": "Reconstructing radiance fields beyond the camera's line of sight using eye reflections captured in portrait images. By optimizing cornea poses, radiance field, and iris texture, this method demonstrates feasibility in recovering scenes reflected in eyes under varied conditions."
    },
    {
        "prompt": "summary: We introduce FaceTalk1, a novel generative approach de-\nsigned for synthesizing high-fidelity 3D motion sequences\nof talking human heads from input audio signal. To capture\nthe expressive, detailed nature of human heads, including\nhair, ears, and finer-scale eye movements, we propose to\ncouple speech signal with the latent space of neural para-\nmetric head models to create high-fidelity, temporally co-\nherent motion sequences. We propose a new latent diffu-\nsion model for this task, operating in the expression space of\nneural parametric head models, to synthesize audio-driven\nrealistic head sequences. In the absence of a dataset with\ncorresponding NPHM expressions to audio, we optimize for\nthese correspondences to produce a dataset of temporally-\noptimized NPHM expressions fit to audio-video recordings\nof people talking.\nTo the best of our knowledge, this is\nthe first work to propose a generative approach for realis-\ntic and high-quality motion synthesis of volumetric human\nheads, representing a significant advancement in the field of\naudio-driven 3D animation. Notably, our approach stands\nout in its ability to generate plausible motion sequences that\ncan produce high-fidelity head animation coupled with the\n1Project Page:\nhttps://shivangi- aneja.github.io/\nprojects/facetalk\nNPHM shape space. Our experimental results substantiate\nthe effectiveness of FaceTalk, consistently achieving supe-\nrior and visually natural motion, encompassing diverse fa-\ncial expressions and styles, outperforming existing methods\nby 75% in perceptual user study evaluation.\n1. Introduction\nModeling 3D animation of humans has a wide range of\napplications in the realm of digital media, including an-\nimated movies, computer games, and virtual assistants.\nIn recent years, there have been numerous works propos-\ning generative approaches for motion synthesis of human\nbodies, enabling the animation of human skeletons condi-\ntioned on various signals such as action [3, 25, 46], lan-\nguage [1, 4, 36, 47, 64, 77] and music [2, 67]. While human\nfaces are critical to synthesis of humans, generative syn-\nthesis of 3D faces in motion has focused on 3D morphable\nmodels (3DMMs) leveraging linear blendshapes [7, 40] to\nrepresent head motion and expression. Such models char-\nacterize a disentangled space of head shape and motion,\nbut lack the capacity to comprehensively represent the com-\nplexity and fine-grained details of human face geometry in\nmotion (e.g., hair, skin furrowing during motion, etc.).\nThus, we propose to represent animated head sequences\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n21263",
        "chosen": "FaceTalk is a novel generative approach for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio. It couples speech signals with the latent space of neural parametric head models, using a new latent diffusion model in the expression space of these models. In the absence of a dataset with NPHM expressions corresponding to audio, the authors optimize for these correspondences. FaceTalk generates plausible motion sequences for high-fidelity head animation, outperforming existing methods by 75% in perceptual user study evaluation.",
        "rejected": "FaceTalk1 synthesizes high-fidelity 3D motion sequences of talking heads from audio signals using a latent diffusion model in expression space. It couples speech with neural parametric head models to create realistic, temporally coherent sequences optimized from audio-video data."
    },
    {
        "prompt": "summary: Learning representations to capture the very fundamental\nunderstanding of the world is a key challenge in machine\nlearning. The hierarchical structure of explanatory factors\nhidden in data is such a general representation and could be\npotentially achieved with a hierarchical VAE. However, train-\ning a hierarchical VAE always suffers from the “posterior\ncollapse”, where the data information is hard to propagate\nto the higher-level latent variables, hence resulting in a bad\nhierarchical representation. To address this issue, we first an-\nalyze the shortcomings of existing methods for mitigating the\nposterior collapse from an information theory perspective,\nthen highlight the necessity of regularization for explicitly\npropagating data information to higher-level latent variables\nwhile maintaining the dependency between different levels.\nThis naturally leads to formulating the inference of the hi-\nerarchical latent representation as a sequential decision\nprocess, which could benefit from applying reinforcement\nlearning (RL). Aligning RL’s objective with the regulariza-\ntion, we first introduce a skip-generative path to acquire\na reward for evaluating the information content of an in-\nferred latent representation, and then the developed Q-value\nfunction based on it could have a consistent optimization\ndirection of the regularization. Finally, policy gradient, one\nof the typical RL methods, is employed to train a hierarchical\nVAE without introducing a gradient estimator. Experimental\nresults firmly support our analysis and demonstrate that our\nproposed method effectively mitigates the posterior collapse\nissue, learns an informative hierarchy, acquires explainable\nlatent representations, and significantly outperforms other\nhierarchical VAE-based methods in downstream tasks.\n1. Introduction\nDeriving meaningful representations of data with mini-\nmal supervision is a central challenge in machine learn-\n*Equal contribution, authors listed alphabetically by last name.\n†Corresponding to: Yewen Li <yewen001@e.ntu.edu.sg>.\nx\nz1\nz2\nz3\nz4\nz5\nHVAE\nx\nz1\nz2\nz3\nz4\nz5\nIVAE\nHVAE\nIVAE\nHierarchy level→\nHierarchy level→\nFigure 1. Visualization of different levels’ latent representations of\nhierarchical VAEs on FashinMNIST and CelebA [53] to demon-\nstrate the learned hierarchical structure. HVAE fails to learn a\n5-level hierarchy, where higher-level latent representations display\nposterior collapse, collapsing into the same modes of “clothing”\nand “female faces” regardless of the input. For IVAE, reconstruc-\ntions of certain consecutive layers appear highly similar, suggesting\na disrupted hierarchy. In contrast, our method preserves detailed\ninformation of inputs at low levels and captures increasingly ab-\nstract semantics at higher levels, mitigating posterior collapse and\nestablishing an informative hierarchy.\ning [5], while existing research has predominantly concen-\ntrated on the discriminative approach [10] that relies on\nmeticulously crafted preprocessing pipelines like pretext\ntasks [22, 30, 62, 65, 95] and data augmentations [3, 59].\nMethods such as contrastive learning [8, 9, 11, 12, 14–\n16, 27, 29, 32, 34, 36, 49, 87, 97, 98] exemplify the success\nof this discriminative approach. However, representations\nderived from these methods are only limited to tasks invari-\nant to the preprocessing pipelines [24], e.g. representations\nlearned with the random cropping data augmentation cannot\nbe applied to pixel-level localization tasks [83, 84], limit-\ning their broader applicability. To transcend these confines,\nBengio et al. [5] advocate for the pursuit of a universal and\nfundamental understanding of the world-a “general-purpose\nprior”-enabling the direct learning of representations with-\nout prior knowledge or assumptions about downstream tasks.\nSuch representations could be learned by generative ap-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n22946",
        "chosen": "A method combining reinforcement learning (RL) is proposed to improve unsupervised hierarchical representation learning in hierarchical VAEs, addressing the posterior collapse issue. The approach involves analyzing existing methods' shortcomings, emphasizing regularization for data information propagation to higher-level latent variables, and formulating hierarchical latent representation inference as a sequential decision process. RL is applied using a skip-generative path for reward acquisition and a Q-value function for consistent optimization. Policy gradient trains the hierarchical VAE without introducing a gradient estimator. Experimental results show the method effectively mitigates posterior collapse, learns informative hierarchies, and significantly outperforms other hierarchical VAE-based methods in downstream tasks.",
        "rejected": "Innovative methods in generating 3D digital humans, measuring image similarity, improving Vision-Language Models, addressing scale ambiguities, detecting anomalies, analyzing histopathological images, extending object detection, reconstructing scenes, and synthesizing head motions are highlighted. Each addresses specific challenges within its domain."
    },
    {
        "prompt": "summary: Human perception and understanding is a major domain\nof computer vision which, like many other vision subdo-\nmains recently, stands to gain from the use of large mod-\nels pre-trained on large datasets. We hypothesize that the\nmost common pre-training strategy of relying on general\npurpose, object-centric image datasets such as ImageNet,\nis limited by an important domain shift.\nOn the other\nhand, collecting domain-specific ground truth such as 2D\nor 3D labels does not scale well. Therefore, we propose\na pre-training approach based on self-supervised learning\nthat works on human-centric data using only images. Our\nmethod uses pairs of images of humans: the first is par-\ntially masked and the model is trained to reconstruct the\nmasked parts given the visible ones and a second image. It\nrelies on both stereoscopic (cross-view) pairs, and tempo-\nral (cross-pose) pairs taken from videos, in order to learn\npriors about 3D as well as human motion. We pre-train a\nmodel for body-centric tasks and one for hand-centric tasks.\nWith a generic transformer architecture, these models out-\nperform existing self-supervised pre-training methods on a\nwide set of human-centric downstream tasks, and obtain\nstate-of-the-art performance for instance when fine-tuning\nfor model-based and model-free human mesh recovery.\n1. Introduction\nThe main catalyst of performance improvement in computer\nvision tasks in the last decade has arguably been the training\nof large models on large datasets [11, 23, 45, 49, 51, 64, 80].\nFor human-centric vision tasks, the standard approach is to\npre-train models on ImageNet classification tasks and then\nfine-tune them on downstream tasks with specific datasets\n[31, 35, 55, 74]. This has at least three drawbacks: a) the\nsize of the pre-training dataset is limited by label acquisi-\ntion, b) there can be a large domain shift between ImageNet\nand downstream images, c) object-centric classification is\ndifferent from human understanding, which may hinder the\nrelevance of pre-trained features. Collecting large annotated\ndatasets for human centric vision tasks is hard: target sig-\nnal is costly and hard to acquire in the wild, e.g. relying on\nmotion capture systems to obtain 3D pose.\nTo leverage large amounts of data and scale to large mod-\nels, self-supervised pre-training methods such as contrastive\nlearning [8, 10, 14, 30, 77] and masked signal modeling\n[24, 33] – have been developed. In these paradigms, a pre-\ntext task is constructed from the data itself so that no manual\nlabeling is required. The epitome of this philosophy is that\nof foundation models, such as GPT [7], trained on a large\ncorpus of data with no manual annotation and fine-tuned\nto perform efficiently various downstream tasks. In com-\nputer vision, most self-supervised learning methods have\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1512",
        "chosen": "A self-supervised pre-training method for human-centric vision tasks is proposed, using pairs of human images with masking and reconstruction. It leverages stereoscopic and temporal pairs to learn 3D and motion priors, outperforming existing methods on human-centric downstream tasks.",
        "rejected": "We propose a self-supervised learning approach for human-centric tasks using transformer models. By training on image pairs of humans with one partially masked, our method learns from cross-view and cross-pose data, outperforming existing techniques in human mesh recovery."
    },
    {
        "prompt": "summary: We introduce the video detours problem for navigating\ninstructional videos. Given a source video and a natural\nlanguage query asking to alter the how-to video’s current\npath of execution in a certain way, the goal is to find a re-\nlated “detour video” that satisfies the requested alteration.\nTo address this challenge, we propose VidDetours, a novel\nvideo-language approach that learns to retrieve the targeted\ntemporal segments from a large repository of how-to’s us-\ning video-and-text conditioned queries. Furthermore, we\ndevise a language-based pipeline that exploits how-to video\nnarration text to create weakly supervised training data.\nWe demonstrate our idea applied to the domain of how-to\ncooking videos, where a user can detour from their current\nrecipe to find steps with alternate ingredients, tools, and\ntechniques. Validating on a ground truth annotated dataset\nof 16K samples, we show our model’s significant improve-\nments over best available methods for video retrieval and\nquestion answering, with recall rates exceeding the state of\nthe art by 35%.\n1. Introduction\nInstructional or “how-to” videos are a compelling\nmedium for people to share and learn new skills. From\neveryday home fix-it projects, cooking, sports, to aspira-\ntional goals like playing piano beautifully, there are so many\nthings that people of all ages and backgrounds want to learn\nor do a bit better. Indeed, online how-to’s are among the\ntop few dominating categories of all content on YouTube,\nalongside entertainment and music. Advances in computer\nvision for keystep recognition [6, 21, 45, 55, 56, 90, 91],\nprocedural task understanding [11, 12, 89], and video sum-\nmarization [5, 60] have the potential to make such content\nmore searchable and accessible.\nHowever, while today’s how-to content is a vast re-\nsource, it is nonetheless disconnected. Human learners ac-\ncess how-to’s in doses of one video at a time, studying the\nadvice and visual demonstrations of one expert at a time.\nWebsite: https://vision.cs.utexas.edu/projects/detours\n126s\nHow to make Chicken Quesadillas \nResponse:\n180s\n225s\nQuery     : How to do this\nwithout an electric grill?\nDetour window:\nFigure 1. An example video detour. In the Chicken Quesadillas\nrecipe, the source video Vs (top) shows the use of an electric grill\nat time instant ts. A user watching this video does not have a grill\nand asks a query Q “how to do this without an electric grill?”. In\nresponse, the system identifies a detour video Vd and timepoint Td\nshowing a similar recipe but using a heating pan instead of a grill.\nWhile there are thousands and thousands of videos address-\ning, for example, “how to repair a bike” or “how to make\na samosa”, any given video offers only a single execution\nof a task using a fixed set of ingredients, tools, approach,\nand assuming a certain skill level. When those criteria do\nnot align, users face a dilemma whether to improvise, risk-\ning “breaking” the final output, or to find and watch another\nvideo hoping it better matches their constraints. Manually\nsynthesizing the information across videos is time consum-\ning if not prohibitively taxing.\nWhat if the wealth of knowledge in online instructional\nvideos was not an array of isolated lessons, but instead an\ninterconnected network of information? What would it take\nto transform a pile of videos into a how-to knowledge base?\nTowards this vision, we explore how to intelligently nav-\nigate between related how-to videos, conditioned on a natu-\nral language query. Suppose a user watching a given video\ndiscovers they do not have the desired ingredients, tools,\nor skill-level. They may ask, “can I do this step without\na wrench?” or “I am on a diet, can I skip adding cheese\nhere?” or “how could I prepare the mix from scratch in-\nstead of using a pre-made one?” or “is there a simpler way\nto do the corners?” and so on. Conditioned on the con-\ntent watched so far in the source video, the goal is to iden-\ntify a detour video—and a temporal segment within it—that\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n18804",
        "chosen": "VidDetours is introduced to solve the video detours problem, retrieving targeted temporal segments from how-to videos using video-and-text conditioned queries. It creates weakly supervised training data from video narration text, significantly improving video retrieval and question answering in cooking videos.",
        "rejected": "Introducing VidDetours, a novel video-language model designed to navigate instructional videos by retrieving related segments based on user queries. This system leverages weakly supervised training data derived from how-to video narrations, significantly improving recall rates over state-of-the-art methods."
    },
    {
        "prompt": "summary: Generative AI (GenAI) is transforming creative work-\nflows through the capability to synthesize and manipulate\nimages via high-level prompts. Yet creatives are not well\nsupported to receive recognition or reward for the use of\ntheir content in GenAI training. To this end, we propose\nProMark, a causal attribution technique to attribute a syn-\nthetically generated image to its training data concepts like\nobjects, motifs, templates, artists, or styles. The concept\ninformation is proactively embedded into the input train-\ning images using imperceptible watermarks, and the diffu-\nsion models (unconditional or conditional) are trained to\nretain the corresponding watermarks in generated images.\nWe show that we can embed as many as 216 unique water-\nmarks into the training data, and each training image can\ncontain more than one watermark. ProMark can maintain\nimage quality whilst outperforming correlation-based attri-\nbution. Finally, several qualitative examples are presented,\nproviding the confidence that the presence of the watermark\nconveys a causative relationship between training data and\nsynthetic images.\n1. Introduction\nGenAI is able to create high-fidelity synthetic images span-\nning diverse concepts, largely due to advances in diffusion\nmodels, e.g. DDPM [18], DDIM [23], LDM [28]. GenAI\nmodels, particularly diffusion models, have been shown to\nclosely adopt and sometimes directly memorize the style\nand the content of different training images – defined as\n“concepts” in the training data [11, 21]. This leads to con-\ncerns from creatives whose work has been used to train\nGenAI. Concerns focus upon the lack of a means for attribu-\ntion, e.g. recognition or citation, of synthetic images to the\ntraining data used to create them and extend even to calls for\na compensation mechanism (financial, reputational, or oth-\nerwise) for GenAI’s derivative use of concepts in training\nimages contributed by creatives.\nWe refer to this problem as concept attribution – the abil-\nity to attribute generated images to the training concept/s\nwhich have most directly influenced their creation. Several\nFigure 1. Causative vs. correlation-based matching for concept\nattribution. ProMark identifies the training data most responsible\nfor a synthetic image (‘attribution’). Correlation-based matching\ndoesn’t always perform the data attribution properly. We propose\nProMark, which is a proactive approach involving adding water-\nmarks to training data and recovering them from the synthetic im-\nage to perform attribution in a causative way.\npassive techniques have recently been proposed to solve the\nattribution problem [5, 30, 34]. These approaches use vi-\nsual correlation between the generated image and the train-\ning images for attribution. Whilst they vary in their method\nand rationale for learning the similarity embedding – all use\nsome forms of contrastive training to learn a metric space\nfor visual correlation.\nWe argue that although correlation can provide visually\nintuitive results, a measure of similarity is not a causative\nanswer to whether certain training data is responsible for the\ngeneration of an image or not. Further, correlation-based\ntechniques can identify close matches with images that were\nnot even present in the training data.\nKeeping this in mind, we explore an intriguing field\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n10802",
        "chosen": "ProMark is a causal attribution technique for GenAI, embedding watermarks in training images to attribute synthetic images to their training data concepts. It outperforms correlation-based attribution while maintaining image quality.",
        "rejected": "ProMark is a causal attribution technique that embeds imperceptible watermarks into training images for generative AI models. These watermarks allow attributing synthetic images back to their original concepts, providing recognition or reward to creatives whose work contributed to the training dataset."
    },
    {
        "prompt": "summary: Determining the location of an image anywhere on Earth\nis a complex visual task, which makes it particularly rel-\nevant for evaluating computer vision algorithms. Yet, the\nabsence of standard, large-scale, open-access datasets with\nreliably localizable images has limited its potential. To ad-\ndress this issue, we introduce OpenStreetView-5M, a large-\nscale, open-access dataset comprising over 5.1 million geo-\nreferenced street view images, covering 225 countries and\nterritories. In contrast to existing benchmarks, we enforce\na strict train/test separation, allowing us to evaluate the\nrelevance of learned geographical features beyond mere\nmemorization. To demonstrate the utility of our dataset,\nwe conduct an extensive benchmark of various state-of-the-\nart image encoders, spatial representations, and training\nstrategies. All associated codes and models can be found at\ngithub.com/gastruc/osv5m.\n1. Introduction\nWhile natural image classification is the standard for evaluat-\ning computer vision methods [11, 49, 59], global geolocation\noffers a compelling alternative task. In contrast to classifi-\ncation, where the focus is often a single object, geolocation\ninvolves detecting and combining various visual clues, like\nroad signage, architectural patterns, climate, and vegeta-\ntion. Predicting a single GPS coordinate or location label\nfrom these observations necessitates a rich representation\nof both the Earth’s culture and geography; see Figure 1 for\nsome examples. Furthermore, the abundance of geo-tagged\nstreet-view images depicting complex scenes with a clear\nand consistent point of view makes this task appropriate for\ntraining and evaluating modern vision models.\nDespite this potential, few supervised approaches are\n*Denotes equal contributions.\ndrivephotograph, and eng, gclem, bootprint, Mapillary, licensed under CC-BY-SA.\nclimate/vegetation\ntraffic markers\narchitecture\nculture/script\nFigure 1. Global Visual Geolocation. Predicting the location of\nan image taken anywhere in the world from just pixels requires\ndetecting a combination of clues of various abstraction levels [36].\nCan you guess where these images were taken?1\ntrained and evaluated for the task of geolocation. We at-\ntribute this to the limitations of existing geolocation datasets:\n(i) Large and open geolocation datasets contain a significant\nportion of noisy and non-localizable images [19, 25, 60]; (ii)\nStreet view datasets are better suited for the task but are both\nproprietary and expensive to download [7, 10, 16, 18, 33, 53].\nTo address these issues, we introduce OpenStreetView-5M\n(OSV-5M), an open-access dataset of 5.1 million high-\n1\nFrom top left to bottom right: Nagoya, Japan; Ontario, Canada; Mato\nGrosso, Brazil; Lofoten, Norway.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n21967",
        "chosen": "OpenStreetView-5M, a large-scale, open-access geolocation dataset with over 5.1 million geo-referenced street view images, is introduced. It enables evaluation of geographical feature learning beyond memorization, benchmarking various image encoders, spatial representations, and training strategies.",
        "rejected": "OpenStreetView-5M (OSV-5M) offers a large-scale open-access dataset of 5.1 million georeferenced street view images for global geolocation tasks. The dataset enables robust evaluation of computer vision algorithms beyond memorization, enhancing geographical feature learning."
    },
    {
        "prompt": "summary: We introduce SAOR, a novel approach for estimating the\n3D shape, texture, and viewpoint of an articulated object\nfrom a single image captured in the wild.\nUnlike prior\napproaches that rely on pre-defined category-specific 3D\ntemplates or tailored 3D skeletons, SAOR learns to ar-\nticulate shapes from single-view image collections with a\nskeleton-free part-based model without requiring any 3D\nobject shape priors. To prevent ill-posed solutions, we pro-\npose a cross-instance consistency loss that exploits disen-\ntangled object shape deformation and articulation. This is\nhelped by a new silhouette-based sampling mechanism to\nenhance viewpoint diversity during training. Our method\nonly requires estimated object silhouettes and relative depth\nmaps from off-the-shelf pre-trained networks during train-\ning. At inference time, given a single-view image, it effi-\nciently outputs an explicit mesh representation. We obtain\nimproved qualitative and quantitative results on challeng-\ning quadruped animals compared to relevant existing work.\n1. Introduction\nConsidered as one of the first PhD theses in computer vi-\nsion, Roberts [49] aimed to reconstruct 3D objects from\nsingle-view images. Despite significant progress in the pre-\nceding sixty years [5, 6, 19, 20], the problem remains very\nchallenging, especially for highly deformable categories\nphotographed in the wild, e.g., animals. In contrast, humans\ncan infer the 3D shape of an object from a single image by\nmaking use of priors about the natural world and familiarity\nwith the object category present. Some of these natural-\nworld low-level priors can be explicitly defined (e.g., sym-\nmetry or smoothness), but manually encoding and utilizing\nhigh-level priors (e.g., 3D category shape templates) for all\ncategories of interest is not a straightforward task.\nRecently, multiple methods have attempted to learn 3D\nshape by making use of advances in deep learning and\nprogress in differentiable rendering [21, 36, 38]. This has\nresulted in impressive results for synthetic man-made cate-\ngories [7, 21, 59] and humans [12, 37], where full or par-\ntial 3D supervision is readily available.\nHowever, when\n3D supervision is not available, the reconstruction of ar-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n10382",
        "chosen": "SAOR is a novel approach for estimating 3D shape, texture, and viewpoint of articulated objects from a single image. It uses a skeleton-free part-based model with a cross-instance consistency loss and silhouette-based sampling, requiring only object silhouettes and depth maps during training.",
        "rejected": "SAOR introduces a novel approach for estimating 3D shape, texture, and viewpoint of articulated objects from single images. Without relying on predefined 3D templates, SAOR uses a skeleton-free part-based model, achieving improved results compared to existing methods on challenging categories like quadruped animals."
    },
    {
        "prompt": "summary: In this work, we study a novel problem which focuses\non person identification while performing daily activities.\nLearning biometric features from RGB videos is challeng-\ning due to spatio-temporal complexity and presence of ap-\npearance biases such as clothing color and background. We\npropose ABNet, a novel framework which leverages dis-\nentanglement of biometric and non-biometric features to\nperform effective person identification from daily activi-\nties. ABNet relies on a bias-less teacher to learn biometric\nfeatures from RGB videos and explicitly disentangle non-\nbiometric features with the help of biometric distortion. In\naddition, ABNet also exploits activity prior for biometrics\nwhich is enabled by joint biometric and activity learning.\nWe perform comprehensive evaluation of the proposed ap-\nproach across five different datasets which are derived from\nexisting activity recognition benchmarks. Furthermore, we\nextensively compare ABNet with existing works in person\nidentification and demonstrate its effectiveness for activity-\nbased biometrics across all five datasets.\nThe code and\ndataset can be accessed at: https://github.com/\nsacrcv/Activity-Biometrics/\n1. Introduction\nPerson identification is an important task with a wide range\nof applications in security, surveillance, and various do-\nmains where recognizing individuals across different lo-\ncations or time frames is essential [41]. We have seen a\ngreat progress in face recognition [2, 31], however sce-\nnarios exist where faces may not be visible, such as at\nlong distances, with uncooperative subjects, under occlu-\nsion, or due to mask-wearing.\nThis limitation prompts\nthe exploration of whole-body-based person identification\nmethods where most of the existing works are often re-\nstricted to image-based approaches [4, 14, 43], overlooking\ncrucial motion patterns. Video-based methods for person\nidentification is comparatively recent area where most of\nthe work is focused on gait recognition; mostly silhouette-\nFigure 1. Different approaches for person identification: (left)\nsamples for existing person identification problems such as face\nrecognition (top: Celeb-A[30]), whole body recognition (mid-\ndle: Market-1501[45]), and gait recognition (bottom: CASIA-\nB[42]). (right) we focus on person identification from daily ac-\ntivities which presents more challenges beyond learning walking\nor facial patterns. We show some samples from datasets we used\nto study this problem; (top: NTU RGB-AB, middle: Charades-\nAB, bottom: ACC-MM1-Activities1).\nbased [12, 13, 27] with some recent works on RGB frames\n[26, 44]. However these works are mainly focused on walk-\ning style of individuals (see Figure 1).\nIn this work, we study a novel problem which focuses on\nface-restricted person identification during routine activi-\nties. The current landscape of image-based and video-based\nwhole-body person identification methods predominantly\ncenters around analyzing human walking patterns from im-\nages or videos [15, 20, 33, 40]. However, in real-world sce-\nnarios, the individual requiring identification might not al-\nways be engaged in walking; instead, they could be involved\nin various daily activities. It is crucial to acknowledge the\nsignificance of capturing and understanding motion cues\nthat extend beyond simple walking patterns to ensure accu-\nrate and reliable identification in diverse and complex situa-\ntions. These activities may offer unique cues that can prove\ninstrumental in identifying individuals even without explicit\nfacial information, paving the way for diverse applications\nin real-world scenarios, like increased surveillance in public\nspaces, workplace security and productivity, assistance for\npeople requiring special needs, and smart home automation.\n1The subjects consented to publication\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n287",
        "chosen": "ABNet is a framework for person identification from daily activities, disentangling biometric and non-biometric features using a bias-less teacher and biometric distortion. It exploits activity priors through joint biometric and activity learning, evaluated across five datasets.",
        "rejected": "ABNet leverages disentanglement of biometric and non-biometric features to perform person identification from daily activities. It utilizes a bias-less teacher model and exploits activity priors for effective biometric feature learning, demonstrating superior performance across five datasets."
    },
    {
        "prompt": "summary: Despite the growing demand for accurate surface nor-\nmal estimation models, existing methods use general-\npurpose dense prediction models, adopting the same induc-\ntive biases as other tasks. In this paper, we discuss the\ninductive biases needed for surface normal estimation and\npropose to (1) utilize the per-pixel ray direction and (2) en-\ncode the relationship between neighboring surface normals\nby learning their relative rotation. The proposed method\ncan generate crisp — yet, piecewise smooth — predictions\nfor challenging in-the-wild images of arbitrary resolution\nand aspect ratio. Compared to a recent ViT-based state-\nof-the-art model, our method shows a stronger generaliza-\ntion ability, despite being trained on an orders of magni-\ntude smaller dataset. The code is available at https:\n//github.com/baegwangbin/DSINE.\n1. Introduction\nWe address the problem of estimating per-pixel surface nor-\nmal from a single RGB image. This task, unlike monocu-\nlar depth estimation, is not affected by scale ambiguity and\nhas a compact output space (a unit sphere vs. positive real\nvalue), making it feasible to collect data that densely cov-\ners the output space. As a result, learning-based surface\nnormal estimation methods show strong generalization ca-\npability for out-of-distribution images, despite being trained\non relatively small datasets [2].\nDespite their essentially local property, predicted sur-\nface normals contain rich information about scene geom-\netry.\nIn recent years, their usefulness has been demon-\nstrated for various computer vision tasks, including im-\nage generation [62], object grasping [60], multi-task learn-\ning [36], depth estimation [3, 41], simultaneous localization\nand mapping [63], human body shape estimation [5, 54, 55],\nand CAD model alignment [33].\nHowever, despite the\ngrowing demand for accurate surface normal estimation\nmodels, there has been little discussion on the right induc-\ntive biases needed for the task.\nState-of-the-art surface normal estimation methods [2,\n14, 29, 56] use general-purpose dense prediction models,\nadopting the same inductive biases as other tasks (e.g.\ndepth estimation and semantic segmentation). For example,\nCNN-based models [2, 14] assume translation equivariance\nand use the same set of weights for different parts of the\nimage. While such weight-sharing can improve sample ef-\nficiency [34], it is sub-optimal for surface normal estima-\ntion as a pixel’s ray direction provides important cues and\nconstraints for its surface normal. This has limited the accu-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n9535",
        "chosen": "A method rethinking inductive biases for surface normal estimation is proposed, utilizing per-pixel ray direction and encoding relative rotation between neighboring surface normals. It shows stronger generalization ability despite being trained on a smaller dataset.",
        "rejected": "This paper discusses inductive biases necessary for surface normal estimation and proposes DSINE, a method utilizing per-pixel ray direction and relative rotation between neighboring normals. DSINE shows stronger generalization ability despite being trained on a smaller dataset compared to state-of-the-art models."
    },
    {
        "prompt": "summary: Recent breakthroughs in text-to-4D generation rely on\npre-trained text-to-image and text-to-video models to gener-\nate dynamic 3D scenes. However, current text-to-4D meth-\nods face a three-way tradeoff between the quality of scene\nappearance, 3D structure, and motion. For example, text-\nto-image models and their 3D-aware variants are trained\non internet-scale image datasets and can be used to pro-\nduce scenes with realistic appearance and 3D structure—but\nno motion. Text-to-video models are trained on relatively\nsmaller video datasets and can produce scenes with motion,\nbut poorer appearance and 3D structure. While these mod-\nels have complementary strengths, they also have opposing\nweaknesses, making it difficult to combine them in a way\nthat alleviates this three-way tradeoff. Here, we introduce\nhybrid score distillation sampling, an alternating optimiza-\ntion procedure that blends supervision signals from multiple\npre-trained diffusion models and incorporates benefits of\neach for high-fidelity text-to-4D generation. Using hybrid\nSDS, we demonstrate synthesis of 4D scenes with compelling\nappearance, 3D structure, and motion.\n1. Introduction\nThe advent of internet-scale image–text datasets [54] and\nadvances in diffusion models [20, 58, 60] have led to new\ncapabilities in stable, high-fidelity image generation from\ntext prompts [6, 51, 52]. Recent methods have also shown\nthat large-scale text-to-image or text-to-video [56] diffusion\nmodels learn useful priors for 3D [25, 44] and 4D scene\ngeneration [57]. Our work focuses on text-to-4D scene gen-\neration (Fig. 1), which promises exciting new capabilities\nfor applications in augmented and virtual reality, computer\nanimation, and industrial design.\nCurrent techniques for generating 3D or 4D scenes from\ntext prompts typically iteratively optimize a representation\nof the scene using supervisory signals from a diffusion\nmodel [44, 67, 71]. Specifically, these methods render an\nimage of a 3D scene, add noise to the rendered image, use a\npre-trained diffusion model to denoise the rendered image,\nand estimate gradients used to update the 3D representa-\ntion [44, 67]. This procedure, known as score distillation\nsampling (SDS) [44], underpins most recent methods for\ntext-conditioned scene generation.\nUsing SDS for text-to-4D generation requires navigating\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n7996",
        "chosen": "4D-fy is a text-to-4D generation method using hybrid score distillation sampling, blending supervision signals from multiple pre-trained diffusion models. It synthesizes 4D scenes with compelling appearance, 3D structure, and motion.",
        "rejected": "Hybrid score distillation sampling combines supervision signals from multiple pre-trained diffusion models for high-fidelity text-to-4D generation. This method synthesizes dynamic scenes with compelling appearance, 3D structure, and motion, addressing tradeoffs faced by current text-to-4D methods."
    },
    {
        "prompt": "summary: Automating visual inspection in industrial production\nlines is essential for increasing product quality across var-\nious industries.\nAnomaly detection (AD) methods serve\nas robust tools for this purpose. However, existing pub-\nlic datasets primarily consist of images without anomalies,\nlimiting the practical application of AD methods in pro-\nduction settings. To address this challenge, we present (1)\nthe Valeo Anomaly Dataset (VAD), a novel real-world in-\ndustrial dataset comprising 5000 images, including 2000\ninstances of challenging real defects across more than 20\nsubclasses.\nAcknowledging that traditional AD methods\nstruggle with this dataset, we introduce (2) Segmentation-\nbased Anomaly Detector (SegAD). First, SegAD leverages\nanomaly maps as well as segmentation maps to compute\nlocal statistics. Next, SegAD uses these statistics and an\noptional supervised classifier score as input features for a\nBoosted Random Forest (BRF) classifier, yielding the final\nanomaly score. Our SegAD achieves state-of-the-art perfor-\nmance on both VAD (+2.1% AUROC) and the VisA dataset\n(+0.4% AUROC). The code and the models are publicly\navailable1.\n1. Introduction\nWithin the manufacturing process, industrial visual inspec-\ntion plays a crucial role in identifying defects in produced\ncomponents. This operation holds significant importance in\nminimizing costs by identifying and removing faulty parts\nearly in the production stages and, more importantly, in pre-\nventing the dispatch of defective components to customers.\nTraditionally, this task has relied on human operators; how-\never, the likelihood of overlooking certain defects can be as\nhigh as 25% for specific defect types [9].\nWhen the inspected product comprises numerous com-\nponents, its examination may create a bottleneck in the pro-\nduction process, causing delays across the entire line. While\n1https://github.com/abc-125/segad\nconventional computer vision methods applied to this task\ndemonstrate superior speed and lower error rates compared\nto human operators [16], their inflexibility and lack of sat-\nisfactory accuracy [15] limit their effectiveness.\nIndustrial deep learning anomaly detection has been an\nactive research field in recent years. Most of anomaly de-\ntection methods use only good images for training and try\nto detect deviations from the training data [1, 7, 23, 29].\nDevelopment of the new methods is restrained by pub-\nlicly available datasets. Recent industrial anomaly detec-\ntion datasets typically contain approximately one hundred\n(or even fewer) abnormal images, showcasing defects in the\ntesting set only [2, 32]. This poses a challenge for super-\nvised anomaly detection methods aiming to utilize both nor-\nmal and defective parts during training. Supervised models\noften undergo training with just ten abnormal images from\nthe testing set, resulting in overfitting and reduced sensitiv-\nity to previously unseen defects [28].\nIn response to the limitations of current datasets, we\nintroduce and publicly release VAD (Valeo Anomaly\nDataset), which contains 1000 bad and 2000 good parts in\nthe training set and 1000 bad and 1000 good parts in the\ntest set, see Fig. 1(a). Among the defective parts used for\ntesting, 165 contain specific defect types not present in the\ntraining data; these parts are explicitly labeled in the re-\nleased dataset. All images in VAD are captured from an\nactual production line, showcasing a diverse range of de-\nfects, from highly obvious to extremely subtle. This dataset\nbridges the gap between the academic community and the\nindustry, offering researchers the chance to advance the per-\nformance of methods in tackling more intricate real-world\nchallenges.\nCurrent approaches to supervised anomaly detection ei-\nther yield unsatisfactory results; see Tab. 3, or demand\npixel-level labels for defective parts, as seen in [28, 31].\nConsequently, we introduce a novel method named SegAD\n(Segmentation-based Anomaly Detector), which is de-\nscribed in Fig. 1(b). The employed approach eliminates the\nneed for pixel-level labels, requiring only a flag for each\nimage. Anomaly map scores from each segment are used\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n17754",
        "chosen": "SegAD is introduced for supervised anomaly detection in industrial images, using anomaly and segmentation maps with a Boosted Random Forest classifier. It achieves state-of-the-art performance on the Valeo Anomaly Dataset and VisA dataset.",
        "rejected": "The Valeo Anomaly Dataset (VAD) and Segmentation-based Anomaly Detector (SegAD) are introduced for industrial anomaly detection. SegAD computes local statistics from anomaly and segmentation maps, achieving state-of-the-art performance on VAD and VisA datasets."
    },
    {
        "prompt": "summary: Contrastive Vision-Language Pre-training, known as\nCLIP, has shown promising effectiveness in addressing\ndownstream image recognition tasks.\nHowever, recent\nworks revealed that the CLIP model can be implanted with a\ndownstream-oriented backdoor. On downstream tasks, one\nvictim model performs well on clean samples but predicts a\nspecific target class whenever a specific trigger is present.\nFor injecting a backdoor, existing attacks depend on a large\namount of additional data to maliciously fine-tune the entire\npre-trained CLIP model, which makes them inapplicable to\ndata-limited scenarios. In this work, motivated by the re-\ncent success of learnable prompts, we address this problem\nby injecting a backdoor into the CLIP model in the prompt\nlearning stage. Our method named BadCLIP is built on a\nnovel and effective mechanism in backdoor attacks on CLIP,\ni.e., influencing both the image and text encoders with the\ntrigger. It consists of a learnable trigger applied to images\nand a trigger-aware context generator, such that the trigger\ncan change text features via trigger-aware prompts, result-\ning in a powerful and generalizable attack. Extensive ex-\nperiments conducted on 11 datasets verify that the clean ac-\ncuracy of BadCLIP is similar to those of advanced prompt\nlearning methods and the attack success rate is higher than\n99% in most cases. BadCLIP is also generalizable to un-\nseen classes, and shows a strong generalization capability\nunder cross-dataset and cross-domain settings. The code is\navailable at https://github.com/jiawangbai/BadCLIP.\n1. Introduction\nRecently, contrastive vision-language models [61] have\nshown a great potential in visual representation learning.\nThey utilize contrastive learning [12, 14, 29] to pull to-\n*Equal contribution.\n†Corresponding author.\ngether images and their language descriptions while push-\ning away unmatched pairs in the representation space, re-\nsulting in aligned features of images and texts. Benefiting\nfrom large-scale pre-training datasets, models can learn rich\nand transferable visual representations. Given a test image,\none can obtain its predicted class by computing the similar-\nity between the image features and the text features of cat-\negory descriptions called prompts. For instance, the prompt\ncan be the class name [CLS] extended by a hand-crafted\ntemplate “a photo of [CLS]” [36, 79]. Many works\n[11, 36, 54, 70, 78] have proven that such a paradigm is\npromising to address downstream recognition tasks.\nUnfortunately, recent works [10, 37] succeeded in in-\njecting the downstream-oriented backdoor into the CLIP\nmodel, which can be activated by some specific patterns\ncalled triggers, e.g., a square image patch [10, 27, 37, 74].\nThe attack is very stealthy because the victim model be-\nhaves normally on clean images but predicts a specific target\nclass only when the trigger is present. On the other hand,\nconsidering that the popularity of CLIP is increasing on di-\nverse tasks [44, 54, 55, 70, 71] including some security-\nsensitive ones in autonomous driving [62] and visual navi-\ngation [93], the vulnerability threatens the real-world appli-\ncations. Therefore, the study of backdoor attacks on CLIP\nis crucial for recognizing potential risks and securely ex-\nploiting the CLIP model.\nCarlini et al.\n[10] first explored the backdoor attack\non CLIP in the training stage. They proposed to pre-train\nCLIP on a poisoned dataset with the assumption that the at-\ntacker has access to the pre-training data. After that, BadEn-\ncoder [37] manipulates a pre-trained CLIP model to inject\nthe backdoor.\nIt maliciously fine-tunes the entire model\nand thus requires a large amount of additional data. How-\never, the pre-training data or large-scale additional data may\nbe not available, which significantly reduces their threats.\nThese limitations also make that they cannot be coupled\nwith one of the most widely-used ways to exploit CLIP, few-\nshot transfer [54, 71, 92, 100, 101], which adapts the public\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n24239",
        "chosen": "BadCLIP is a backdoor attack on CLIP models during prompt learning, influencing both image and text encoders with a trigger. It achieves high attack success rates while maintaining clean accuracy and generalizing to unseen classes and domains.",
        "rejected": "Our research focuses on person identification during routine activities, proposing ABNet to disentangle biometric and non-biometric features using RGB videos. This framework enhances identification accuracy beyond traditional gait or facial recognition methods, showing effectiveness across diverse datasets."
    },
    {
        "prompt": "summary: Federated learning (FL) has emerged as a powerful\nparadigm for learning from decentralized data, and fed-\nerated domain generalization further considers the test\ndataset (target domain) is absent from the decentralized\ntraining data (source domains).\nHowever, most existing\nFL methods assume that domain labels are provided during\ntraining, and their evaluation imposes explicit constraints\non the number of domains, which must strictly match the\nnumber of clients. Because of the underutilization of nu-\nmerous edge devices and additional cross-client domain an-\nnotations in the real world, such restrictions may be im-\npractical and involve potential privacy leaks. In this pa-\nper, we propose an efficient and novel approach, called\nDisentangled Prompt Tuning (DiPrompT), a method that\ntackles the above restrictions by learning adaptive prompts\nfor domain generalization in a distributed manner. Specif-\nically, we first design two types of prompts, i.e., global\nprompt to capture general knowledge across all clients\nand domain prompts to capture domain-specific knowledge.\nThey eliminate the restriction on the one-to-one mapping\nbetween source domains and local clients. Furthermore, a\ndynamic query metric is introduced to automatically search\nthe suitable domain label for each sample, which includes\ntwo-substep text-image alignments based on prompt tun-\ning without labor-intensive annotation. Extensive experi-\nments on multiple datasets demonstrate that our DiPrompT\nachieves superior domain generalization performance over\nstate-of-the-art FL methods when domain labels are not\nprovided, and even outperforms many centralized learning\nmethods using domain labels.\n1. Introduction\nFederated learning (FL) is an emerging privacy-preserving\nmachine-learning technique [26], which enables multiple\n∗Equal contribution, † Corresponding authors, ‡ Project leader\n(70.59) \nCartoon \nReal World \nSketch\nTarget\nD-Prompt 2\nD-Prompt 1\nD-Prompt 3\nG-Prompt\n(b) Training Stage in clients-side\nCartoon\nClient 1\nClient 2\nClient 3\nClient 4\nServer\nD-Prompt 1\nD-Prompt 2\nD-Prompt 3\nG-Prompt\nPrediction\n0.2\n0.7\n0.1\n1.0\n(c) Test Stage in Server-side\n(a) Diversified Source domains \n70.59 \n403.86 \n1878.98\n307.92\nFigure 1. The motivation example and our main idea. (a) When\nclients outnumber source domains, learning domain-invariant fea-\ntures may become challenging due to imbalanced contributions\nacross domains/clients. Note that the contribution imbalance of\nlocal data is measured through its feature distances with the tar-\nget domain. (b) DiPrompT separates domain-specific features and\ngeneral knowledge during local training. (c) DiPrompT adaptive\nensembles for generic and valuable specific knowledge for better\ntarget domain prediction during inference.\nclients (e.g., mobile devices or organizations) to collabo-\nratively learn a global model without exchanging their pri-\nvate data. However, a practical concern with conventional\nFL methods is that they usually ignore the possible domain\nshift between training data (source domains) and test data\n(target domain) [1], which can incur poor performance on\nunseen target domains due to domain discrepancies.\nRecently, some research efforts have attempted to incor-\nporate domain generalization into the FL framework. For\nexample, FedDG [21] shares the amplitude spectrum of im-\nages among local clients for medical image segmentation.\nFedSR [29] builds a simple algorithm that utilizes two lo-\ncal regularizers for domain generalization.\nThese meth-\nods extract domain-invariant features across all source do-\nmains. Nevertheless, most of these methods only focus on\ndomain-invariant features across clients, and they rely on\nthe assumption of one-to-one mapping of client and domain.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n27284",
        "chosen": "DiPrompT is a novel federated learning approach for domain generalization that learns adaptive prompts in a distributed manner, eliminating the one-to-one mapping restriction between source domains and local clients. It introduces global and domain prompts to capture general and domain-specific knowledge, and a dynamic query metric to automatically search suitable domain labels. Extensive experiments show DiPrompT outperforms state-of-the-art FL methods without domain labels and even surpasses centralized methods using domain labels.",
        "rejected": "Federated learning (FL) has emerged as a method for decentralized data learning. DiPrompT proposes adaptive prompts for domain generalization in FL without explicit domain labels. It introduces global and domain-specific prompts, enhancing performance on unseen domains. The approach achieves superior results compared to centralized methods."
    },
    {
        "prompt": "summary: 3D head avatars built with neural implicit volumetric\nrepresentations have achieved unprecedented levels of pho-\ntorealism. However, the computational cost of these meth-\nods remains a significant barrier to their widespread adop-\ntion, particularly in real-time applications such as virtual\nreality and teleconferencing.\nWhile attempts have been\nmade to develop fast neural rendering approaches for static\nscenes, these methods cannot be simply employed to sup-\nport realistic facial expressions, such as in the case of a dy-\nnamic facial performance. To address these challenges, we\npropose a novel fast 3D neural implicit head avatar model\nthat achieves real-time rendering while maintaining fine-\ngrained controllability and high rendering quality. Our key\nidea lies in the introduction of local hash table blendshapes,\nwhich are learned and attached to the vertices of an under-\nlying face parametric model. These per-vertex hash-tables\nare linearly merged with weights predicted via a CNN, re-\nsulting in expression dependent embeddings. Our novel rep-\nresentation enables efficient density and color predictions\nusing a lightweight MLP, which is further accelerated by a\nhierarchical nearest neighbor search method. Extensive ex-\nperiments show that our approach runs in real-time while\nachieving comparable rendering quality to state-of-the-arts\nand decent results on challenging expressions.\n1. Introduction\nThe demand of high performing photo-realistic human\navatars has dramatically increased with emerging VR/AR\napplications, e.g. VR gaming [2, 33], virtual assistant [3],\ntele-presence [28], and 3D videos [15, 26]. How to build\nefficient high quality avatars from monocular RGB videos\nbecomes a promising direction due to the convenience of\nmonocular data acquisition.\nWhile early works mostly\nadopt surface-based models for convenient controllability,\nrecent methods (e.g. MonoAvatar [5]) leverage a sophisti-\ncated pipeline to build human avatars on neural radiance\nfields, which delivers vivid animations as well as signifi-\ncantly better rendering quality, especially over challenging\nparts such as hairs and glasses. On the downside, these ap-\nproaches tend to be prohibitively slow, and most of the com-\nputation is consumed by the neural radiance field inference\nwith large Multilayer Perceptrons (MLPs).\nRecently, fast approaches for neural radiance fields (e.g.\nhash encoding in Instant NGPs [27]) have been proposed,\nwhich are designed mostly for static scenes or pre-recorded\ntemporal sequences.\nDespite their great success, it is\nnot straightforward to extend these approaches for human\navatars, which requires real-time rendering of dynamic fa-\ncial performances when controlling the avatar. NeRFBlend-\n∗Work was conducted while Ziqian Bai was an intern at Google.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1975",
        "chosen": "A novel fast 3D neural implicit head avatar model is proposed, achieving real-time rendering while maintaining fine-grained controllability and high quality. It uses local hash table blendshapes attached to an underlying face parametric model, with per-vertex hash-tables merged via CNN-predicted weights. This enables efficient density and color predictions using a lightweight MLP accelerated by a hierarchical nearest neighbor search. Experiments show real-time performance with comparable rendering quality to state-of-the-arts.",
        "rejected": "A novel 3D neural implicit head avatar model is introduced, achieving real-time rendering while maintaining high quality and controllability. By using local hash table blendshapes and an efficient MLP, it ensures fine-grained expression control and fast performance suitable for dynamic facial performances in VR."
    },
    {
        "prompt": "summary: We introduce the Fixed Point Diffusion Model (FPDM),\na novel approach to image generation that integrates the\nconcept of ﬁxed point solving into the framework of diffusion-\nbased generative modeling. Our approach embeds an im-\nplicit ﬁxed point solving layer into the denoising network\nof a diffusion model, transforming the diffusion process\ninto a sequence of closely-related ﬁxed point problems.\nCombined with a new stochastic training method, this ap-\nproach signiﬁcantly reduces model size, reduces memory\nusage, and accelerates training. Moreover, it enables the\ndevelopment of two new techniques to improve sampling\nefﬁciency: reallocating computation across timesteps and\nreusing ﬁxed point solutions between timesteps. We con-\nduct extensive experiments with state-of-the-art models on\nImageNet, FFHQ, CelebA-HQ, and LSUN-Church, demon-\nstrating substantial improvements in performance and efﬁ-\nciency. Compared to the state-of-the-art DiT model [38],\nFPDM contains 87% fewer parameters, consumes 60% less\nmemory during training, and improves image generation\n*Equal Contribution.\n†Corresponding author.\nquality in situations where sampling computation or time is\nlimited. Our code and pretrained models are available at\nhttps://lukemelas.github.io/ﬁxed-point-diffusion-models/.\n1. Introduction\nThe ﬁeld of image generation has experienced signiﬁcant\nrecent advancements driven by the development of large-\nscale diffusion models [23, 37, 38, 41, 47, 48]. Key to these\nadvancements have been increased model size, computa-\ntional power, and the collection of extensive datasets [4, 12,\n16, 25, 45, 46, 54], collectively contributing to a marked\nimprovement in generation performance.\nDespite these strides, the core principles of diffusion net-\nworks have remained largely unchanged since their develop-\nment [23]. They typically consist of a ﬁxed series of neural\nnetwork layers, either with a UNet architecture [42] or, more\nrecently, a vision transformer architecture [14, 51]. However,\nas diffusion models are increasingly deployed in production,\nespecially on mobile and edge devices, their large size and\ncomputational costs pose signiﬁcant challenges.\nThis paper introduces the Fixed Point Diffusion Model\n(FPDM), which integrates an implicit ﬁxed point solving\nlayer into the denoising network of a diffusion model. In\n1\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n9430",
        "chosen": "The Fixed Point Diffusion Model (FPDM) integrates fixed point solving into diffusion-based generative modeling, embedding an implicit fixed point layer in the denoising network. Combined with stochastic training, it reduces model size, memory usage, and accelerates training. It also improves sampling efficiency through computation reallocation and solution reuse. Experiments on ImageNet, FFHQ, CelebA-HQ, and LSUN-Church show FPDM significantly improves performance and efficiency, with 87% fewer parameters and 60% less memory than DiT.",
        "rejected": "The Fixed Point Diffusion Model (FPDM) integrates fixed point solving into diffusion-based generative modeling, reducing model size and memory usage. This approach accelerates training and improves sampling efficiency, offering significant improvements over state-of-the-art models like DiT."
    },
    {
        "prompt": "summary: Out-of-distribution (OOD) detection methods often ex-\nploit auxiliary outliers to train model identifying OOD sam-\nples, especially discovering challenging outliers from aux-\niliary outliers dataset to improve OOD detection. However,\nthey may still face limitations in effectively distinguishing\nbetween the most challenging OOD samples that are much\nlike in-distribution (ID) data, i.e., ID-like samples. To\nthis end, we propose a novel OOD detection framework that\ndiscovers ID-like outliers using CLIP [32] from the vicin-\nity space of the ID samples, thus helping to identify these\nmost challenging OOD samples. Then a prompt learning\nframework is proposed that utilizes the identified ID-like\noutliers to further leverage the capabilities of CLIP for OOD\ndetection. Benefiting from the powerful CLIP, we only need\na small number of ID samples to learn the prompts of the\nmodel without exposing other auxiliary outlier datasets. By\nfocusing on the most challenging ID-like OOD samples\nand elegantly exploiting the capabilities of CLIP, our method\nachieves superior few-shot learning performance on various\nreal-world image datasets (e.g., in 4-shot OOD detection on\nthe ImageNet-1k dataset, our method reduces the average\nFPR95 by 12.16% and improves the average AUROC by\n2.76%, compared to state-of-the-art methods). Code is avail-\nable at https://github.com/ycfate/ID-like\n1. Introduction\nWhen deploying machine learning models in practical set-\ntings, it is possible to come across OOD samples that were\nnot encountered during training. The risk of incorrect de-\ncisions rises when it comes to these OOD inputs, which\ncould pose serious safety issues, particularly in applications\nlike autonomous driving and medical diagnosis. The system\nneeds to identify OOD samples in addition to performing\n*Equal contribution. †Corresponding author.\nID: husky\neasy OOD: cat\nhard OOD: wolf\nFigure 1. Hard OOD samples typically contain more features corre-\nlated to ID samples, i.e., they behave more ID-like.\nwell on ID samples in order to produce trustworthy predic-\ntions. OOD detection is therefore quite critical for safely\ndeploying machine learning models in reality.\nExisting methods [9, 18, 21] usually focus on detecting\nOOD examples only using ID data in training to predict\nlower confidence [8, 27] or higher energy [22] for OOD sam-\nples. However, due to the lack of OOD information, these\nmodels struggle to be effective in OOD detection. There-\nfore, some studies [10, 22] suggest using auxiliary outliers\nto regularize the models and identity OOD samples. Chen\net al. [1] and Ming et al. [29] suggested that selecting more\nchallenging outlier samples can help the model learn a better\ndecision boundary between ID and OOD. However, these\nauxiliary outliers usually contain limited challenging outliers.\nFurthermore, most of these methods require additional out-\nlier data, which makes them ineffective when outlier datasets\nare unavailable. Recently, Du et al. [4] proposed to synthe-\nsize virtual outlier data in the feature space of ID data to\nconstruct outliers during training without additional data.\nThis method shows strong efficacy in distinguishing between\nID and OOD. However, there are two main limitations: i)\nit assumes that ID data in the feature space conforms to a\nclass conditional Gaussian distribution, which does not al-\nways hold in the complex real-world applications [34]; ii) it\nrequires numerous ID samples to construct a more accurate\ndistribution of ID data, while obtaining a large number of ID\nsamples is often costly. Accordingly, in this work, we focus\non flexibly constructing challenging outliers with few-shot\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n17480",
        "chosen": "A novel OOD detection framework discovers ID-like outliers using CLIP from the vicinity of ID samples, then uses prompt learning to leverage CLIP's capabilities for OOD detection. This method focuses on challenging ID-like OOD samples, requiring few ID samples to learn prompts without auxiliary outlier datasets. Extensive experiments demonstrate superior few-shot learning performance, reducing average FPR95 by 12.16% and improving AUROC by 2.76% on ImageNet-1k in 4-shot OOD detection.",
        "rejected": "A new OOD detection framework leveraging CLIP identifies ID-like outliers to improve detection accuracy. Prompt learning with these outliers enhances CLIP's capabilities, demonstrating superior few-shot learning performance on various datasets by significantly reducing FPR95 and improving AUROC."
    },
    {
        "prompt": "summary: We introduce a novel sequential modeling approach which\nenables learning a Large Vision Model (LVM) without mak-\ning use of any linguistic data. To do this, we deﬁne a common\nformat, “visual sentences”, in which we can represent raw\nimages and videos as well as annotated data sources such\nas semantic segmentations and depth reconstructions with-\nout needing any meta-knowledge beyond the pixels. Once\nthis wide variety of visual data (comprising 420 billion to-\nkens) is represented as sequences, the model can be trained\nto minimize a cross-entropy loss for next token prediction.\nBy training across various scales of model architecture and\ndata diversity, we provide empirical evidence that our models\nscale effectively. Many different vision tasks can be solved\nby designing suitable visual prompts at test time.\n1. Introduction\nLarge language models (LLMs) such as GPT [10] and\nLLaMA [80] have taken the world by storm. What would\nit take to build a Large Vision Model (LVM)? From the\nanimal world, we know that visual competences are not de-\npendent on language. In particular, many experiments have\nshown that the visual world of non-human primates is re-\nmarkably similar to that of humans. So while the space of\nvision-language models such as LLaVA [53] is interesting\nand worthwhile to pursue, in this paper we seek an answer to\na different question – how far can we go from pixels alone?\nThe key features of contemporary LLMs that we seek\nto emulate in LVMs are: 1) scaling in the presence of big\ndata, and 2) ﬂexible speciﬁcation of tasks through prompting\n(in-context learning). How do we achieve this? As usual,\nthere are three main components that must be speciﬁed:\nData: We want to exploit all the remarkable diversity in\nvisual data. First of all, just raw unannotated images and\nvideos. Next, we want to exploit the variety of annotated\nvisual data sources that have been produced over the last\n*Equal Contribution. Webpage: https://yutongbai.com/lvm.html.\ncouple of decades – semantic segmentations, depth recon-\nstructions, keypoints, multiple views of 3D objects, among\nothers. We deﬁne a common format, “visual sentences”, in\nwhich to represent these different annotations without need-\ning any meta-knowledge beyond the pixels. The total size of\nour training dataset is 1.64 billion images/frames.\nArchitecture: We use a large transformer architecture\n(3 billion parameters) trained on visual data represented as\nsequence of tokens, using a learned tokenizer that maps each\nimage to a string of 256 vector-quantized tokens.\nLoss function: We draw inspiration from the natural\nlanguage community, where masked token modeling has\ngiven way to sequential autoregressive prediction. Once\nimages/videos/annotated images can all be represented as\nsequences, we can train the model to minimize the cross-\nentropy loss for predicting the next token.\nWith this extremely simple design, we demonstrate some\nnoteworthy behaviors:\n• Appropriate scaling behavior as one increases model size\nand data size.\n• Many different vision tasks can now be “solved” by design-\ning suitable prompts at test time. While the results don’t\nshow as high performance as bespoke, speciﬁcally-trained\nmodels, the fact that so many tasks are all addressed by a\nsingle vision model is quite encouraging.\n• We see a clear beneﬁt of the amount of unsupervised data\non the performance on various standard vision tasks.\n• We see a hint of an ability for general visual reasoning –\nhandling out-of-distribution data, and performing novel\ntasks. But further investigation is needed.\n2. Related Work\nPretrained Vision Models. The value of using pretrained\nmodels (such as ImageNet-pretrained AlexNet [45]) has\nbeen demonstrated as far back as 2015 in R-CNN [34],\nand it has since become standard practice in computer vi-\nsion. Self-supervised pretraining was proposed as a way\nto vastly increase the amount of data available for pretrain-\ning [16, 25, 37, 61, 62, 99]. Unfortunately, this was not\nvery successful, likely because the CNN-based architectures\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n22861",
        "chosen": "A sequential modeling approach enables learning a Large Vision Model (LVM) using only visual data, defining \"visual sentences\" to represent diverse visual data as sequences. A 3 billion parameter transformer trained on 1.64 billion images minimizes cross-entropy loss for next token prediction. This model demonstrates effective scaling, solves various vision tasks via test-time prompts, benefits from unsupervised data, and shows potential for general visual reasoning.",
        "rejected": "Introducing a Large Vision Model (LVM) trained without linguistic data, this work defines \"visual sentences\" to represent diverse visual data formats. LVM scales effectively across different architectures and data diversities, solving many vision tasks through prompt design at test time."
    },
    {
        "prompt": "summary: This paper presents DriveTrack, a new benchmark and\ndata generation framework for long-range keypoint tracking in\nreal-world videos. DriveTrack is motivated by the observation\nthat the accuracy of state-of-the-art trackers depends strongly\non visual attributes around the selected keypoints, such as\ntexture and lighting. The problem is that these artifacts are\nespecially pronounced in real-world videos, but these trackers\nare unable to train on such scenes due to a dearth of annotations.\nDriveTrack bridges this gap by building a framework to auto-\nmatically annotate point tracks on autonomous driving datasets.\nWe release a dataset consisting of 1 billion point tracks across\n24 hours of video, which is seven orders of magnitude greater\nthan prior real-world benchmarks and on par with the scale\nof synthetic benchmarks. DriveTrack unlocks new use cases\nfor point tracking in real-world videos. First, we show that fine-\ntuning keypoint trackers on DriveTrack improves accuracy on\nreal-world scenes by up to 7%. Second, we analyze the sensitiv-\nity of trackers to visual artifacts in real scenes and motivate the\nidea of running assistive keypoint selectors alongside trackers.\n1. Introduction\nLong-range keypoint tracking in videos underpins many com-\nputer vision applications, including autonomous driving [14],\nrobotics [24], pose estimation [8], 3D reconstruction [16], and\nmedical imaging [26]. Each of these applications involves mov-\ning objects and moving cameras. Keypoint tracking—whose\ngoal is to track unique points in the presence of mobility and\nocclusions—is an active area of research [4, 5, 10, 29].\nMost proposals follow the Track Any Point (TAP) [4]\nformulation: given a video and a set of query points, the\nalgorithm must estimate the locations of those points in all\nother frames where they are visible. The underlying tracking\nalgorithms vary significantly. TAPIR [4, 5] is an end-to-end\nmethod that predicts correspondences using feature maps and\ncost volumes. By contrast, PIPs++ [10, 29] stitches optical flow\nvectors together to construct long-range trajectories. These are\ntwo recent methods that improve the state-of-the-art, adding\nto a number of techniques proposed over the last two decades.\nThis paper observes that the accuracy of state-of-the-art\nFigure 1. DriveTrack automatically generates dense, accurate, and\nlong-range point track annotations for autonomous driving videos.\ntrackers suffers on real videos.\nIn particular, noisy visual\ncharacteristics—such as texture, lighting variations, occlusions,\nand motion-induced image distortions—can hinder tracking\nperformance (§3). The key problem is that modern trackers train\non vast synthetic datasets [9, 13, 29] whose scenes do not exhibit\nthese imperfections. We are aware of only two benchmarks for\nthe TAP task on real-world videos [12, 19], with each offering\n(only) tens of human-labeled annotations per scene.\nTo overcome this shortcoming, we propose DriveTrack,\na large-scale benchmark for long-range point tracking tasks.\nDriveTrack brings to real-world videos the density and fidelity\nof annotations available only for synthetic benchmarks today.\nBy using camera feeds from cars driven in urban areas,\nDriveTrack captures realistic motion, noisy visual attributes, and\nocclusions, which synthetic [9, 13] or rendered [29] datasets\ndo not model. Fig. 1 shows the annotations computed by\nDriveTrack for a driving scene. Although DriveTrack is built\non autonomous driving videos, it captures the wide variety of\nvisual artifacts typical in real-world scenes.\nTo generate point tracks, we adapt methods used by synthetic\nbenchmarks [9, 29] that use rendering software to precisely\nannotate the motion of simulated trajectories.\nHowever,\nreal-world videos do not have the luxury of a simulator.\nTo overcome this challenge, DriveTrack uses timestamped\nLiDAR point clouds, object bounding box annotations, and\ncamera poses and orientations [2, 7, 21, 25]. Since LiDAR\npoint sweeps do not have 1:1 correspondence over time [21],\nDriveTrack cannot compute correspondences between adjacent\npoint clouds, as synthetic benchmarks are able to. DriveTrack\ninstead transforms each timestamped point cloud according\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n22488",
        "chosen": "DriveTrack is a benchmark and data generation framework for long-range keypoint tracking in real-world videos. It automatically annotates point tracks on autonomous driving datasets, releasing 1 billion point tracks across 24 hours of video. This improves tracker accuracy on real-world scenes by up to 7% and analyzes tracker sensitivity to visual artifacts, motivating assistive keypoint selectors.",
        "rejected": "DriveTrack provides a benchmark for long-range keypoint tracking in real-world videos, addressing limitations of synthetic datasets. Fine-tuning trackers on DriveTrack improves real-world scene accuracy and analyzes tracker sensitivity to visual artifacts, proposing assistive keypoint selectors."
    },
    {
        "prompt": "summary: Sign Languages (SL) serve as the primary mode of com-\nmunication for the Deaf and Hard of Hearing communities.\nDeep learning methods for SL recognition and translation\nhave achieved promising results. However, Sign Language\nProduction (SLP) poses a challenge as the generated mo-\ntions must be realistic and have precise semantic meaning.\nMost SLP methods rely on 2D data, which hinders their re-\nalism. In this work, a diffusion-based SLP model is trained\non a curated large-scale dataset of 4D signing avatars and\ntheir corresponding text transcripts. The proposed method\ncan generate dynamic sequences of 3D avatars from an un-\nconstrained domain of discourse using a diffusion process\nformed on a novel and anatomically informed graph neu-\nral network defined on the SMPL-X body skeleton. Through\nquantitative and qualitative experiments, we show that the\nproposed method considerably outperforms previous meth-\nods of SLP. This work makes an important step towards re-\nalistic neural sign avatars, bridging the communication gap\nbetween Deaf and hearing communities.1\n1Project page: https://baltatzisv.github.io/neural-sign-actors/\n1. Introduction\nSign language (SL) is a form of language in which visual-\nmanual modalities are used instead of spoken words to con-\nvey meaning. It is the predominant form of communication\nfor more than 70 million Deaf and Hard of Hearing peo-\nple around the world. Akin to verbal languages, SLs have\nextremely rich vocabulary and grammar, yet the complexi-\nties differ drastically [55]. To enable effective visual com-\nmunication, they consist of both manual and non-manual\ncomponents [35]. The manual modality encompasses hand\narticulation, orientation, position, and motion, while non-\nmanual elements include arm movements and facial expres-\nsions [6]. Whilst it is possible to convey some meaning us-\ning just hand articulations, expressiveness is limited since\nnon-manual elements often convey emotions [3, 55].\nRecently, several methods have been proposed to bridge\nthe domain gap between sign and spoken languages. Most\nmethods focus on Sign Language Recognition (SLR) which\nincludes the translation of a specific sign to its correspond-\ning meaning, as well as Sign Language Translation (SLT)\nthat extends SLR to the translation of a sign sequence to\nits spoken word equivalent. This is usually tackled using\nglosses [10, 14, 15, 28], which are simplified mid-level\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1985",
        "chosen": "A diffusion-based Sign Language Production (SLP) model is trained on a large-scale 4D signing avatar dataset. It generates dynamic 3D avatar sequences using a diffusion process on a novel graph neural network defined on the SMPL-X body skeleton. Experiments show significant improvement over previous methods, advancing realistic neural sign avatars and bridging the communication gap between Deaf and hearing communities.",
        "rejected": "A diffusion-based Sign Language Production (SLP) model generates realistic 3D avatar motions from text transcripts. Using an anatomically informed graph neural network, it outperforms previous SLP methods, marking a significant step towards bridging communication gaps between Deaf and hearing communities."
    },
    {
        "prompt": "summary: Recent advances in large-scale pretraining have yielded\nvisual foundation models with strong capabilities. Not only\ncan recent models generalize to arbitrary images for their\ntraining task, their intermediate representations are useful\nfor other visual tasks such as detection and segmentation.\nGiven that such models can classify, delineate, and local-\nize objects in 2D, we ask whether they also represent their\n3D structure? In this work, we analyze the 3D awareness\nof visual foundation models. We posit that 3D awareness\nimplies that representations (1) encode the 3D structure of\nthe scene and (2) consistently represent the surface across\nviews. We conduct a series of experiments using task-speciﬁc\nprobes and zero-shot inference procedures on frozen fea-\ntures. Our experiments reveal several limitations of the\ncurrent models. Our code and analysis can be found at\nhttps://github.com/mbanani/probe3d.\n1. Introduction\nLarge-scale pretraining on image datasets has yielded visual\nfoundation models with impressive generalization capabili-\nties. Such models can classify [46, 65], segment [36], and\ngenerate [10, 69, 70] arbitrary images. Furthermore, the\ndense representations learned by such models extend beyond\ntheir training tasks and exhibit strong zero-shot capabilities\nin other tasks such as segmentation [56, 95] and part discov-\nery [1, 27]. This suggests that models are learning strong\nimage representations, but how well do they represent the\n3D world that images depict?\nRecent work suggests that visual foundation models are\nuseful for some 3D tasks despite being trained with 2D\ndata. For instance, models implicitly represent depth and\nsurface normals when generating images of scenes and\nfaces [6, 12]. Furthermore, the intermediate representations\nof self-supervised and generative models can be used to esti-\nmate semantic correspondence [1, 27, 30, 83, 99] and object\npose [25]. However, when reconstructing 3D objects, they\ngenerate artifacts that suggest a lack of 3D consistency [50];\n* Current afﬁliation is Stability AI.\nEvaluation Tasks\nSingle-Image 3D \nMultiview Consistency\nSingle-Image 3D \nMultiview Consistency\nObjects\nScenes\nEvaluated Models\nMAE \niBOT\nDeiT III\nCLIP\nMiDaS \nDINO \nDINOv2\nStableDiffusion\nSigLIP\nSAM \nFigure 1. Are current visual foundation models 3D aware? We\nprobe the 3D awareness of the learned representations by evaluating\ntheir ability to encode the 3D structure of the visible surface and\ntheir consistency across views.\ne.g., animals with multiple faces. Therefore, it remains un-\nclear how those modes represent or understand the 3D world.\nThe goal of this paper is to study the 3D awareness of\nvisual foundation models. Previous benchmarks evaluate\nvisual models on semantic tasks [24, 26, 87], but their 3D\nunderstanding remains understudied. Representations can\nvary from having no 3D awareness (e.g., class label or bag\nof words) to accurately representing the 3D geometry of\nthe scene (e.g., 3D surface map or mesh). We posit that\n3D awareness can be evaluated through two distinct capa-\nbilities: single-view surface reconstruction and multiview\nconsistency. If a model is 3D aware, we expect that its repre-\nsentations would encode the geometry of the surfaces visible\nin the image; i.e., how far is each point in the scene? what\nis the orientation of the surface? Moreover, the representa-\ntions should be consistent for different views of the scene;\nallowing them to establish accurate correspondence.\n1\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n21795",
        "chosen": "This work analyzes the 3D awareness of visual foundation models through task-specific probes and zero-shot inference. It evaluates whether representations encode 3D structure and maintain multiview consistency. Experiments reveal several limitations in current models, indicating further research is needed to enhance their 3D understanding.",
        "rejected": "This paper explores the 3D awareness of visual foundation models. Through experiments on task-specific probes, it reveals that current models lack consistent 3D representation across views, despite their strong 2D capabilities. The study highlights the need for improved 3D understanding in these models."
    },
    {
        "prompt": "summary: We propose SketchINR, to advance the representation\nof vector sketches with implicit neural models.\nA vari-\nable length vector sketch is compressed into a latent space\nof fixed dimension that implicitly encodes the underlying\nshape as a function of time and strokes. The learned func-\ntion predicts the xy point coordinates in a sketch at each\ntime and stroke. Despite its simplicity, SketchINR outper-\nforms existing representations at multiple tasks: (i) Encod-\ning an entire sketch dataset into a fixed size latent vec-\ntor, SketchINR gives 60× and 10× data compression over\nraster and vector sketches, respectively. (ii) SketchINR’s\nauto-decoder provides a much higher-fidelity representa-\ntion than other learned vector sketch representations, and\nis uniquely able to scale to complex vector sketches such\nas FS-COCO. (iii) SketchINR supports parallelisation that\ncan decode/render ∼100× faster than other learned vector\nrepresentations such as SketchRNN. (iv) SketchINR, for the\nfirst time, emulates the human ability to reproduce a sketch\nwith varying abstraction in terms of number and complex-\nity of strokes. As a first look at implicit sketches, Sketch-\nINR’s compact high-fidelity representation will support fu-\nture work in modelling long and complex sketches.\n1. Introduction\nThe prevalence of touch-screen devices has triggered signif-\nicant research progress on sketches [4, 6, 8, 51, 57]. Mod-\nelling digital sketches has become an important challenge\nfor learning systems that aim to stimulate human creativity\n[17, 27]. Large-scale datasets [24, 28, 53] motivated sev-\neral downstream applications like image retrieval [15, 61],\nimage generation [37, 65], image editing [1, 41], and 3D\ncontent creation [5, 8, 38], among others [58, 60].\nThese applications are underpinned by sketches captured\nin raw form either as raster or vector images, and usu-\nally encoded into derived representations by ConvNets [15],\nsplines, and so on. For over a decade, discourse around\nwhat is a “good” representation of human sketches has\npersisted [31].\nA now substantial body of work has fo-\ncused on representations for sequential vector sketches that\nmodel both explicit strokes, and their drawing over time\n— most famously by the auto-regressive SketchRNN [28],\nbut also using representations such as parametric curves\n[16], Gaussians [2], and Neural ODEs [17]. We introduce\na novel continuous-time representation of sequential vec-\ntor sketches by taking an implicit neural representation per-\nspective on sketches for the first time.\nFirst, we review the limitations of current sketch repre-\nsentations. The two most popular raw sketch representa-\ntions are (i) raster sketch – a black and white image of a\nline drawing, and (ii) vector sketch – a temporal sequence\nof points and strokes. Raster sketch has a large but fixed\nstorage (e.g., 256 × 256). Its compatibility with ConvNets\n[64] made raster sketches popular but they lack the tempo-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n12565",
        "chosen": "SketchINR advances vector sketch representation with implicit neural models, compressing sketches into a latent space encoding shape as a function of time and strokes. It outperforms existing representations in data compression, fidelity, scalability, and rendering speed, supporting future work in modeling complex sketches.",
        "rejected": "SketchINR advances vector sketch representations using implicit neural models. It compresses variable-length sketches into a fixed latent space, enabling the encoding of shape details over time. This approach facilitates more effective processing and manipulation of vector sketches for various applications."
    },
    {
        "prompt": "summary: Pre-trained Vision Language Models (VLMs) have\ndemonstrated notable progress in various zero-shot tasks,\nsuch as classiﬁcation and retrieval. Despite their perfor-\nmance, because improving performance on new tasks re-\nquires task-speciﬁc knowledge, their adaptation is essen-\ntial. While labels are needed for the adaptation, acquiring\nthem is typically expensive. To overcome this challenge, ac-\ntive learning, a method of achieving a high performance by\nobtaining labels for a small number of samples from ex-\nperts, has been studied. Active learning primarily focuses\non selecting unlabeled samples for labeling and leverag-\ning them to train models. In this study, we pose the ques-\ntion, “how can the pre-trained VLMs be adapted under the\nactive learning framework?” In response to this inquiry,\nwe observe that (1) simply applying a conventional active\nlearning framework to pre-trained VLMs even may degrade\nperformance compared to random selection because of the\nclass imbalance in labeling candidates, and (2) the knowl-\nedge of VLMs can provide hints for achieving the balance\nbefore labeling. Based on these observations, we devise a\nnovel active learning framework for VLMs, denoted as PCB.\nTo assess the effectiveness of our approach, we conduct ex-\nperiments on seven different real-world datasets, and the\nresults demonstrate that PCB surpasses conventional active\nlearning and random sampling methods. Code is available\nat https://github.com/kaist-dmlab/pcb.\n1. Introduction\nIn the past, as emerging research in deep neural networks\n(DNNs) progressed, there was a substantial focus on study-\ning speciﬁc types of datasets, including image/video (vi-\nsion) [1, 10, 16], natural language [5, 54, 55], graph [63], ta-\nble [58], and more. However, recent research has raised the\nquestion: “can we develop DNNs capable of understand-\ning multiple types of datasets interactively?” Among vari-\nous candidates for multi-modality models, vision language\n⇤indicates corresponding author.\nmodels (VLMs) [31–33, 46, 59] have garnered signiﬁcant\nattention due to not only to their wide domain knowledge\nbut also to their superior performance on various tasks.\nMost of VLMs, for instance CLIP [46], comprises two\nencoders: image and text encoders. They have consistently\nshown impressive zero-shot performance across a wide\nrange of tasks without ﬁne-tuning. For example, CLIP is\nwell-known for its remarkable zero-shot classiﬁcation per-\nformance on various benchmarks, even if the model has not\nencountered the datasets previously. Despite these notable\nzero-shot performances, many researchers are focusing on\ndeveloping adaptation methods for new target tasks because\nof necessity to make the model aware of the target tasks.\nSince updating all parameters can be computationally ex-\npensive, a key research focus lies in reducing the adapta-\ntion computing cost [23, 66, 67]. For example, CoOp [66]\ntakes the approach of freezing both encoders and only al-\nlowing a small number of trainable parameters (with a size\nranging from 4 to 16) to serve as prompts. This strategy\nhas demonstrated substantial improvements in classiﬁcation\nperformance with only a small number of trainable param-\neters and a limited amount of data for each class.\nEven though we can reduce the adpation cost, the barrier\nof high labeling costs still persists. To mitigate this inefﬁ-\nciency, there have been extensive studies in an area of active\nlearning [48, 51]. The central objective of active learning\nis to select samples for labeling so that the model perfor-\nmance is signiﬁcantly improved, making a noticebale gap\ncompared to random samples of the same quantity. These\nactive learning methods can be roughly divided into two cat-\negories: (1) uncertainty-based sampling [12, 18, 19, 25, 47]\nand (2) diversity-based sampling [43, 50] which leverages\nfeature embeddings from the image encoder. In a hybrid\nperspective, BADGE [2] was introduced by combining un-\ncertainty and diversity through the use of k-means++ clus-\ntering within the gradient embedding space.\nUnder these two researches, our initial inquiry pertains\nto the determination of whether the implementation sim-\nply combining active learning with VLMs can effectively\nlead to enhanced classiﬁcation performance. If it does not\nresult in such improvement, what constitutes the critical in-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n27004",
        "chosen": "A novel active learning framework for pre-trained Vision Language Models (VLMs) is proposed, addressing class imbalance issues in labeling candidates. This framework, called PCB, leverages VLM knowledge before labeling to achieve better balance. Experiments on seven real-world datasets show PCB surpasses conventional active learning and random sampling methods.",
        "rejected": "SketchINR advances the representation of vector sketches through implicit neural models. It compresses variable-length sketches into a fixed-dimension latent space, encoding the underlying shape as a function of time and structure. This method facilitates efficient processing and manipulation of vector sketches, enhancing their application in various domains."
    },
    {
        "prompt": "summary: Despite being (pre)trained on a massive amount of data,\nstate-of-the-art video-language alignment models are not\nrobust to semantically-plausible contrastive changes in the\nvideo captions.\nOur work addresses this by identifying\na broad spectrum of contrast misalignments, such as re-\nplacing entities, actions, and flipping event order, which\nalignment models should be robust against. To this end,\nwe introduce the VideoCon, a video-language alignment\ndataset constructed by a large language model that gen-\nerates plausible contrast video captions and explanations\nfor differences between original and contrast video cap-\ntions.\nThen, a generative video-language model is fine-\ntuned with VideoCon to assess video-language entailment\nand generate explanations. Our VideoCon-based alignment\nmodel significantly outperforms current models. It exhibits\na 12-point increase in AUC for the video-language align-\nment task on human-generated contrast captions. Finally,\nour model sets new state of the art zero-shot performance\nin temporally-extensive video-language tasks such as text-\nto-video retrieval (SSv2-Temporal) and video question an-\nswering (ATP-Hard). Moreover, our model shows superior\nperformance on novel videos and human-crafted captions\nand explanations.\n1. Introduction\nSemantically aligning data points from diverse modalities\nis a long-standing goal of AI. We focus on video-language\nalignment, which is challenging due to the complexities in-\nvolved in understanding of entities, their relationships, and\ntemporal order of the depicted events [17]. Recent mod-\nels such as VideoCLIP [55], ImageBind [14] learn a shared\nembedding space.\nSimilarly, generative models such as\nFlamingo [1], mPLUG-Owl-Video [61] can provide a clas-\nsification label (e.g., yes/no) when queried about video-\nlanguage alignment.\n*Equal Advising.\nDespite large-scale pretraining, prior work [5, 37, 38, 51]\nhighlights that video-language alignment models are not ro-\nbust to semantically plausible manipulations to an original\naligned caption in the form of contrast captions, such as\nfrom ‘dog runs away before it eats food’ to ‘dog runs away\nafter it eats food’. Such pitfalls in robustness questions the\ntrustworthiness of alignment models for large-scale deploy-\nment. To mitigate these shortcomings, one possible solution\nis to scale video-language pairs more for increased diver-\nsity during pretraining. However, this is challenging due to\nthe difficulties in sourcing new, high-quality and permissi-\nble content, as well as the requirement for substantial stor-\nage capacity. Several works [11, 13, 16] have shown that\nnaively training models on web-scale data has diminishing\nreturns on downstream tasks, and emphasize the importance\nof data quality. Furthermore, the recent studies [28, 62]\ndemonstrate that applying a contrastive objective to the pre-\ntraining datasets does not encourage the model to grasp the\nfine-grained details within image/region-caption data.\nTo this end, we take a scalable, active strategy to gather\nhigh-quality data that is deliberately enriched with the at-\ntributes that we want to instill in alignment models. We\ncreate a novel dataset, VideoCon, to improve the robust-\nness of models. Specifically, the dataset consists of a vari-\nety of semantically plausible video-language misalignments\nin contrast captions.\nThese misalignments include alter-\ning objects (entities), actions, attributes, relations, counts,\nevent orders, and introducing hallucinations (Figure 2).\nTo construct VideoCon, a large language model (PaLM-\n2 API) takes video-caption pairs as input and generates\nhigh-quality contrast captions for a given misalignment\ntype.\nTo make our dataset temporally-challenging, we\nskipped “easy” video-caption pairs whose alignment could\nbe inferred based on a single frame (image) understanding\n[9, 26] (§3.1). In addition, the LLM generates natural lan-\nguage explanations (NLEs) [42] to the differences between\noriginal and altered captions, which are used for further ro-\nbust training. We performed human verification on a sam-\nple of VideoCon and found that it is of high-quality. Finally,\nto evaluate the model’s generalization capabilities, we col-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n13927",
        "chosen": "VideoCon is a video-language alignment dataset constructed by a large language model, generating plausible contrast video captions and explanations for differences. A generative video-language model fine-tuned with VideoCon significantly outperforms current models, achieving a 12-point increase in AUC for the video-language alignment task and setting new state-of-the-art zero-shot performance in temporally-extensive video-language tasks.",
        "rejected": "In addressing video-language alignment robustness, VideoCon dataset is introduced, containing semantically plausible contrastive captions. Fine-tuned models on VideoCon show significant improvement in AUC and zero-shot performance on tasks like text-to-video retrieval and video question answering."
    },
    {
        "prompt": "summary: Recently, we have witnessed the explosive growth of\nvarious volumetric representations in modeling animatable\nhead avatars. However, due to the diversity of frameworks,\nthere is no practical method to support high-level applica-\ntions like 3D head avatar editing across different represen-\ntations. In this paper, we propose a generic avatar editing\napproach that can be universally applied to various 3DMM-\ndriving volumetric head avatars. To achieve this goal, we\ndesign a novel expression-aware modification generative\nmodel, which enables lift 2D editing from a single image to\na consistent 3D modification field. To ensure the effective-\nness of the generative modification process, we develop sev-\neral techniques, including an expression-dependent mod-\nification distillation scheme to draw knowledge from the\nlarge-scale head avatar model and 2D facial texture editing\ntools, implicit latent space guidance to enhance model con-\nvergence, and a segmentation-based loss reweight strategy\nfor fine-grained texture inversion. Extensive experiments\ndemonstrate that our method delivers high-quality and con-\nsistent results across multiple expression and viewpoints.\nProject page: https://zju3dv.github.io/geneavatar/.\n1. Introduction\nRecently various volumetric representations [3, 4, 15, 16,\n57, 68, 69, 76] have achieved remarkable success in re-\nconstructing personalized, animatable, and photorealistic\nhead avatars using implicit [15, 16, 57, 68, 69] or ex-\nplicit [3, 4, 76] conditioning of 3D Morphable Models\n*Authors contributed equally.\n†Corresponding authors.\n§The work was partially done when visiting ETHZ.\n(3DMM) [6]. A popular demand, once with a created avatar\nmodel, is to edit the avatar, e.g., for face shape, facial\nmakeup, or apply artistic effects, for the downstream ap-\nplications, e.g., in virtual/augmented reality.\nIdeally, the desired editing functionality on the animat-\nable avatar should have the following properties. (1) Adapt-\nable: The editing method should be applicable across var-\nious volumetric avatar representations. This is particularly\nvaluable in light of the growing diversity of avatar frame-\nworks [16, 50, 76]. (2) User-friendly: The editing should\nbe user-friendly and intuitive. Preferably, the editing of ge-\nometry and texture of the 3D avatar could be accomplished\non a single-perspective rendered image. (3) Faithful: The\nediting results should be consistent across various facial ex-\npression and camera viewpoints. (4) Flexible: Both inten-\nsive editing (e.g., global appearance transfer following style\nprompts) and delicate local editing (e.g., dragging to enlarge\neyes or ears) should be supported as illustrated in Fig. 1.\nHowever, 3D-aware avatar editing is still underexplored\nin both geometry and texture. One plausible way is to per-\nform 3D editing via animatable 3D GAN [50, 52, 54], but\nthe editing results may not be consistently reflected when\nexpression and camera viewpoint change. Alternatively, the\nediting can be done on the generated 2D video using 2D\npersonalized StyleGAN [28]; however, the identity shift is\noften observed. Some face-swapping methods [11, 12, 42]\nare capable of substituting the face in a video with another\nface derived from a reference image or video; however, they\ndo not support texture editing and local geometry editing.\nTo this end, we propose GeneAvatar – a generic ap-\nproach to support fine-grained 3D editing in various vol-\numetric avatar representations from a single perspective by\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8952",
        "chosen": "GeneAvatar is a generic avatar editing approach for various 3DMM-driven volumetric head avatars. It uses an expression-aware modification generative model to lift 2D editing from a single image to a consistent 3D modification field. The method includes techniques like expression-dependent modification distillation, implicit latent space guidance, and segmentation-based loss reweighting, delivering high-quality and consistent results across multiple expressions and viewpoints.",
        "rejected": "GeneAvatar proposes a novel approach to edit 3D head avatars across various volumetric representations. It employs an expression-aware modification generative model that ensures consistency across expressions and viewpoints, supporting both intensive and delicate editing from single-perspective images."
    },
    {
        "prompt": "summary: Adversarial attacks aim to perturb images such that a pre-\ndictor outputs incorrect results. Due to the limited research\nin structured attacks, imposing consistency checks on natural\nmulti-object scenes is a practical defense against conven-\ntional adversarial attacks. More desired attacks should be\nable to fool defenses with such consistency checks. Therefore,\nwe present the first approach GLOW that copes with various\nattack requests by generating global layout-aware adversar-\nial attacks, in which both categorical and geometric layout\nconstraints are explicitly established. Specifically, we focus\non object detection tasks and given a victim image, GLOW\nfirst localizes victim objects according to target labels. And\nthen it generates multiple attack plans, together with their\ncontext-consistency scores. GLOW, on the one hand, is ca-\npable of handling various types of requests, including single\nor multiple victim objects, with or without specified victim\nobjects. On the other hand, it produces a consistency score\nfor each attack plan, reflecting the overall contextual consis-\ntency that both semantic category and global scene layout\nare considered. We conduct our experiments on MS COCO\nand Pascal. Extensive experimental results demonstrate that\nwe can achieve about 30% average relative improvement\ncompared to state-of-the-art methods in conventional single\nobject attack request; Moreover, such superiority is also\nvalid across more generic attack requests, under both white-\nbox and zero-query black-box settings. Finally, we conduct\ncomprehensive human analysis, which not only validates\nour claim further but also provides strong evidence that our\nevaluation metrics reflect human reviews well.\n1. Introduction\nObject detection aims to localize and recognise multiple\nobjects in given images with their 2D bounding boxes and\ncorresponding semantic categories [14, 19]. Due to the phys-\nical commonsense and viewpoint preferences [16], detected\n*Equal contribution.\n†Corresponding author.\nFigure 1. We propose a novel attack generation algorithm GLOW\nthat manages both conventional single targeted object (R1) and our\ngeneric attack requests (R2,R3). Specifically, GLOW consists of\ntwo steps. The first step localizes victim objects, if not provided.\nThe second step generates various attack plans with their consis-\ntency scores. Then the one with the highest score is our final attack\nplan and parsed to attackers. Best viewed in color.\nbounding boxes in natural images are not only semantically\nlabeled but also placed relative to each other within a co-\nherent scene geometry, reflecting the underlying 3D scene\nstructure. Such bounding box representation allows us to\nderive a notion of both semantic and geometric constraints.\nFor example, co-occurrence matrix is a commonly exploited\nsemantic constraint where certain object categories are more\nlikely to co-occur, e.g., bed and pillow [20]. Geometric\nconstraints, on the other hand, leverage the inductive bias\nof scene layout [11], such as when oc-occurring in a scene,\ntraffic light is more likely to be appeared on the upper region\nwith a smaller bounding box compared to car.\nAdversarial attacks on object detectors mainly focus on\ntargeted victim setting [7, 45] where the goal is to perturb a\nspecific victim object to target class. In this case, the location,\nground truth and target class of the victim object are assumed\nto be known to attackers. Naturally, contextual cues are lever-\naged in attack and defense mechanisms [6, 7, 48] on detec-\ntors to enhance or detect holistic context (in)consistency [6].\nThough being well-motivated and demonstrating good per-\nformances in conventional setting, the state-of-the-art meth-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n12057",
        "chosen": "GLOW is the first approach to generate global layout-aware adversarial attacks on object detection, establishing categorical and geometric layout constraints. It localizes victim objects and generates multiple attack plans with context-consistency scores. Experiments on MS COCO and Pascal show GLOW achieves about 30% average relative improvement compared to state-of-the-art methods in conventional single object attack requests and is valid across more generic attack requests under both white-box and zero-query black-box settings.",
        "rejected": "GLOW introduces a global layout-aware adversarial attack method targeting object detection tasks. By generating attacks with categorical and geometric constraints, GLOW achieves about 30% average relative improvement over state-of-the-art methods, proving effective under white-box and black-box settings."
    },
    {
        "prompt": "summary: Conditional human motion generation is an important\ntopic with many applications in virtual reality, gaming, and\nrobotics. While prior works have focused on generating mo-\ntion guided by text, music, or scenes, these typically result\nin isolated motions confined to short durations. Instead,\nwe address the generation of long, continuous sequences\nguided by a series of varying textual descriptions. In this\ncontext, we introduce FlowMDM, the first diffusion-based\nmodel that generates seamless Human Motion Composi-\ntions (HMC) without any postprocessing or redundant de-\nnoising steps. For this, we introduce the Blended Positional\nEncodings, a technique that leverages both absolute and\nrelative positional encodings in the denoising chain. More\nspecifically, global motion coherence is recovered at the ab-\nsolute stage, whereas smooth and realistic transitions are\nbuilt at the relative stage. As a result, we achieve state-of-\nthe-art results in terms of accuracy, realism, and smooth-\nness on the Babel and HumanML3D datasets. FlowMDM\nexcels when trained with only a single description per mo-\ntion sequence thanks to its Pose-Centric Cross-ATtention,\nwhich makes it robust against varying text descriptions at\ninference time. Finally, to address the limitations of existing\nHMC metrics, we propose two new metrics: the Peak Jerk\nand the Area Under the Jerk, to detect abrupt transitions.\n1. Introduction\nIn the field of computer vision, recent progress has been\nmade in developing photorealistic avatars [53] for appli-\ncations like virtual reality, gaming, and robotics [60, 76].\nAside from looking visually realistic, avatars must also\nmove in a convincing manner. This is challenging due to\nthe intricate nature of human motion, strongly influenced\nby factors such as the environment, interactions, and phys-\nical contact [14]. Furthermore, complexity increases when\nattempting to control these motions. Recent advances in-\nclude the generation of motion sequences from control sig-\nnals like text descriptions or actions [106]; however, such\nmethods only produce isolated, standalone motion. There-\nfore, these approaches fail to handle scenarios where a long\nmotion is driven by distinct control signals on different time\nslices. Such capability is needed to provide full control over\nthe sequence of desired actions and their duration. In these\nscenarios, the generated motion needs to feature seamless\nand realistic transitions between actions. In this work, we\ntackle this problem, which we refer to as generative Hu-\nman Motion Composition (HMC). In particular, we focus\non generating single-human motion from text (Fig. 1).\nOne of the primary obstacles in HMC is the lack of\ndatasets that offer long motion sequences with diverse tex-\ntual annotations.\nExisting datasets typically feature se-\nquences of limited duration, often lasting only up to 10 sec-\nonds, and with just a single control signal governing the en-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n457",
        "chosen": "FlowMDM is the first diffusion-based model generating seamless Human Motion Compositions without postprocessing. It uses Blended Positional Encodings for global motion coherence and smooth transitions, achieving state-of-the-art results on Babel and HumanML3D datasets. FlowMDM is robust against varying text descriptions at inference time due to its Pose-Centric Cross-ATtention.",
        "rejected": "FlowMDM introduces diffusion-based human motion generation driven by textual descriptions. Utilizing blended positional encodings, it achieves seamless transitions between actions, demonstrating state-of-the-art results in accuracy, realism, and smoothness on benchmarks like Babel and HumanML3D."
    },
    {
        "prompt": "summary: Training a linear classifier or lightweight model on top\nof pretrained vision model outputs, so-called ‘frozen fea-\ntures’, leads to impressive performance on a number of\ndownstream few-shot tasks. Currently, frozen features are\nnot modified during training.\nOn the other hand, when\nnetworks are trained directly on images, data augmenta-\ntion is a standard recipe that improves performance with\nno substantial overhead. In this paper, we conduct an ex-\ntensive pilot study on few-shot image classification that ex-\nplores applying data augmentations in the frozen feature\nspace, dubbed ‘frozen feature augmentation (FroFA)’, cov-\nering twenty augmentations in total.\nOur study demon-\nstrates that adopting a deceptively simple pointwise FroFA,\nsuch as brightness, can improve few-shot performance con-\nsistently across three network architectures, three large pre-\ntraining datasets, and eight transfer datasets.\n1. Introduction\nVision transformers (ViTs) [19] achieve remarkable perfor-\nmance on ImageNet-sized [43, 69] and smaller [21, 38, 41]\ndatasets. In this setup, data augmentation, i.e., a predefined\nset of stochastic input transformations, is a crucial ingredi-\nent. Examples for image augmentations are random crop-\nping or pixel-wise modifications that change brightness or\ncontrast. These are complemented by more advanced strate-\ngies [13, 46, 75], such as AutoAugment [12].\nA more prevalent trend is to first pretrain vision mod-\nels on large-scale datasets and then adapt them downstream\n[6, 8, 49, 73]. Notable, even training a simple linear classi-\nfier or lightweight model on top of ViT outputs, also known\nas frozen features, can yield remarkable performance across\na number of diverse downstream few-shot tasks [16, 25, 52].\nGiven the success of image augmentations and frozen fea-\ntures, we ask: Can we effectively combine image augmen-\ntations and frozen features to train a lightweight model?\n*Work done as Research Intern at Google DeepMind. †Project lead.\n1\n5\n10\n25\nshots\n0\n2\n4\n6\ntop-1 accuracy\n(absolute gains)\nJFT-3B\n1\n5\n10\n25\nshots\nWebLI + SigLIP\nMAPwd\nlinear probe\nFigure 1. Average top-1 accuracy gains across seven few-shot test\nsets (CIFAR100 [1], SUN397 [71], ...) on various few-shot set-\ntings. We train on frozen features from an L/16 ViT [19] with\nJFT-3B pretraining [73] or WebLI sigmoid language-image pre-\ntraining (SigLIP) [6, 74]. Our proposed frozen feature augmenta-\ntion (FroFA) method gives consistent gains over a weight decay-\nregularized multi-head attention pooling [37] (MAPwd) and an L2-\nregularized linear probe baseline, both without FroFA.\nIn this paper, we revisit standard image augmentation\ntechniques and apply them on top of frozen features in a\ndata-constrained, few-shot setting. We dub this type of aug-\nmentation frozen feature augmentation (FroFA). Inspired di-\nrectly by image augmentations, we first stochastically trans-\nform frozen features and then train a lightweight model on\ntop. Our only modification before applying image augmen-\ntations on top of frozen features is a point-wise scaling such\nthat each feature value lies in [0, 1] or [0, 255].\nWe investigate eight (few-shotted) image classification\ndatasets using ViTs pretrained on JFT-3B [73], ImageNet-\n21k [17], or WebLI [6]. After extracting features from each\nfew-shot dataset we apply twenty different frozen feature\naugmentations and train a lightweight multi-head attention\npooling (MAP) [37] on top. Our major insights are:\n1. Geometric augmentations that modify the shape and\nstructure of two-dimensional frozen features always lead\nto worse performance on ILSVRC-2012 [57]. On the\nother hand, simple stylistic (point-wise) augmentations,\nsuch as brightness, contrast, and posterize, give steady\nimprovements on 1-, 5-, and 10-shot settings.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n16046",
        "chosen": "FroFA explores applying data augmentations in the frozen feature space for few-shot image classification. The study shows that simple pointwise augmentations like brightness can consistently improve few-shot performance across different network architectures, pre-training datasets, and transfer datasets.",
        "rejected": "Frozen feature augmentation (FroFA) applies data augmentations to frozen features from pretrained vision models, improving few-shot learning performance. Simple point-wise augmentations such as brightness consistently enhance performance across multiple architectures and datasets."
    },
    {
        "prompt": "summary: Non-isometric shape correspondence remains a funda-\nmental challenge in computer vision. Traditional methods\nusing Laplace-Beltrami operator (LBO) eigenmodes face\nlimitations in characterizing high-frequency extrinsic shape\nchanges like bending and creases. We propose a novel ap-\nproach of combining the non-orthogonal extrinsic basis of\neigenfunctions of the elastic thin-shell hessian with the in-\ntrinsic ones of the LBO, creating a hybrid spectral space\nin which we construct functional maps. To this end, we\npresent a theoretical framework to effectively integrate non-\northogonal basis functions into descriptor- and learning-\nbased functional map methods. Our approach can be in-\ncorporated easily into existing functional map pipelines\nacross varying applications and can handle complex de-\nformations beyond isometries. We show extensive evalua-\ntions across various supervised and unsupervised settings\nand demonstrate significant improvements. Notably, our ap-\nproach achieves up to 15% better mean geodesic error for\nnon-isometric correspondence settings and up to 45% im-\nprovement in scenarios with topological noise. Code is avail-\nable at: https://hybridfmaps.github.io/\n∗Equal Contribution\n\u0000 : {lennart.bastian,yizheng.xie}@tum.de\n1. Introduction\nEstablishing dense correspondences between 3D shapes is\na cornerstone for numerous computer vision and graphics\ntasks such as object recognition, character animation, and\ntexture transfer. The complexity of this task varies signifi-\ncantly depending on the nature of the transformation a shape\nundergoes. Many classic correspondence methods leverage\nthat rigid transformations can be represented in six degrees\nof freedom in R3 and preserve the Euclidean distance be-\ntween pairs of points. Iterative closest point (ICP) [4], and\nits variations [25, 40], which alternate between transforma-\ntion and correspondence estimation, are very popular due\nto their simplicity. In this setting, local extrinsic surface\nproperties in the embedding space stay invariant under rigid\ntransformations such that they can be used as features during\noptimization, for example, the change of normals. For the\nwider class of isometric deformations (w.r.t. the geodesic\ndistance), the relative embedding of the shape can change\nsignificantly, and Euclidean distances between points may\nnot be preserved. In this class, only intrinsic properties –\nthose that do not depend on a specific embedding of the\nsurface – stay invariant, and the correspondence problem\nbecomes much harder due to the quadratic size of the so-\nlution space. For example, solving a quadratic assignment\nproblem preserving geodesic distances [24] or heat kernel\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n3313",
        "chosen": "This novel approach combines non-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell hessian with intrinsic Laplace-Beltrami operator eigenmodes to create a hybrid spectral space for functional maps. It effectively integrates non-orthogonal basis functions into descriptor- and learning-based methods, handling complex deformations beyond isometries and showing significant improvements in non-isometric correspondence settings and scenarios with topological noise.",
        "rejected": "A novel shape correspondence approach integrates non-orthogonal extrinsic basis functions with intrinsic Laplace-Beltrami operator eigenmodes. This hybrid spectral space method shows up to 15% better mean geodesic error for non-isometric shapes and handles topological noise effectively."
    },
    {
        "prompt": "summary: Point cloud matching, a crucial technique in computer\nvision, medical and robotics ﬁelds, is primarily concerned\nwith ﬁnding correspondences between pairs of point clouds\nor voxels.\nIn some practical scenarios, emphasizing lo-\ncal differences is crucial for accurately identifying a cor-\nrect match, thereby enhancing the overall robustness and\nreliability of the matching process. Commonly used shape\ndescriptors have several limitations and often fail to pro-\nvide meaningful local insights about the paired geome-\ntries.\nIn this work, we propose a new technique, based\non graph Laplacian eigenmaps, to match point clouds by\ntaking into account ﬁne local structures. To deal with the\norder and sign ambiguity of Laplacian eigenmaps, we in-\ntroduce a new operator, called Coupled Laplacian1, that\nallows to easily generate aligned eigenspaces for multiple\nregistered geometries. We show that the similarity between\nthose aligned high-dimensional spaces provides a locally\nmeaningful score to match shapes. We ﬁrstly evaluate the\nperformance of the proposed technique in a point-wise man-\nner, focusing on the task of object anomaly localization on\nthe MVTec 3D-AD dataset. Additionally, we deﬁne a new\nmedical task, called automatic Bone Side Estimation (BSE),\nwhich we address through a global similarity score derived\nfrom coupled eigenspaces. In order to test it, we propose a\nbenchmark collecting bone surface structures from various\npublic datasets. Our matching technique, based on Cou-\npled Laplacian, outperforms other methods by reaching an\nimpressive accuracy on both tasks.\n1. Introduction\nPoint cloud matching, or more generally 3D shape match-\ning, is a fundamental task in computer vision. It involves\n⇤Corresponding author: matteo.bastico@minesparis.psl.eu\n1Code: https://github.com/matteo-bastico/CoupLap\nﬁnding the closest matching geometry to a target shape\nwithin a set of reference shapes [65]. In addition, if the\ntask involves ﬁnding rigid transformations that best align\nthe target shape with the reference, it is often part of a reg-\nistration process. In particular, point-set rigid registration\ndetermines the relative transformation needed to align two\npoint clouds without altering their internal structures [41].\nThis problem is essential for many practical computer vi-\nsion tasks, such as medical image analysis [3, 32, 48, 61],\nintelligent vehicles [21, 34], human pose estimation [22]\nand objects retrieval and tracking [46, 64].\nTraditional\n[8, 19, 57] and probabilistic registration and matching meth-\nods [15, 20, 30, 45], while robust, often struggle to opti-\nmally align complex geometries, especially in cases with\nintricate local structures or slight deformations.\nOver the years, several methods have been proposed\nto tackle the challenge of accurate and efﬁcient 3D shape\nmatching and retrieval [5, 9, 11, 51, 53, 65, 73]. Data-driven\n3D shape descriptors [54], capturing underlying properties\nof the shapes under study, are the common denominator of\nearly shape matching techniques. Global descriptors, such\nas volume and areas descriptors [78], describe the entirety\nof the shape in one compact representation, often failing\nto capture local ﬁne details of complex geometries. On the\nother hand, local descriptors [38, 57] aim to tackle this issue\nbut they generally are sensitive to noise, based on landmarks\nand they might not capture semantic information [63]. More\nrecently, deep-learned shape descriptors [6, 72] and neural\nnetworks for shape matching, based on auto-encoders [73]\nor transformers [59, 68], have been proposed. Despite their\ngood performances, these methods require a huge amount\nof annotated data for training, which are hard to collect in\nﬁelds such as medical imaging [37]. Furthermore, non-rigid\npoint cloud matching and retrieval methods [35, 36, 71] are\ndesigned to handle shape deformations and, therefore, they\nmight be excessively ﬂexible ignoring ﬁne local details that\nare not due to deformations, such as anomalies.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n3447",
        "chosen": "A new technique based on graph Laplacian eigenmaps is proposed for point cloud matching, introducing a Coupled Laplacian operator to generate aligned eigenspaces for multiple registered geometries. The similarity between aligned high-dimensional spaces provides a locally meaningful score for shape matching, outperforming other methods in object anomaly localization and automatic Bone Side Estimation tasks.",
        "rejected": "Using graph Laplacian eigenmaps, a new point cloud matching technique addresses local structure differences. The Coupled Laplacian operator aligns eigenspaces, providing meaningful similarity scores. It outperforms existing methods in object anomaly localization and introduces Bone Side Estimation for medical applications."
    },
    {
        "prompt": "summary: In recent years, automated Gallbladder Cancer (GBC)\ndetection has gained the attention of researchers.\nCur-\nrent state-of-the-art (SOTA) methodologies relying on ultra-\nsound sonography (US) images exhibit limited generaliza-\ntion, emphasizing the need for transformative approaches.\nWe observe that individual US frames may lack sufficient\ninformation to capture disease manifestation. This study\nadvocates for a paradigm shift towards video-based GBC\ndetection, leveraging the inherent advantages of spatiotem-\nporal representations. Employing the Masked Autoencoder\n(MAE) for representation learning, we address shortcom-\nings in conventional image-based methods.\nWe propose\na novel design called FocusMAE to systematically bias\nthe selection of masking tokens from high-information re-\ngions, fostering a more refined representation of malig-\nnancy. Additionally, we contribute the most extensive US\nvideo dataset for GBC detection. We also note that, this\nis the first study on US video-based GBC detection. We\nvalidate the proposed methods on the curated dataset, and\nreport a new SOTA accuracy of 96.4% for the GBC de-\ntection problem, against an accuracy of 84% by current\nImage-based SOTA – GBCNet and RadFormer, and 94.7%\nby Video-based SOTA – AdaMAE. We further demonstrate\nthe generality of the proposed FocusMAE on a public CT-\nbased Covid detection dataset, reporting an improvement\nin accuracy by 3.3% over current baselines. Project page\nwith source code, trained models, and data is available at:\nhttps://gbc-iitd.github.io/focusmae.\n1. Introduction\nGallbladder Cancer (GBC). Lately, automated GBC de-\ntection has drawn an increased interest from the researchers\n[5,6,10,31]. GBC is difficult to detect at an early stage [27],\nand surgical resection becomes infeasible for most patients\nas the disease gets detected at a late stage. As a result, the\n* Soumen is currently affiliated to Samsung R&D Institute Bangalore\n† Joint first authors\n(a)\n(b)\nFigure 1. (a) Masking strategy of FocusMAE in comparison to\nexisting random patch [14], frame [50], tube [45] masking. Our\napproach selects more tokens from the semantically meaningful\nregions with a small number of background tokens for masking.\n(b) Inflating the masking probability of the tokens which spatially\nlie within the object region (gray region) by π increases the accu-\nracy. However, excessive masking of the object region degrades\nperformance. Blue line: accuracy of the original random masking.\ndisease shows bleak survival statistics. The 5-year survival\nrate for patients with advanced GBC is only 5%, and the\nmean survival time is six months [24,40]. Hence, early de-\ntection of GBC is crucial for timely intervention and im-\nproving the survival rate [26].\nUltrasound (US) for GBC Detection. US has been the\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n11715",
        "chosen": "FocusMAE is a novel design for video-based Gallbladder Cancer detection, systematically biasing masking token selection from high-information regions to refine malignancy representation. The method achieves a new state-of-the-art accuracy of 96.4% on the curated dataset and demonstrates generality on a public CT-based Covid detection dataset.",
        "rejected": "FocusMAE enhances masked autoencoders for ultrasound-based Gallbladder Cancer detection using a spatiotemporal approach. By biasing masking towards high-information regions, it achieves 96.4% accuracy, surpassing current image-based and video-based SOTA methods. Code and dataset are available."
    },
    {
        "prompt": "summary: We propose a novel method for 3D point cloud ac-\ntion recognition.\nUnderstanding human actions in RGB\nvideos has been widely studied in recent years, however,\nits 3D point cloud counterpart remains under-explored de-\nspite the clear value that 3D information may bring. This\nis mostly due to the inherent limitation of the point cloud\ndata modality—lack of structure, permutation invariance,\nand varying number of points—which makes it difﬁcult to\nlearn a spatio-temporal representation. To address this lim-\nitation, we propose the 3DinAction pipeline that ﬁrst esti-\nmates patches moving in time (t-patches) as a key build-\ning block, alongside a hierarchical architecture that learns\nan informative spatio-temporal representation.\nWe show\nthat our method achieves improved performance on existing\ndatasets, including DFAUST and IKEA ASM. Code is pub-\nlicly available at https://github.com/sitzikbs/3dincaction.\n1. Introduction\nIn this paper, we address the task of action recognition\nfrom 3D point cloud sequences.\nWe propose a novel\npipeline wherein points are grouped into temporally evolv-\ning patches that capture discriminative action dynamics.\nOur work is motivated by the massive growth of online me-\ndia, mobile and surveillance cameras that have enabled the\ncomputer vision community to develop many data-driven\naction-recognition methods [5, 12, 26, 31], most of which\nrely on RGB video data. Recently, commodity 3D sensors\nare gaining increased momentum, however, the 3D point\ncloud modality for action recognition has yet been under-\nexploited due to the scarcity of 3D action-labeled data.\nIn many cases, a pure RGB video-based inference may\nnot be enough and incorporating other modalities like ge-\nometry is required. This is especially necessary for safety\ncritical applications such as autonomous systems, where\nredundancy is crucial, or in scenarios where the video is\nheavily degraded (e.g., due to poor lighting).\nSome ap-\nproaches incorporate geometrical information implicitly,\ne.g., through intermediate pose estimation [7]. This often\nentails extra steps that require more time and resources and\nis still limited to video input. Therefore a more explicit ap-\nproach is desirable.\n3D sensors provide an alternative modality in the form of\npoint clouds sampled on the environment. Despite the vast\nresearch on 3D vision and learning, even static 3D point\ncloud datasets are signiﬁcantly smaller than their RGB im-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n19978",
        "chosen": "3DinAction is a novel method for 3D point cloud action recognition, proposing a pipeline that estimates patches moving in time (t-patches) and uses a hierarchical architecture to learn spatio-temporal representations. The method improves performance on existing datasets like DFAUST and IKEA ASM.",
        "rejected": "This study investigates non-isometric shape correspondence using a hybrid spectral space combining intrinsic and extrinsic basis functions. The proposed method significantly improves upon traditional approaches, particularly in handling complex deformations and topological noise."
    },
    {
        "prompt": "summary: Domain Generalized Semantic Segmentation (DGSS)\ndeals with training a model on a labeled source domain\nwith the aim of generalizing to unseen domains during in-\nference. Existing DGSS methods typically effectuate robust\nfeatures by means of Domain Randomization (DR). Such an\napproach is often limited as it can only account for style\ndiversification and not content. In this work, we take an\northogonal approach to DGSS and propose to use an as-\nsembly of CoLlaborative FOUndation models for Domain\nGeneralized Semantic Segmentation (CLOUDS). In detail,\nCLOUDS is a framework that integrates Foundation Mod-\nels of various kinds: (i) CLIP backbone for its robust feature\nrepresentation, (ii) Diffusion Model to diversify the con-\ntent, thereby covering various modes of the possible tar-\nget distribution, and (iii) Segment Anything Model (SAM)\nfor iteratively refining the predictions of the segmentation\nmodel. Extensive experiments show that our CLOUDS ex-\ncels in adapting from synthetic to real DGSS benchmarks\nand under varying weather conditions, notably outperform-\ning prior methods by 5.6% and 6.7% on averaged mIoU, re-\nspectively. Our code is available at https://github.\ncom/yasserben/CLOUDS\n1. Introduction\nDeep Neural Networks have showcased remarkable abil-\nity in scene understanding tasks like Semantic Segmenta-\ntion (SS) [7, 70, 72], when the training and test distribu-\ntion are the same. This dependency reveals a significant\nvulnerability: their performance substantially diminishes\nwhen encountering domain shifts [66], highlighting a fun-\ndamental challenge in generalizing these networks to un-\nseen domains [43, 46, 61]. To address this, Domain Gen-\neralized Semantic Segmentation (DGSS) has gained promi-\nnence [28, 31, 33, 82]. DGSS aims to develop models that\nleverage a source-annotated dataset while remaining effec-\nmIoU\nYear\nTraditional DGSS\nOpen-vocabulary (zero-shot)\nDGSS with foundation models\nOurs\nFC-CLIP\nCATSeg\nHRDA\nMoDify\nDRPC\nSHADE\nODISE\n2019\n2020\n2021\n2022\n2023\n60\n50\n40\n55\n45\nFSDR\nGTR\nTLDR\nFigure 1.\nPerformance over time by various methods on the\nGTA →{Cityscapes, BDD, Mapillary} benchmark. Recent open-\nvocabulary approaches, like FC-CLIP, are shown to excel in zero-\nshot learning and surpass traditional domain generalization meth-\nods trained in closed-set scenarios, challenging the relevance of\nthe DGSS setting. CLOUDS, by harnessing multiple foundation\nmodels, demonstrates its ability to effectively utilize the source\ndataset, thereby outperforming both conventional DGSS and open-\nvocabulary methods.\ntive across unseen domains, thus overcoming the limitations\nof traditional DNNs in handling unseen environments.\nRecently, the advent of large-scale pretrained models,\noften referred to as Foundation Models (FMs) [2, 35, 56,\n59, 67], have brought a paradigm shift in computer vi-\nsion tasks. The FMs comprise of contrastively trained im-\nage classification models (e.g., CLIP [56], ALIGN [30]),\ntext-conditioned generative models (e.g., Stable Diffusion\n[59], DALL-E [57]), vision transformer-based segmenta-\ntion model trained on mammoth dataset (e.g., Segment\nAnything Model (SAM) [35]), to name a few.\nOf par-\nticular interest to SS, SAM is a promptable segmenta-\ntion model that accepts as input an image and geometric\nprompts (points, scribbles, boxes, masks) and outputs class-\nagnostic masks. Owing to its pre-training on billion-scale\ndataset [35], SAM has demonstrated excellent performance\non varied out-of-distribution tasks namely, medical imaging\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n3108",
        "chosen": "CLOUDS is a framework integrating various foundation models for Domain Generalized Semantic Segmentation, including CLIP for feature representation, Diffusion Model for content diversification, and Segment Anything Model for prediction refinement. It outperforms prior methods by 5.6% and 6.7% on averaged mIoU in adapting from synthetic to real DGSS benchmarks and under varying weather conditions.",
        "rejected": "To improve point cloud matching, a technique based on graph Laplacian eigenmaps is introduced, focusing on fine local structures. The Coupled Laplacian operator aligns eigenspaces, providing a locally meaningful score for shape matching. The method excels in tasks such as object anomaly localization and Bone Side Estimation."
    },
    {
        "prompt": "summary: Precise geospatial vegetation forecasting holds potential\nacross diverse sectors, including agriculture, forestry, hu-\nmanitarian aid, and carbon accounting. To leverage the\nvast availability of satellite imagery for this task, various\nworks have applied deep neural networks for predicting\nmultispectral images in photorealistic quality.\nHowever,\nthe important area of vegetation dynamics has not been\nthoroughly explored. Our study introduces GreenEarthNet,\nthe first dataset specifically designed for high-resolution\nvegetation forecasting, and Contextformer, a novel deep\nlearning approach for predicting vegetation greenness from\nSentinel 2 satellite images with fine resolution across Eu-\nrope. Our multi-modal transformer model Contextformer\nleverages spatial context through a vision backbone and\npredicts the temporal dynamics on local context patches\nincorporating meteorological time series in a parameter-\nefficient manner.\nThe GreenEarthNet dataset features a\nlearned cloud mask and an appropriate evaluation scheme\nfor vegetation modeling.\nIt also maintains compatibil-\nity with the existing satellite imagery forecasting dataset\nEarthNet2021, enabling cross-dataset model comparisons.\nOur extensive qualitative and quantitative analyses reveal\nthat our methods outperform a broad range of baseline\ntechniques. This includes surpassing previous state-of-the-\nart models on EarthNet2021, as well as adapted models\nfrom time series forecasting and video prediction. To the\nbest of our knowledge, this work presents the first mod-\nels for continental-scale vegetation modeling at fine reso-\nlution able to capture anomalies beyond the seasonal cycle,\nthereby paving the way for predicting vegetation health and\nbehaviour in response to climate variability and extremes.\nWe provide open source code and pre-trained weights to re-\nproduce our experimental results under https://gith\nub.com/vitusbenson/greenearthnet [10].\nPast Satellite Data\nMeteorology\nElevation\nFuture Vegetation State\n■ Training       ■ OOD-t Test       ■ OOD-st Test\nMulti-modal deep learning:\nglobal pyramid vision transformer encoder\n+ patchwise meteo-guided temporal transformer\n+ delta prediction scheme\nModel\nInputs\nContextformer\nOutputs\nFigure 1. GreenEarthNet approach (map shows sample locations).\n1. Introduction\nOptical satellite images have been proven useful for mon-\nitoring vegetation status. This is essential for a variety of\napplications in agricultural planning, forestry advisory, hu-\nmanitarian assistance or carbon monitoring.\nIn all these\ncases, prognostic information is relevant: Farmers want to\nknow how their farmland may react to a given weather sce-\nnario [83]. Humanitarian organisations need to understand\nthe localized impact of droughts on pastoral communities\nfor mitigation of famine with anticipatory action [49]. Af-\nforestation efforts need to consider how their forests react\nto future climate [71]. However, providing such prognostic\ninformation through fine resolution vegetation forecasts is\nchallenging due to ecological memory effects [35], spatial\ninteractions and the influence of weather variations. Deep\nneural networks can model relationships in space, time or\nacross modalities. Hence, their application to vegetation\nforecasting given a sufficiently large dataset seems natural.\n1\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n27788",
        "chosen": "GreenEarthNet is the first dataset specifically designed for high-resolution vegetation forecasting, paired with Contextformer, a novel deep learning approach for predicting vegetation greenness from Sentinel 2 satellite images. This multi-modal transformer model leverages spatial context through a vision backbone and predicts temporal dynamics on local context patches incorporating meteorological time series efficiently. GreenEarthNet includes a learned cloud mask and an appropriate evaluation scheme, maintaining compatibility with EarthNet2021. Extensive analyses show Contextformer outperforms various baselines, including previous SOTA models on EarthNet2021 and adapted models from time series forecasting and video prediction.",
        "rejected": "GreenEarthNet introduces Contextformer, a novel deep learning model for predicting vegetation greenness from Sentinel 2 satellite images across Europe. This study presents the first continental-scale vegetation modeling at fine resolution, capable of capturing anomalies beyond seasonal cycles."
    },
    {
        "prompt": "summary: Astronaut photography, spanning six decades of human\nspaceflight, presents a unique Earth observations dataset\nwith immense value for both scientific research and disaster\nresponse. Despite their significance, accurately localizing\nthe geographical extent of these images, which is crucial\nfor effective utilization, poses substantial challenges. Cur-\nrent, manual localization efforts are time-consuming, mo-\ntivating the need for automated solutions. We propose a\nnovel approach – leveraging image retrieval – to address\nthis challenge efficiently. We introduce innovative training\ntechniques which contribute to the development of a high-\nperformance model, EarthLoc. We develop six evaluation\ndatasets and perform a comprehensive benchmark compar-\ning EarthLoc to existing methods, showcasing its superior\nefficiency and accuracy.\nOur approach marks a signifi-\ncant advancement in automating the localization of astro-\nnaut photography, which will help bridge a critical gap in\nEarth observations data. Code and datasets are available\nat https://github.com/gmberton/EarthLoc.\n1. Introduction\nAstronaut photography of Earth is a unique remote sensing\ndataset that spans 60 years of human spaceflight, offering a\nrare perspective on our planet to the public and valuable\ndata to Earth and atmospheric science researchers.\nThis\ndataset contains over 4.5 million images and is growing by\nthe tens of thousands per month, as astronauts are contin-\nually tasked with taking new photographs that enable sci-\nentific research as well as assist in natural disaster response\nefforts in the wake of events like floods and wildfires. To ef-\nfectively use these images, the geographical area depicted in\nthem needs to be identified. Unfortunately, this task - Astro-\nnaut Photography Localization (APL) - is very challenging.\nFor each photo, only a coarse estimate of location is known,\ngiven by the point on Earth directly under the International\nSpace Station (ISS) at the time the photo is taken. This\npoint – called the nadir – can be easily computed using the\nimage’s timestamp and the ISS’s orbit path. However, two\nFigure 1. Overview of the astronaut photography localization\ntask. Astronauts take hundreds of photos a day from the Interna-\ntional Space Station (ISS) cupola (top-left) with hand-held cam-\neras. For each image (example bottom right), the geographic lo-\ncation depicted is not known, and needs to be searched for across\na huge area centered at the ISS’s (known) nadir point at the mo-\nment that the photo is taken. A simulated view of the astronaut’s\nperspective when the ISS is above Europe is shown. The goal of\nour paper is to automate the task of localizing these images, which\ncould be anywhere within the view. In the figure’s example, the\nphoto the astronaut took is indicated by the green line and shown\nin inset – other possible photo extents are in red, illustrating the\nwide array of potential locations to search.\nimages taken with the same nadir can be thousands of kilo-\nmeters apart, as even a slight inclination of the astronaut’s\nhand-held camera can move the image’s location hundreds\nof kilometers in any direction, as depicted in Fig. 1. Lo-\ncalization must thus be performed over a wide area, and is\nadditionally complicated by (i) astronauts using hand-held\ncameras and a variety of zoom lenses, (ii) the large, 2500\nkilometer (km) visibility range in all directions, (iii) most\nphotographs being oblique, and (iv) the Earth’s appearance\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n12754",
        "chosen": "EarthLoc is a novel approach leveraging image retrieval to automate the localization of astronaut photography. This method efficiently addresses the challenge of identifying geographical areas in astronaut photographs, which is crucial for effective utilization. EarthLoc employs innovative training techniques to develop a high-performance model, supported by six evaluation datasets and comprehensive benchmark comparisons. The approach significantly advances automated localization of astronaut photography, bridging a critical gap in Earth observations data.",
        "rejected": "EarthLoc proposes an innovative approach for automating the localization of astronaut photography using image retrieval techniques. The method significantly advances the efficiency and accuracy in localizing these unique Earth observation datasets."
    },
    {
        "prompt": "summary: We describe a novel method, StyLitGAN, for relighting\nand resurfacing images in the absence of labeled data. StyL-\nitGAN generates images with realistic lighting effects, includ-\ning cast shadows, soft shadows, inter-reflections, and glossy\neffects, without the need for paired or CGI data. StyLit-\nGAN uses an intrinsic image method to decompose an image,\nfollowed by a search of the latent space of a pretrained Style-\nGAN to identify a set of directions. By prompting the model\nto fix one component (e.g., albedo) and vary another (e.g.,\nshading), we generate relighted images by adding the identi-\nfied directions to the latent style codes. Quantitative metrics\nof change in albedo and lighting diversity allow us to choose\neffective directions using a forward selection process. Qual-\nitative evaluation confirms the effectiveness of our method.\n1. Introduction\nScene appearance shifts dramatically with varying lighting\nconditions - a sunlit room takes on a different character as\ndaylight fades, and interior spaces transform with the flick of\na switch. Similarly, surface changes, like a wall’s paint color,\nchange not only the wall’s appearance but also the overall\nimage due to light reflection. Despite the impressive realism\nachieved by current generative models like StyleGAN [22–\n24], they fall short in dynamically controlling scene lighting,\na key aspect of realistic image generation.\nIn this work, we present StyLitGAN, a novel approach\nthat extends the editing capabilities of StyleGAN [38, 45,\n46, 53]. StyLitGAN uniquely manipulates style codes to\nselectively change lighting while preserving other image\nattributes like albedo and geometry. This selective editing\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n4231",
        "chosen": "StyLitGAN is a novel method for relighting and resurfacing images without labeled data. It generates images with realistic lighting effects, including cast shadows, soft shadows, inter-reflections, and glossy effects. StyLitGAN uses an intrinsic image method to decompose an image, followed by a search of the latent space of a pretrained StyleGAN to identify effective directions for relighting. By fixing one component (e.g., albedo) and varying another (e.g., shading), it generates relighted images. Quantitative metrics and qualitative evaluation confirm the method's effectiveness.",
        "rejected": "StyLitGAN generates relighted images without paired or CGI data, utilizing intrinsic image methods and StyleGAN's latent space to manipulate lighting while preserving albedo and geometry, achieving realistic lighting effects."
    },
    {
        "prompt": "summary: Object detection in radar imagery with neural networks\nshows great potential for improving autonomous driving.\nHowever, obtaining annotated datasets from real radar im-\nages, crucial for training these networks, is challenging,\nespecially in scenarios with long-range detection and ad-\nverse weather and lighting conditions where radar perfor-\nmance excels. To address this challenge, we present Rad-\nSimReal, an innovative physical radar simulation capable\nof generating synthetic radar images with accompanying\nannotations for various radar types and environmental con-\nditions, all without the need for real data collection. Re-\nmarkably, our findings demonstrate that training object de-\ntection models on RadSimReal data and subsequently eval-\nuating them on real-world data produce performance lev-\nels comparable to models trained and tested on real data\nfrom the same dataset, and even achieves better perfor-\nmance when testing across different real datasets.\nRad-\nSimReal offers advantages over other physical radar sim-\nulations that it does not necessitate knowledge of the radar\ndesign details, which are often not disclosed by radar sup-\npliers, and has faster run-time. This innovative tool has the\npotential to advance the development of computer vision al-\ngorithms for radar-based autonomous driving applications.\nOur GitHub: https://yuvalhg.github.io/RadSimReal.\n1. Introduction\nAutomotive radar plays an important role in autonomous\ndriving systems, offering long-range object detection ca-\npabilities and robustness against challenging weather and\nlighting conditions. The radar emits radio frequency (RF)\nsignals and, through the processing of reflected echoes from\nthe surrounding environment, creates a radar reflection in-\ntensity image [6]. The image contains reflection intensities\n*Both authors contributed equally to this work.\nBoth authors are with General Motors, Yuval Haitman is also with the\nSchool of Electrical and Computer Engineering in Ben Gurion University\nof the Negev.\n-30\n0  \n30 \n90 \nAzimuth [deg]\n10\n15\n20\n25\n30\n35\nRange [m]\n-53\n-37\n-24\n-12\n0  \n12 \n24 \n37 \n53 \n90 \nAzimuth [deg]\n10\n15\n20\n25\n30\n35\nRange [m]\nImage \nSimulation\nRadar Image \nSimulation\nRadar Image \nRadDet\nImage \nRadDet\n-30\n0  \n30 \n90 \nAzimuth [deg]\n5\n10\n15\n20\nRange [m]\n-53\n-37\n-24\n-12\n0  \n12 \n24 \n37 \n53 \n90 \nAzimuth [deg]\n4\n6\n8\n10\n12\n14\n16\n18\n20\nRange [m]\n(c)\n(d)\n(a)\n(b)\nFigure 1. Comparison between synthetic and real radar images\nfrom four different scenarios. Each scenario shows the camera\nimage and the corresponding radar image. (a) and (b) simulation\nscenarios. (c) and (d) real scenarios.\ncorresponding to range and angle coordinates, providing a\nvisual representation of the scene. Afterwards, computer vi-\nsion algorithms are employed to identify objects within this\nvisual image.\nNumerous Deep Neural Network (DNN) methods have\nemerged for detecting objects in radar images [11, 22, 25,\n30, 44, 45]. These techniques involve training the DNN\nusing annotated real data. Several datasets containing real\nannotated radar images have been introduced [27–29, 36,\n40, 44]. These datasets vary in terms of the radar type and\nenvironmental conditions. However, the primary challenge\nwith object detection DNNs trained on real data lies in the\nconsiderable effort required to collect and annotate the data.\nThis challenge is particularly hard in the case of radar since\nit is used to detect objects at long range, adverse weather\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n15407",
        "chosen": "RadSimReal is an innovative physical radar simulation capable of generating synthetic radar images with annotations for various radar types and environmental conditions without real data collection. This simulation addresses the challenge of obtaining annotated radar datasets, crucial for training neural networks in object detection. RadSimReal demonstrates comparable performance to real-data-trained models when evaluated on real-world data and even outperforms them across different real datasets. The simulation offers advantages over other physical radar simulations, including faster run-time and no need for radar design details.",
        "rejected": "RadSimReal simulates synthetic radar images with annotations, offering an effective solution for training object detection models in radar imagery. It demonstrates comparable performance to real-world data-trained models, even surpassing them in cross-dataset evaluations."
    },
    {
        "prompt": "summary: Recent advancements in large vision-language models\nenabled visual object detection in open-vocabulary scenar-\nios, where object classes are defined in free-text formats\nduring inference. In this paper, we aim to probe the state-\nof-the-art methods for open-vocabulary object detection to\ndetermine to what extent they understand fine-grained prop-\nerties of objects and their parts.\nTo this end, we intro-\nduce an evaluation protocol based on dynamic vocabulary\ngeneration to test whether models detect, discern, and as-\nsign the correct fine-grained description to objects in the\npresence of hard-negative classes.\nWe contribute with a\nbenchmark suite of increasing difficulty and probing dif-\nferent properties like color, pattern, and material. We fur-\nther enhance our investigation by evaluating several state-\nof-the-art open-vocabulary object detectors using the pro-\nposed protocol and find that most existing solutions, which\nshine in standard open-vocabulary benchmarks, struggle to\naccurately capture and distinguish finer object details. We\nconclude the paper by highlighting the limitations of current\nmethodologies and exploring promising research directions\nto overcome the discovered drawbacks. Data and code are\navailable at https://lorebianchi98.github.io/FG-OVD/ .\n1. Introduction\nOpen-vocabulary object detection (OVD) consists of recog-\nnizing objects not present at training time, therefore solving\nthe limitations imposed by traditional detectors that could\nonly recognize a fixed pool of object classes. In the last\nyears, open-vocabulary detectors have captured large re-\nsearch attention thanks to their wide flexibility in many\npotential downstream applications like autonomous driv-\ning [20], extended reality [19], and robotics [9, 10, 16].\nThe core idea behind open-vocabulary detectors is to es-\ntablish a semantic connection between object regions and\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n22520",
        "chosen": "This study evaluates state-of-the-art open-vocabulary object detection methods using a dynamic vocabulary generation protocol to test their ability to detect, discern, and assign fine-grained descriptions to objects. The benchmark suite increases in difficulty and probes properties like color, pattern, and material. Results show that most existing solutions struggle to accurately capture and distinguish finer object details, highlighting limitations and suggesting research directions for improvement.",
        "rejected": "This paper evaluates open-vocabulary object detection methods' ability to understand fine-grained properties of objects. Results show limitations in accurately distinguishing finer details, highlighting challenges and suggesting future research directions."
    },
    {
        "prompt": "summary: Learning compatible representations enables the inter-\nchangeable use of semantic features as models are updated\nover time. This is particularly relevant in search and re-\ntrieval systems where it is crucial to avoid reprocessing of\nthe gallery images with the updated model. While recent\nresearch has shown promising empirical evidence, there\nis still a lack of comprehensive theoretical understanding\nabout learning compatible representations. In this paper, we\ndemonstrate that the stationary representations learned by\nthe d-Simplex fixed classifier optimally approximate compat-\nibility representation according to the two inequality con-\nstraints of its formal definition. This not only establishes a\nsolid foundation for future works in this line of research but\nalso presents implications that can be exploited in practical\nlearning scenarios. An exemplary application is the now-\nstandard practice of downloading and fine-tuning new pre-\ntrained models. Specifically, we show the strengths and criti-\ncal issues of stationary representations in the case in which\na model undergoing sequential fine-tuning is asynchronously\nreplaced by downloading a better-performing model pre-\ntrained elsewhere. Such a representation enables seamless\ndelivery of retrieval service (i.e., no reprocessing of gallery\nimages) and offers improved performance without opera-\ntional disruptions during model replacement. Code available\nat: https://github.com/miccunifi/iamcl2r.\n1. Introduction\nBy learning powerful internal feature representations from\ndata, Deep Neural Networks (DNNs) [1–4] have made\ntremendous progress in some of the most challenging\nsearch tasks such as face recognition [5–9], person re-\nidentification [10–12], image retrieval [13–15] and this sig-\nnificance also extends to a variety of other data modalities\n[16, 17]. Although all of the works mentioned above have\nfocused on learning feature representations from static and,\n𝜙𝑇\n \ntask 2\ntask 𝑇\ntask 1\n𝜙2\n \n𝜙1\n \nStationary \nRepresentation\n(⟹ Compatible)\n𝜙 \nFigure 1. Improved Asynchronous Model Compatible Lifelong\nLearning Representation (IAM-CL2R pronounced “I am clear”). In\nthe process of lifelong learning, a model is sequentially fine-tuned\nand asynchronously replaced with improved third-party models\nthat are pre-trained externally. Stationary representations ensure\nseamless retrieval services and better performance, without the\nneed to reprocess gallery images.\nmore recently, dynamic datasets [18–21], the now-standard\npractice is downloading and fine-tuning representations from\nmodels pre-trained elsewhere [22, 23]. These “third-party”\npre-trained models often incorporate new data, utilize alter-\nnative architectures, adopt different loss functions or more\nin general provide novel methodologies. Whether applied\nindividually or combined, these advancements aim to en-\ncapsulate the field’s rapid progress within a single unified\nmodel [24]. This greatly facilitates the exploitation of in-\nternally learned semantic representations, particularly as\nmodels, datasets, and computational infrastructure continue\nto expand in size, complexity, and cost [25, 26].\nThe challenge of fully exploiting such standard practice\nin retrieval/search systems has to deal with the underlying\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n28793",
        "chosen": "IAM-CL2R demonstrates that stationary representations learned by the d-Simplex fixed classifier optimally approximate compatible representations according to theoretical constraints. This establishes a foundation for learning compatible representations and has practical implications for retrieval systems. Stationary representations enable seamless retrieval services during model replacement without reprocessing gallery images, crucial for systems requiring minimal operational disruptions.",
        "rejected": "The paper explores learning compatible representations for search and retrieval systems, demonstrating that stationary representations optimally approximate compatibility, enabling seamless service delivery during model updates."
    },
    {
        "prompt": "summary: Significant progress has been made in scene text detec-\ntion models since the rise of deep learning, but scene text\nlayout analysis, which aims to group detected text instances\nas paragraphs, has not kept pace. Previous works either\ntreated text detection and grouping using separate mod-\nels, or train a model from scratch while using a unified\none. All of them have not yet made full use of the already\nwell-trained text detectors and easily obtainable detection\ndatasets. In this paper, we present Text Grouping Adapter\n(TGA), a module that can enable the utilization of various\npre-trained text detectors to learn layout analysis, allowing\nus to adopt a well-trained text detector right off the shelf\nor just fine-tune it efficiently. Designed to be compatible\nwith various text detector architectures, TGA takes detected\ntext regions and image features as universal inputs to as-\nsemble text instance features. To capture broader contex-\ntual information for layout analysis, we propose to predict\ntext group masks from text instance features by one-to-many\nassignment. Our comprehensive experiments demonstrate\nthat, even with frozen pre-trained models, incorporating our\nTGA into various pre-trained text detectors and text spotters\ncan achieve superior layout analysis performance, simul-\ntaneously inheriting generalized text detection ability from\npre-training. In the case of full parameter fine-tuning, we\ncan further improve layout analysis performance.\n1. Introduction\nWith the rise of deep learning, text detection [17, 18, 22,\n44], text recognition [3, 8, 16, 34], and end-to-end text spot-\nting [19, 20, 38, 39] models have greatly improved the accu-\nracy and efficiency of identifying text instances like words\n*Work done during the internship at Microsoft Research Asia.\n‡Corresponding authors.\n(a)\nBackbone\nLayout\nBranch\n🔥\nDetection\nBranch\ntext\nregions\naffinity \nmatrix\nlearnable query\n(b)\nImage\ntext \nregions\naffinity\nmatrix\n🔥\nText Grouping\nAdapter\ngroup \nmasks\nBackbone\nDetection\nBranch\nText Detection\nlayout\nLayout Analysis\nimage\nimage\nlayout\nText Detector\nFigure 1. Top: Visualization of the scene text detection and layout\nanalysis tasks. The mask with the same color denotes detected as a\ngroup. Bottom: Comparison between (a) the previous work Uni-\nfied Detector [23] and (b) proposed TGA. TGA also provides the\nflexibility of freezing or fine-tuning the pre-trained text detector.\nor text lines. To fully understand the text semantics in vari-\nous applications [1, 2, 30, 41, 45], it is essential to determine\nhow to organize these text instances into coherent semantic\nentities, e.g., determining which detected words constitute a\nline and which detected lines form a paragraph. This prob-\nlem, as visualized in the top of Figure 1, named scene text\nlayout analysis [23], has not been advanced at the same pace\nas other scene text understanding tasks.\nPrevious layout analysis works [13, 29, 46] adopt sep-\narate semantic segmentation models to localize different\nhigh-level entities in scanned and digital document images,\nwhich only focus on limited scenarios instead of general-\nscope natural scenes and ignore low-level text instances like\nwords and text lines. Recent design on a Unified Detec-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n28150",
        "chosen": "Text Grouping Adapter (TGA) is a module enabling the use of pre-trained text detectors for layout analysis. Designed to be compatible with various text detector architectures, TGA takes detected text regions and image features as inputs to assemble text instance features. It predicts text group masks by one-to-many assignment to capture broader contextual information. Experiments show TGA enhances layout analysis performance even with frozen pre-trained models, inheriting generalized text detection ability and improving performance further with full parameter fine-tuning.",
        "rejected": "TGA enables pre-trained text detectors to learn layout analysis, improving scene text grouping without retraining. It uses detected text regions and image features to predict text group masks, enhancing layout analysis performance."
    },
    {
        "prompt": "summary: The increasing use of transformer-based large lan-\nguage models brings forward the challenge of processing\nlong sequences.\nIn document visual question answering\n(DocVQA), leading methods focus on the single-page set-\nting, while documents can span hundreds of pages.\nWe\npresent GRAM, a method that seamlessly extends pre-\ntrained single-page models to the multi-page setting, with-\nout requiring computationally-heavy pretraining.\nTo do\nso, we leverage a single-page encoder for local page-level\nunderstanding, and enhance it with document-level desig-\nnated layers and learnable tokens, facilitating the flow of\ninformation across pages for global reasoning. To enforce\nour model to utilize the newly introduced document tokens,\nwe propose a tailored bias adaptation method.\nFor ad-\nditional computational savings during decoding, we intro-\nduce an optional compression stage using our compression-\ntransformer(C-Former ),reducing the encoded sequence\nlength, thereby allowing a tradeoff between quality and\nlatency.\nExtensive experiments showcase GRAM’s state-\nof-the-art performance on the benchmarks for multi-page\nDocVQA, demonstrating the effectiveness of our approach.\n1. Introduction\nDocument understanding, particularly in the context of\nDocVQA, has gained substantial research interest [5, 6,\n16, 25, 36, 37] and offers a wide array of practical appli-\ncations, focusing on data extraction and analysis of sin-\ngle page documents. However, Multi-Page DocVQA (MP-\nDocVQA) poses a more realistic challenge, considering that\nthe majority of documents, including contracts, manuals\n*Work conducted during an internship at Amazon.\n†Corresponding author: alongolt@amazon.com\n‡Corresponding author: litmanr@amazon.com\n+\nHow many \ndiagrams\nare there?\nMulti Page Encoder \nGlobal-Local Encoder Block\nPage Sub-Layer\nDoc Sub-Layer \nPage Sub-Layer\nDoc Sub-Layer \nC-Former\nPage \nAttention\nDoc \nAttention\nCompression Transformer\nDecoder\nFigure 1. An Overview of GRAM. We suggest an interleaved en-\ncoder architecture combining page- with document-attention lay-\ners, allowing information to propagate between different pages.\nAn optional compression transformer (C-former) is introduced to\nallow a trade-off between quality and latency.\nand scientific papers, often extend well beyond a single\npage. Despite the practical relevance of MPDocVQA, it\nhas received limited attention, primarily due to the absence\nof suitable datasets. Two recently introduced datasets, MP-\nDocVQA [33] and DUDE [18], have opened up new av-\nenues for MP-DocVQA research.\nRecent DocVQA approaches rely on transformers [35],\nat the heart of their architecture. While transformers are a\npowerful tool, they face challenges when dealing with long\ninput sequences [4, 7, 10–12, 27, 38]. This difficulty stems\nfrom the self-attention mechanism, which scales quadrati-\ncally in terms of computation and memory, with respect to\nthe input sequence length. Addressing this limitation has\nattracted significant research efforts, primarily in the field\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n15598",
        "chosen": "GRAM extends pretrained single-page models to multi-page DocVQA without heavy pretraining. It uses a single-page encoder for local understanding and enhances it with document-level layers and learnable tokens for global reasoning. A compression stage using C-Former reduces sequence length, allowing a quality-latency tradeoff. GRAM achieves state-of-the-art performance on multi-page DocVQA benchmarks.",
        "rejected": "GRAM extends single-page document visual question answering (DocVQA) models to multi-page settings, facilitating global reasoning through document-level designated layers and learnable tokens. An optional compression stage allows quality-latency trade-offs."
    },
    {
        "prompt": "summary: Knowledge of lane topology is a core problem in au-\ntonomous driving. Aerial imagery can provide high res-\nolution, quickly updatable lane source data but detecting\nlanes from such data has so far been an expensive man-\nual process or, where automated solutions exist, undriv-\nable and requiring of downstream processing.\nWe pro-\npose a method for large-scale lane topology extraction from\naerial imagery while ensuring that the resulting lanes are\nrealistic and drivable by introducing a novel B´ezier Graph\nshared parameterisation of B´ezier curves. We develop a\ntransformer-based model to predict these B´ezier Graphs\nfrom input aerial images, demonstrating competitive results\non the UrbanLaneGraph dataset. We demonstrate that our\nmethod generates realistic lane graphs which require both\nminimal input, and minimal downstream processing. We\nmake our code publicly available at https://github.\ncom/driskai/BGFormer\n1. Introduction\nAutonomous Vehicles (AVs) require knowledge of their sur-\nroundings to operate. So far, all systems not reliant on a\nsafety driver have required pre-built High Deﬁnition maps\n(HD maps). HD maps can deliver a strong prior about the\nroad and lane topology, e.g. so the AV can navigate amid\nocclusions. But creation of HD maps is time-consuming\nand expensive, typically requiring a ﬂeet of road vehicles\nequipped with LiDAR and cameras followed by extensive\nmanual curation and human annotation [18].\nOne of the core components of an HD map is precise\nlane geometry and topology data. Previous works have at-\nFor the purpose of open access, the author has applied a Creative Com-\nmons Attribution (CC BY) license to any Author Accepted Manuscript\nversion arising. This work was funded by the Centre for Connected and\nAutonomous Vehicles, delivered by Innovate UK, part of UK Research\nand Innovation.\nFigure 1. Our trained model detecting a lane B´ezier Graph on an\nintersection. B´ezier endpoints P 0,3 shown in cyan, control points\nP 1,2 in white, resulting curves in red.\ntempted to automate the generation of this data from on-\nboard sensors [4, 5, 10] or - recently - from aerial imagery\n[3, 9, 30, 31]. Generating lane-level data from aerial im-\nagery is of particular interest since this can be efﬁciently\nobtained at large scale using Unmanned Aerial Vehicles\n(UAVs) or satellites. In this way, AVs could be equipped\nwith regularly updated HD maps that cover a large area.\nHowever, these approaches are not without their chal-\nlenges. Lane geometry and topology is typically predicted\nin the form of a lane graph: a graph in which nodes repre-\nsent a discrete sampling of lane centre lines, with edges rep-\nresenting connectivity. Existing methods typically predict\nthe position of each node in 2D space, but this frequently\nresults in noisy, non-physical lane centre lines requiring ex-\ntensive downstream processing and human oversight.\nWe improve on these methods by introducing a shared\nparameterisation of cubic B´ezier curves in a graph struc-\nture that we refer to as a B´ezier Graph.\nBy associating\nB´ezier direction vectors with nodes, we enforce a strong\nprior that lane direction should be continuous at boundaries.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n15365",
        "chosen": "This method extracts large-scale lane topology from aerial imagery using a novel Bézier Graph parameterization of cubic Bézier curves. A transformer-based model predicts these graphs, generating realistic, drivable lane graphs with minimal input and downstream processing. The approach improves on existing methods by ensuring lane direction continuity at boundaries, demonstrated on the UrbanLaneGraph dataset.",
        "rejected": "This paper presents BGFormer, a transformer-based model for predicting Bezier Graphs from aerial images, generating drivable lane graphs with minimal downstream processing, ideal for HD map creation in autonomous driving."
    },
    {
        "prompt": "summary: Image keypoint descriptions that are discriminative and\nmatchable over large changes in viewpoint are vital for 3D\nreconstruction. However, descriptions output by learned\ndescriptors are typically not robust to camera rotation.\nWhile they can be made more robust by, e.g., data aug-\nmentation, this degrades performance on upright images.\nAnother approach is test-time augmentation, which incurs\na significant increase in runtime. Instead, we learn a lin-\near transform in description space that encodes rotations\nof the input image. We call this linear transform a steerer\nsince it allows us to transform the descriptions as if the im-\nage was rotated. From representation theory, we know all\npossible steerers for the rotation group. Steerers can be\noptimized (A) given a fixed descriptor, (B) jointly with a de-\nscriptor or (C) we can optimize a descriptor given a fixed\nsteerer. We perform experiments in these three settings and\nobtain state-of-the-art results on the rotation invariant im-\nage matching benchmarks AIMS and Roto-360. We publish\ncode and model weights at this https url.\n1. Introduction\nDiscriminative local descriptions are vital for multiple 3D\nvision tasks, and learned descriptors have recently been\nshown to outperform traditional handcrafted local fea-\ntures [17, 19, 23, 43].\nOne major weakness of learned\ndescriptors compared to handcrafted features such as\nSIFT [35] is the relative lack of robustness to non-upright\nimages [55]. While images taken from ground level can\nsometimes be made upright by aligning with gravity as the\ncanonical orientation, this is not always possible. For exam-\nple, descriptors robust to rotation are vital in space applica-\ntions [49], as well as medical applications [42], where no\nsuch canonical orientation exists. Even when a canonical\norientation exists, it may be difficult or impossible to esti-\nmate. Rotation invariant matching is thus a key challenge.\nThe most straightforward manner to get rotation invari-\nant matching is to train or design a descriptor to be rotation\ninvariant [17, 35]. However, this sacrifices distinctiveness\nin matching images with small relative rotations [41]. An\nalternative approach is to train a rotation-sensitive descrip-\ntor and perform test-time-augmentation, selecting the pair\nthat produces the most matches. The obvious downside of\nTTA is computational cost. For example, testing all 45◦ro-\ntations requires running the model eight times.\nIn this paper, we present an approach that maintains dis-\ntinctiveness for small rotations and allows for rotation in-\nvariant matching when we have images with large rota-\ntions. We do this while adding only negligible additional\nruntime, running the descriptor only a single time.\nThe\nmain idea is to learn a linear transform in description space\nthat corresponds to a rotation of the input image; see Fig-\nure 2. We call this linear transform a steerer as it allows\nus to modify keypoint descriptions as if they were describ-\ning rotated images—we can steer the descriptions without\nhaving to rerun the descriptor network. We show empiri-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n4885",
        "chosen": "Steerers are learned linear transforms in description space that encode image rotations for keypoint descriptors. This framework allows rotation-equivariant matching without sacrificing distinctiveness for small rotations or incurring significant runtime costs. Experiments show state-of-the-art results on rotation-invariant benchmarks, with code and model weights published.",
        "rejected": "The study focuses on creating discriminative image keypoint descriptors robust over large viewpoint changes for 3D reconstruction, addressing the lack of robustness in current learned descriptors."
    },
    {
        "prompt": "summary: Neural character models can now reconstruct detailed\ngeometry and texture from video, but they lack explicit shad-\nows and shading, leading to artifacts when generating novel\nviews and poses or during relighting. It is particularly dif-\nficult to include shadows as they are a global effect and the\nrequired casting of secondary rays is costly. We propose\na new shadow model using a Gaussian density proxy that\nreplaces sampling with a simple analytic formula. It sup-\nports dynamic motion and is tailored for shadow computa-\ntion, thereby avoiding the affine projection approximation\nand sorting required by the closely related Gaussian splat-\nting. Combined with a deferred neural rendering model, our\nGaussian shadows enable Lambertian shading and shadow\ncasting with minimal overhead. We demonstrate improved\nreconstructions, with better separation of albedo, shading,\nand shadows in challenging outdoor scenes with direct sun\nlight and hard shadows. Our method is able to optimize\nthe light direction without any input from the user. As a\nresult, novel poses have fewer shadow artifacts, and re-\nlighting in novel scenes is more realistic compared to the\nstate-of-the-art methods, providing new ways to pose neu-\nral characters in novel environments, increasing their ap-\nplicability. Code available at: https://github.com/\nLuisBolanos17/GaussianShadowCasting\n1. Introduction\nIt is now possible to reconstruct animatable 3D neural\navatars from video but methods do not account for accurate\nlighting and shadows. They have to rely on recordings that\nhave soft uniform lighting, which precludes recording out-\ndoors in direct sun light and on film sets with spotlights, and\nmost are unable to relight characters in novel environments,\nlimiting their applicability in content creation.\nThe most recent body models [17,20,28,36,37,40] which\nare based on neural radiance fields (NeRFs) [26], approxi-\nmate the light transport by casting primary rays between the\ncamera and the scene, sampling the underlying neural net-\nwork dozens of times along each ray to obtain the density\nand color. As they do not include an illumination model, the\ncolor that the NeRF learns includes lighting, shadow, and\nview-dependent effects. Learning a body model in a chal-\nlenging scene with a strong directional light source, such as\nthe sun, leads to the neural field overfitting to the observed\nshadows. It does not generalize to novel poses, as the cast\nshadows are global effects where movement of a joint could\naffect the appearance of other distant areas of the body. Fig-\nure 1 shows such setting. This is in contrast to local shad-\ning effects such as wrinkles in clothing which current body\nmodels can successfully reconstruct.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n20997",
        "chosen": "Gaussian Shadow Casting for Neural Characters presents a new shadow model using a Gaussian density proxy for efficient shadow computation in neural character models. This method improves reconstructions by better separating albedo, shading, and shadows, enabling realistic relighting and novel pose generation without user input.",
        "rejected": "The paper introduces Gaussian shadows, a new model for accurate lighting and shadow computation in neural avatars reconstructed from video. This method supports dynamic motion and integrates with deferred neural rendering to improve shading and shadow casting. It enables realistic relighting in novel scenes without user input on light direction."
    },
    {
        "prompt": "summary: In the context of autonomous navigation of terrestrial robots,\nthe creation of realistic models for agent dynamics and sens-\ning is a widespread habit in the robotics literature and in\ncommercial applications, where they are used for model\nbased control and/or for localization and mapping. The\nmore recent Embodied AI literature, on the other hand, fo-\ncuses on modular or end-to-end agents trained in simulators\nlike Habitat or AI-Thor, where the emphasis is put on photo-\nrealistic rendering and scene diversity, but high-fidelity robot\nmotion is assigned a less privileged role. The resulting\nsim2real gap significantly impacts transfer of the trained\nmodels to real robotic platforms. In this work we explore\nend-to-end training of agents in simulation in settings which\nminimize the sim2real gap both, in sensing and in actuation.\nOur agent directly predicts (discretized) velocity commands,\nwhich are maintained through closed-loop control in the real\nrobot. The behavior of the real robot (including the under-\nlying low-level controller) is identified and simulated in a\nmodified Habitat simulator. Noise models for odometry and\nlocalization further contribute in lowering the sim2real gap.\nWe evaluate on real navigation scenarios, explore different\nlocalization and point goal calculation methods and report\nsignificant gains in performance and robustness compared\nto prior work.\n1. Introduction\nPoint goal navigation of terrestrial robots in indoor buildings\nhas traditionally been addressed in the robotics community\nwith mapping and planning [6, 30, 50], which led to solu-\ntions capable of operating on robots in real environments.\nThe field of computer vision and embodied AI has addressed\nthis problem through large-scale machine learning in sim-\nulated 3D environments from reward with RL [24, 34] or\nwith imitation learning [14]. Learning from large-scale data\nallows the agent to pick up more complex regularities, to\nprocess more subtle cues and therefore (in principle) to be\nmore robust, to exploit data regularities to infer hidden and\noccluded information, and generally to learn more powerful\nTraining/sim (ours)\nTest/real\nxt\n1\n2\n1\n2 xt+1\nTraining/sim (classical)\nxt\n1\n2 xt+1\nPolicy: \n∆π\nOnboard ctrl\n∆φ\n333ms/3 Hz (Unknown)\nPolicy: \n∆π\nDyn. model\n∆φ\n333ms/3 Hz 33ms/30 Hz\nPolicy: \n∆π\nconstant\n333ms/3 Hz velocities\nPiecewise\nFigure 1. Efficient navigation with policies end-to-end trained in\n3D photorealistic simulators requires closing the sim2real gap in\nsensing and actuation. Efficiency demands that the robot continues\nto move during decision taking (as opposed to stopping for each\nsensing operation), and this requires a realistic motion model in\nsimulation allowing the agent to internally anticipate its future state.\nThis requirement is exacerbated by the delay between sensing\n①and actuation ②caused by the computational complexity of\nhigh-capacity deep networks (visual encoders, policy). To model\nrealistic motion while training in simulation, we create a 2nd order\ndynamical model running with higher frequency, which models\nthe robot and its low-level closed-loop controller. We identify the\nmodel from real data and add it to the Habitat [44] Simulator.\ndecision rules. In this context, the specific task of point goal\nnavigation (navigation to coordinates) is now sometimes\nconsidered “solved” in the literature [38], incorrectly, as we\nargue. While the machine learning and computer vision\ncommunity turns its attention towards the exciting goal of\nintegrating language models into navigation [15, 23], we\nthink that further improvements are required to make trained\nagents perform reliably in real environments with sufficient\nspeed.\nExperiments and evaluations of trained models in real\nenvironments and the impact of the sim2real gap do exist in\nthe Embodied AI literature [11, 20, 43], but they are rare and\nwere performed in restricted environments. Known models\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n17837",
        "chosen": "This work explores end-to-end training of agents in simulation to minimize the sim2real gap for autonomous robot navigation. The agent predicts velocity commands and is trained with a modified Habitat simulator modeling real robot behavior and noise. Evaluations show significant performance and robustness gains in real navigation scenarios.",
        "rejected": "This work addresses minimizing the sim2real gap for terrestrial robots by end-to-end training in simulations that closely mimic real-world sensing and actuation dynamics. By using a modified Habitat simulator with noise models for odometry and localization, it achieves significant improvements in navigation performance and robustness compared to prior methods."
    },
    {
        "prompt": "summary: Local\nInterpretable\nModel-agnostic\nExplanations\n(LIME) - a widely used post-ad-hoc model agnostic ex-\nplainable AI (XAI) technique. It works by training a simple\ntransparent (surrogate) model using random samples\ndrawn around the neighborhood of the instance (image)\nto be explained (IE). Explanations are then extracted for\na black-box model and a given IE, using the surrogate\nmodel.\nHowever, the explanations of LIME suffer from\ninconsistency across different runs for the same model and\nthe same IE. We identify two main types of inconsistencies:\nvariance in the sign and importance ranks of the segments\n(superpixels). These factors hinder LIME from obtaining\nconsistent explanations. We analyze these inconsistencies\nand propose a new method, Stabilized LIME for Consistent\nExplanations (SLICE). The proposed method handles the\nstabilization problem in two aspects: using a novel feature\nselection technique to eliminate spurious superpixels and\nan adaptive perturbation technique to generate perturbed\nimages in the neighborhood of IE. Our results demonstrate\nthat the explanations from SLICE exhibit signiﬁcantly\nbetter consistency and ﬁdelity than LIME (and its variant\nBayLime).\n1. Introduction\nIn the broad spectrum of post-ad-hoc explanation methods,\nmodel-agnostic methods like LIME [13], SHAP [9] and\ntheir variants have been popular for extracting explanations\nfrom Black-Box models. While explanation methods such\nas Grad-CAM [16], Grad-CAM++ [2], and Ablation-CAM\n[12] require access to the intermediate layers of the model,\nmethods like LIME, and SHAP methods require access only\nto the input and the output of the model.\nIn our paper, we focus on the category of post-ad-\nhoc methods that uses local surrogate models for expla-\nnations. While LIME has the advantages of being model-\nagnostic and being able to extract explanations in a post-\nad-hoc manner, it is also inconsistent in the explanations\n[5, 7, 8, 22, 23, 25, 26]. [23] observes three types of un-\ncertainty: sampling variance in explaining a data point, sen-\nsitivity to the choice of parameters such as the size of the\nneighborhood and sample size, and variation of model cred-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n10988",
        "chosen": "SLICE is a new method enhancing LIME's consistency for image classification explanations. It uses feature selection to eliminate spurious superpixels and adaptive perturbation to generate neighborhood images, yielding more reliable and consistent explanations than LIME and its variants.",
        "rejected": "SLICE is proposed to stabilize explanations generated by LIME, addressing inconsistencies due to variance in segment importance ranks. SLICE uses feature selection and adaptive perturbation techniques to produce more consistent and faithful explanations for black-box models than LIME, enhancing reliability in explainable AI."
    },
    {
        "prompt": "summary: We study the underexplored but fundamental problem of\nmachine understanding of abstract freehand scene sketches.\nWe introduce a sketch encoder that ensures a semantically-\naware feature space, which we evaluate by testing its per-\nformance on a semantic sketch segmentation task. To train\nour model, we rely only on bitmap sketches accompanied\nby brief captions, avoiding the need for pixel-level anno-\ntations. To generalize to a large set of sketches and cat-\negories, we build upon a vision transformer encoder pre-\ntrained with the CLIP model. We freeze the text encoder and\nperform visual-prompt tuning of the visual encoder branch\nwhile introducing a set of critical modifications. First, we\naugment the classical key-query (k-q) self-attention blocks\nwith value-value (v-v) self-attention blocks. Central to our\nmodel is a two-level hierarchical training that enables ef-\nficient semantic disentanglement: The first level ensures\nholistic scene sketch encoding, and the second level focuses\non individual categories. In the second level of the hierar-\nchy, we introduce cross-attention between the text and vi-\nsion branches.\nOur method outperforms zero-shot CLIP\nsegmentation results by 37 points, reaching a pixel accu-\nracy of 85.5% on the FS-COCO sketch dataset. Finally, we\nconduct a user study that allows us to identify further im-\nprovements needed over our method to reconcile machine\nand human understanding of freehand scene sketches.\n1. Introduction\nEven a quick sketch can convey rich information about what\nis relevant in a visual scene: what objects there are and how\nthey are arranged. However, little work has been devoted\nto the task of machine scene sketch understanding, largely\ndue to a lack of data. Understanding sketches with meth-\nods designed for images is challenging because sketches\nhave very different statistics from images – they are sparser\nand lack detailed color and texture information. Moreover,\nsketches contain abstraction at multiple levels: the holis-\nCLIP zero shot\nsegmentation\nCLIP zero shot\nsegmentation\nOur\nsegmentation\nOur\nsegmentation\nA giraffe and a zebra are \nstanding on the grass.\nA man with a kite and a tree \nin the background.\nFigure 1. Comparison of the segmentation result obtained with\nCLIP visual encoder features and features from our model.\ntic scene level and the object level. Here we explore the\npromise of two main ideas: (1) the use of language to guide\nthe learning of how to parse scene sketches and (2) a two-\nlevel training network design for holistic scene understand-\ning and individual categories recognition.\nFreehand sketches can be represented as a sequence or\ncloud of individual strokes, or as a bitmap image. As one\nof the first works on scene sketch understanding, we target\na general setting where we assume only the availability of\nbitmap representations. We also aim at the method that can\ngeneralize to a large number of scenes and object categories.\nTo this end, we build our sketch encoder on a Visual Trans-\nformer (ViT) encoder pre-trained with a popular CLIP [44]\nfoundation model (Fig. 1). We propose a two-level hierar-\nchical training of our network, where the two levels (“Holis-\ntic” and “Category-level”) share the weights of our visual\nencoder. The first level focuses on ensuring that our model\ncan capture holistic scene understanding (Fig. 2: I. Holis-\ntic), while the second level ensures that the encoder can effi-\nciently encode and distinguish individual categories (Fig. 2:\nII. Category-level). We avoid reliance on tedious user per-\npixel annotations by leveraging sketch-caption pairs from\nthe FS-COCO dataset [9], and aligning the visual tokens of\nsketch patches with textual tokens from the sketch captions,\nusing triplet loss training. We strengthen the alignment by\nintroducing sketch-text cross-attention in the second level\nof the network’s hierarchy (Fig. 2: g.). Additionally, we in-\ntroduce a modified self-attention computation to the visual\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n4176",
        "chosen": "This paper introduces a sketch encoder for understanding abstract freehand scene sketches, evaluated on semantic sketch segmentation. The model, built on a CLIP-pretrained ViT, uses two-level hierarchical training and cross-attention to achieve high pixel accuracy on the FS-COCO dataset.",
        "rejected": "A sketch encoder is introduced that leverages a vision transformer pre-trained with CLIP for semantic segmentation of freehand scene sketches. The hierarchical training approach allows efficient disentanglement at both holistic and category levels, achieving superior zero-shot segmentation accuracy compared to CLIP on the FS-COCO dataset."
    },
    {
        "prompt": "summary: Vision-language foundation models have shown remark-\nable performance in various zero-shot settings such as im-\nage retrieval, classification, or captioning. But so far, those\nmodels seem to fall behind when it comes to zero-shot lo-\ncalization of referential expressions and objects in images.\nAs a result, they need to be fine-tuned for this task.\nIn\nthis paper, we show that pretrained vision-language (VL)\nmodels allow for zero-shot open-vocabulary object local-\nization without any fine-tuning. To leverage those capabil-\nities, we propose a Grounding Everything Module (GEM)\nthat generalizes the idea of value-value attention introduced\nby CLIPSurgery [17] to a self-self attention path. We show\nthat the concept of self-self attention corresponds to cluster-\ning, thus enforcing groups of tokens arising from the same\nobject to be similar while preserving the alignment with\nthe language space. To further guide the group formation,\nwe propose a set of regularizations that allows the model\nto finally generalize across datasets and backbones.\nWe\nevaluate the proposed GEM framework on various bench-\nmark tasks and datasets for semantic segmentation. GEM\nnot only outperforms other training-free open-vocabulary\nlocalization methods, but also achieves state-of-the-art re-\nsults on the recently proposed OpenImagesV7 large-scale\nsegmentation benchmark. 1\n1. Introduction\nVision-Language models, trained on large-scale web-based\ndatasets such as WIT-400M [29], LAION400M [30], or\nmetaclip-400M [35] with image-text supervision only, have\nso far shown a remarkable set of capabilities. These mod-\nels such as CLIP [29], OpenCLIP [30], BLIP [15], or re-\ncently MetaCLIP [35] exhibit the ability to generalize to a\nbroad range of downstream tasks like zero-shot image clas-\nsification [6, 12, 29], visual question answering [13], action\nrecognition [38, 40], image captioning [15, 16], and view\nsynthesis [11]. However, models trained with image-level\nobjectives such as contrastive loss, image-text matching, or\n1Code is available at https://github.com/WalBouss/GEM.\nCLIP\nGEM (ours)\nCLIPSurgery\nMaskCLIP\nBowl\nMan\nHamburger\nJet ski\nFigure 1. Qualitative results of training-free methods: given a text\nprompt, the similarity of each image token with the prompt is cal-\nculated (red:high, blue:low). The proposed GEM method provides\nimproved grouping and alignment compared to other approaches.\nimage captioning struggle to maintain their zero-shot ca-\npabilities for tasks related to visual localization [17, 42].\nEven worse, when prompting such models, e.g., for spe-\ncific objects, they often exhibit an inverse vision-language\nrelation: the prompt embedding has a larger distance from\nimage patches containing the object compared to patches of\nthe background, as shown in Figure 1.\nIn order to leverage vision-language models (VLMs)\nto localize objects in an open-vocabulary setting, different\nsets of approaches have been proposed. The first line of\nwork trains models to detect or segment regions in an im-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n3828",
        "chosen": "GEM enables zero-shot open-vocabulary object localization using pretrained VL models. The proposed Grounding Everything Module uses self-self attention and regularizations to improve token grouping and alignment, achieving state-of-the-art results on segmentation benchmarks without fine-tuning.",
        "rejected": "GEM enhances zero-shot object localization capabilities of pretrained vision-language models through self-self attention paths, promoting clustering of tokens representing the same object. GEM outperforms existing methods on various benchmarks and achieves state-of-the-art results on OpenImagesV7 large-scale segmentation."
    },
    {
        "prompt": "summary: Text-to-image diffusion models have recently received in-\ncreasing interest for their astonishing ability to produce\nhigh-ﬁdelity images from solely text inputs.\nSubsequent\nresearch efforts aim to exploit and apply their capabili-\nties to real image editing.\nHowever, existing image-to-\nimage methods are often inefﬁcient, imprecise, and of lim-\nited versatility. They either require time-consuming ﬁne-\ntuning, deviate unnecessarily strongly from the input im-\nage, and/or lack support for multiple, simultaneous edits.\nTo address these issues, we introduce LEDITS++, an efﬁ-\ncient yet versatile and precise textual image manipulation\ntechnique. LEDITS++’s novel inversion approach requires\nno tuning nor optimization and produces high-ﬁdelity re-\nsults with a few diffusion steps. Second, our methodology\nsupports multiple simultaneous edits and is architecture-\nagnostic. Third, we use a novel implicit masking technique\nthat limits changes to relevant image regions. We propose\nthe novel TEdBench++ benchmark as part of our exhaus-\ntive evaluation. Our results demonstrate the capabilities of\nLEDITS++ and its improvements over previous methods.\n1. Introduction\nText-to-image diffusion models (DM) have garnered recog-\nnition for their ability to generate high-quality images from\ntextual descriptions. A growing body of research has re-\ncently been dedicated to utilizing these models for manip-\nulating real images.\nHowever, several barriers prevent\nmany real-world applications of diffusion-based image edit-\ning. Current methods often entail computationally expen-\nsive model tuning or other optimization, presenting prac-\ntical challenges [6, 18, 28, 30, 44]. Additionally, existing\ntechniques have the proclivity to induce profound changes\nto the original image [17, 26], often resulting in completely\ndifferent images. Lastly, all these approaches are inherently\nconstrained when editing multiple (arbitrary) concepts si-\nmultaneously.\nWe tackle these problems by introducing\nLEDITS++1, a diffusion-based image editing technique ad-\ndressing these limitations.\nLEDITS++2 offers a streamlined approach for textual\nimage editing, eliminating the need for extensive parameter\ntuning. To this end, we derive image inversion for a more ef-\nﬁcient diffusion sampling algorithm to a) drastically reduce\ncomputational resources and b) guarantee perfect image re-\nconstruction. Thus, we overcome computational obstacles\nand avoid changes in the edited image in the ﬁrst place. Fur-\nthermore, we use a novel implicit masking approach to se-\nmantically ground each edit instruction to its relevant image\nregion. This further optimizes changes to the image by re-\ntaining the overall image composition and object identity.\n*Equal contribution\n†Partially as research intern at Adobe\n1LEDITS++ stands for Limitless Edits with sde-dpm-solver++.\n2https://huggingface.co/spaces/leditsplusplus/project\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8861",
        "chosen": "LEDITS++ is an efficient textual image manipulation technique using a novel inversion approach and implicit masking. It requires no tuning, supports multiple edits, and produces high-fidelity results with few diffusion steps, outperforming previous methods.",
        "rejected": "LEDITS++ presents an efficient textual image manipulation technique requiring no fine-tuning, supporting multiple simultaneous edits, and using implicit masking to limit changes to relevant regions. It provides high-fidelity results with fewer diffusion steps, making it suitable for versatile and precise image editing applications."
    },
    {
        "prompt": "summary: Neural approaches have shown a significant progress\non camera-based reconstruction. But they require either a\nfairly dense sampling of the viewing sphere, or pre-training\non an existing dataset, thereby limiting their generaliz-\nability.\nIn contrast, photometric stereo (PS) approaches\nhave shown great potential for achieving high-quality re-\nconstruction under sparse viewpoints. Yet, they are imprac-\ntical because they typically require tedious laboratory con-\nditions, are restricted to dark rooms, and often multi-staged,\nmaking them subject to accumulated errors.\nTo address\nthese shortcomings, we propose an end-to-end uncalibrated\nmulti-view PS framework for reconstructing high-resolution\nshapes acquired from sparse viewpoints in a real-world en-\nvironment. We relax the dark room assumption, and allow\na combination of static ambient lighting and dynamic near\nLED lighting, thereby enabling easy data capture outside\nthe lab. Experimental validation confirms that it outper-\nforms existing baseline approaches in the regime of sparse\nviewpoints by a large margin. This allows to bring high-\naccuracy 3D reconstruction from the dark room to the real\nworld, while maintaining a reasonable data capture com-\nplexity.\n1. Introduction\nThe challenge of 3D reconstruction stands as a corner-\nstone in both computer vision and computer graphics. De-\nspite notable progress in recovering an object’s shape from\ndense image viewpoints, predicting consistent geometry\nfrom sparse viewpoints remains a difficult task. Contem-\nporary approaches employing neural data structures depend\nheavily on extensive training data to achieve generalization\nin the context of sparse views. Additionally, the presence of\na wide baseline or textureless objects form significant ob-\nstacles. In contrast, photometric methodologies like photo-\nmetric stereo (PS) excel in reconstructing geometry, even in\ntextureless regions. This capability is attributed to the abun-\n*The contribution was done while at TUM.\nMulti-\nillumination\ndata at\nviewpoint 1\nMulti-\nillumination\ndata at\nviewpoint 2\nPS-NeRF [60]\nOurs\nFigure 1. We introduce the first framework for multi-view un-\ncalibrated point-light photometric stereo. Given a set of PS im-\nages captured from different viewpoints (left), our method recov-\ners high-fidelity 3D reconstruction (right). The acquisition of un-\ncalibrated point-light PS imagery captured under ambient lighting\nin a sparse multi-view setup does not only allow for easy data cap-\nture, but also leads to 3D reconstructions of unprecedented detail.\nHere, with as few as two views we are able to reconstruct the squir-\nrel’s 3D geometry at higher precision than the state-of-the-art.\ndance of shading information derived from images acquired\nunder diverse illumination. Yet, such approaches typically\nrequire a controlled laboratory setup to fulfill the necessary\ndark room and directional light assumptions. As a conse-\nquence, PS approaches become impractical beyond the con-\nfines of a laboratory.\nTo address these shortcomings we combine state-of-the-\nart volume rendering formulations with a sparse multi-view\nphotometric stereo model.\nIn particular, we advocate a\nphysically realistic lighting model that combines ambient\nlight and uncalibrated point-light illumination.\nOur ap-\nproach facilitates simplified data acquisition, and we intro-\nduce a novel pipeline capable of reconstructing an object’s\ngeometry from a sparse set of viewpoints, even if the object\nis completely textureless. Furthermore, since we assume a\npoint-light model instead of distant directional lighting, we\ncan infer absolute depth from a single viewpoint, allowing\nus to address the challenge of wide camera baselines.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n11862",
        "chosen": "This paper proposes an end-to-end uncalibrated multi-view photometric stereo framework for high-resolution 3D reconstruction from sparse viewpoints in real-world settings. It combines static ambient and dynamic LED lighting, outperforming baselines and enabling lab-quality reconstruction in practical conditions.",
        "rejected": "An end-to-end uncalibrated multi-view photometric stereo framework is introduced for 3D reconstruction from sparse viewpoints under ambient and near LED lighting conditions. The approach overcomes limitations of laboratory setups, enabling high-accuracy reconstructions in real-world environments without controlled lighting."
    },
    {
        "prompt": "summary: This paper introduces a versatile paradigm for inte-\ngrating multi-view reﬂectance (optional) and normal maps\nacquired through photometric stereo. Our approach em-\nploys a pixel-wise joint re-parameterization of reﬂectance\nand normal, considering them as a vector of radiances\nrendered under simulated, varying illumination. This re-\nparameterization enables the seamless integration of re-\nﬂectance and normal maps as input data in neural volume\nrendering-based 3D reconstruction while preserving a sin-\ngle optimization objective. In contrast, recent multi-view\nphotometric stereo (MVPS) methods depend on multiple,\npotentially conﬂicting objectives. Despite its apparent sim-\nplicity, our proposed approach outperforms state-of-the-art\napproaches in MVPS benchmarks across F-score, Chamfer\ndistance, and mean angular error metrics. Notably, it sig-\nniﬁcantly improves the detailed 3D reconstruction of areas\nwith high curvature or low visibility.\n1. Introduction\nAutomatic 3D reconstruction is pivotal in various ﬁelds,\nsuch as archaeological and cultural heritage (virtual recon-\nstruction), medical imaging (surgical planning), virtual and\naugmented reality, games and ﬁlm production.\nMulti-view stereo (MVS) [5], which retrieves the geom-\netry of a scene seen from multiple viewpoints, is the most\nfamous 3D reconstruction solution. Coupled with neural\nvolumetric rendering (NVR) techniques [22], it effectively\nhandles complex structures and self-occlusions. However,\ndealing with non-Lambertian scenes remains a challenge\ndue to the breakdown of the underlying brightness consis-\ntency assumption. The problem is also ill-posed in certain\nconﬁgurations e.g., poorly textured scene [25] or degener-\n*Equal contributions. brument.bcb@gmail.com / rb@di.ku.dk\nFigure 1. One image from DiLiGenT-MV’s Buddha dataset [12],\nand 3D reconstruction results from several recent MVPS methods:\n[11, 26, 27] and ours. The latter provides the ﬁne details closest to\nthe ground truth (GT), while being remarkably simpler.\nate viewpoints conﬁgurations with limited baselines. More-\nover, despite recent efforts in this direction [13], recovering\nthe thinnest geometric details remains difﬁcult under ﬁxed\nillumination. In such a setting, estimating the reﬂectance of\nthe scene also remains a challenge.\nOn the other hand, photometric stereo (PS) [24], which\nrelies on a collection of images acquired under varying\nlighting, excels in the recovery of high-frequency details\nunder the form of normal maps. It is also the only pho-\ntographic technique that can estimate reﬂectance.\nAnd,\nwith the recent advent of deep learning techniques [8], PS\ngained enough maturity to handle non-Lambertian surfaces\nand complex illumination. Yet, its reconstruction of geom-\netry’s low frequencies remains suboptimal.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n5230",
        "chosen": "RNb-NeuS integrates multi-view reflectance and normal maps into neural volume rendering via a joint re-parameterization. This approach outperforms state-of-the-art methods in MVPS benchmarks, improving 3D reconstruction of detailed areas.",
        "rejected": "The paper proposes a joint re-parameterization of reflectance and normal maps as radiances under simulated illumination for integration into neural volume rendering-based 3D reconstruction. This method simplifies optimization and improves reconstruction quality, especially in areas with high curvature or low visibility, outperforming state-of-the-art approaches in MVPS benchmarks."
    },
    {
        "prompt": "summary: In this paper we tackle the problem of learning Structure-\nfrom-Motion (SfM) through the use of graph attention net-\nworks. SfM is a classic computer vision problem that is\nsolved though iterative minimization of reprojection errors,\nreferred to as Bundle Adjustment (BA), starting from a good\ninitialization.\nIn order to obtain a good enough initial-\nization to BA, conventional methods rely on a sequence\nof sub-problems (such as pairwise pose estimation, pose\naveraging or triangulation) which provide an initial solu-\ntion that can then be refined using BA. In this work we re-\nplace these sub-problems by learning a model that takes\nas input the 2D keypoints detected across multiple views,\nand outputs the corresponding camera poses and 3D key-\npoint coordinates.\nOur model takes advantage of graph\nneural networks to learn SfM-specific primitives, and we\nshow that it can be used for fast inference of the recon-\nstruction for new and unseen sequences. The experimen-\ntal results show that the proposed model outperforms com-\npeting learning-based methods, and challenges COLMAP\nwhile having lower runtime.\nOur code is available at:\nhttps://github.com/lucasbrynte/gasfm/.\n1. Introduction\nStructure-from-Motion (SfM) is a classic and still relevant\nproblem in computer vision.\nThe goal of SfM is to es-\ntimate camera poses and 3D coordinates of keypoints de-\ntected across multiple images, and can be formulated as an\noptimization over m camera matrices {Pi}, i = 1, . . . , m\nand n 3D points {Xj} , j = 1, . . . , n of the form\n  \\begin\n {aligned\n}\n &\n \\unde rset\n {\\{\\ma th\nbf { P} _i\\} ,\\{ \\mathbf {X}_j\\}}{\\text {minimize}} & & \\sum _{ij} r\\left (\\mathbf {m}_{ij}, \\mathbf {z}_{ij} \\right ) \\\\ & \\text {subject to} & & \\mathbf {z}_{ij} = \\mathbf {P}_i \\mathbf {\\bar {X}}_j, \\text { } \\forall i,j \\end {aligned} \\label {eq:sfm_optimization} \n(1)\nwhere mij holds the 2D coordinates of the jth keypoint in\nthe ith image. The loss in (1) is generally chosen as the\nreprojection error\n  r\\le ft ( \\ mathb f  {m}_{ ij}, \\mathbf {z}_{ij} \\right ) = || \\mathbf {m}_{ij} - \\Pi \\left (\\mathbf {z}_{ij}\\right ) ||^2 \n(2)\nwhere Π(x) =\n\u0010\nx1\nx3 , x2\nx3\n\u0011\n, and the nonlinear least squares\nproblem (1), referred to as Bundle Adjustment (BA) [15],\ncan be solved iteratively using second-order methods like\nLevenberg-Marquardt [15, 41]. Given the sparsity of the\nproblem, sparse computation methods [27] can be used in\norder to increase the efficiency of the optimization, allow-\ning BA to be used even for scenes with a large number of\nviews or points. However, it is widely known that BA is\nhighly non-convex and tends to converge to the nearest local\nminimum when not initialized close to the globally optimal\nsolution. As a consequence, BA is typically the last step of\na reconstruction pipeline, preceeding global SfM methods\nsuch as [9, 10, 20, 25], or incremental SfM methods such\nas [2, 37, 38] that solve a sequence of subproblems like pair-\nwise pose estimation, pose averaging, triangulation or cam-\nera resection [13, 15, 22, 32]. A different approach consists\nof projective factorization methods [8, 17, 18, 28, 39, 48]\nwhich factorize the 2m × n measurement matrix into two\nrank four matrices corresponding to the camera matrices\nand 3D points (in homogeneous coordinates). In particular,\nworks like [17, 18, 48] allow initialization-free SfM given\ntheir wide basin of convergence, meaning that their meth-\nods can be initialized with random camera poses and still\nconverge with a high rate of success to the desired global\nminimum. Even though these methods have been improv-\ning in terms of accuracy and robustness to missing data,\nfactorization-based methods require the input data to be al-\nmost completely free of outliers which unfortunately cannot\nbe guaranteed in most real world sequences or datasets, and\nhence severely compromises the usability of these methods.\nA common challenge with all these approaches to solve\nSfM is their scalability as the number of views and key-\npoints increase.\nIncremental SfM tries to tackle this is-\nsue by starting with a subset of the views, estimate its re-\nconstruction and incrementally adding more views. Some\nfactorization-based methods can also take advantage of the\nsame sparse computation methods used in BA, which sig-\nnificantly improves their ability to scale with sequence size.\nWhile this allows to reconstruct scenes with thousands of\nviews and millions of points, it can still take hours to re-\ncover the reconstruction of a single scene.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n4808",
        "chosen": "This work presents GASFM, a graph attention network for learning Structure-from-Motion. The model takes 2D keypoints as input and outputs camera poses and 3D coordinates, offering faster reconstruction than COLMAP with competitive accuracy.",
        "rejected": "Using graph attention networks, this work replaces traditional sub-problems in Structure-from-Motion (SfM) with a learned model for fast inference of camera poses and 3D keypoints. Experimental results demonstrate superior performance compared to competing learning-based methods and challenge COLMAP while reducing runtime."
    },
    {
        "prompt": "summary: Existing object recognition models have been shown to\nlack robustness in diverse geographical scenarios due to\ndomain shifts in design and context. Class representations\nneed to be adapted to more accurately reflect an object con-\ncept under these shifts.\nIn the absence of training data\nfrom target geographies, we hypothesize that geographi-\ncally diverse descriptive knowledge of categories can en-\nhance robustness. For this purpose, we explore the feasibil-\nity of probing a large language model for geography-based\nobject knowledge, and we examine the effects of integrat-\ning knowledge into zero-shot and learnable soft prompt-\ning with CLIP. Within this exploration, we propose geog-\nraphy knowledge regularization to ensure that soft prompts\ntrained on a source set of geographies generalize to an un-\nseen target set. Accuracy gains over prompting baselines on\nDollarStreet while training only on Europe data are up to\n+2.8/1.2/1.6 on target data from Africa/Asia/Americas, and\n+4.6 overall on the hardest classes. Competitive perfor-\nmance is shown vs. few-shot target training, and analysis is\nprovided to direct future study of geographical robustness.\n1. Introduction\nThe performance of object recognition models degrades\nwhen tested in new geographies (e.g., cities, countries, con-\ntinents) [7, 21, 33, 39, 43]. Numerous factors contribute to\nthe challenging problem of geographical domain shift, such\nas cross-geography changes in object design/parts, materi-\nals, and context. These changes in turn may be due to cul-\ntural, climate, or economic differences around the world.\nRecent work has shown standard adaptation techniques fail\nwhen used for geographical domain shifts [21, 33], but there\nhas yet to be significant progress in the creation of tech-\nniques that improve geographical robustness. Such progress\nis necessary to ensure equitable use of AI in the future.\nFigure 1.\nDescriptive knowledge can address concept shifts\nacross geographies. Observe the wide range of object designs and\ncontexts in the DollarStreet [11] category tools around the world.\nOur work’s premise is that textual representations for classes in\nvision-language models can be enhanced to better suit diverse ob-\nject representations across geographies. Map made with [16].\nOverall, models need representations that adequately\ncapture a category’s various forms around the world. A\nnatural solution is to collect training data of objects from\ndifferent regions.\nHowever, this approach is expensive,\ntakes significant effort, and is difficult for regions with lim-\nited Internet access. Fortunately, geographical shifts have a\nunique property compared to other common domain shifts\n(e.g. ones due to artistic style or weather changes)—they\ncan be addressed with descriptive knowledge about con-\ncept changes. In other words, it is possible to describe the\nfeatures of an object in a region and use this information\nto adapt a model’s default representation. For instance, as\nshown in Fig. 1, for rural areas in Papua New Guinea, tools\ncan be described as being used for “cooking, hunting, and\nfishing”, and for rural areas in Malawi, tools may often be\n“made of metal and wood, for farming”. Models should ac-\ncount for diverse presentations and contexts of a category\nand not be limited to biased presentations (e.g. if the model\nlearns tools as just being “metallic with logos”).\nWe examine the effects of probing geo-diverse knowl-\nedge in two ways.\nFirst, we analyze whether a vision-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n13515",
        "chosen": "The paper explores incorporating geo-diverse knowledge into prompting for geographical robustness in object recognition. Using a large language model for geography-based knowledge and integrating it with CLIP via soft prompting and regularization improves accuracy on target data from diverse regions.",
        "rejected": "The research develops an advanced framework combining photometric stereo and neural rendering for reconstructing textureless objects from sparse viewpoints. By integrating ambient and point-light illumination, it achieves detailed 3D reconstructions outside controlled environments, overcoming limitations associated with conventional photometric stereo techniques."
    },
    {
        "prompt": "summary: Despite noise and caption quality having been acknowl-\nedged as important factors impacting vision-language con-\ntrastive pre-training, in this paper, we show that the full\npotential of improving the training process by addressing\nsuch issues is yet to be realized.\nSpecifically, we firstly\nstudy and analyze two issues affecting training: incorrect\nassignment of negative pairs, and low caption quality and\ndiversity. Then, we devise effective solutions for address-\ning both problems, which essentially require training with\nmultiple true positive pairs. Finally, we propose training\nwith sigmoid loss to address such a requirement. We show\nvery large gains over the current state-of-the-art for both\nimage recognition (∼+6% on average over 11 datasets)\nand image retrieval (∼+19% on Flickr30k and ∼+15%\non MSCOCO).\n1. Introduction\nLarge-scale contrastive image-text pre-training has emerged\nas the prevalent method for vision-language representation\nlearning [14, 21, 27, 29, 38, 51, 52].\nThe majority of\ndatasets employed for pre-training are web-collected [4, 10,\n34, 40–43, 45]. They offer a varied data distribution and\nare sufficiently large to effectively train high-performing\nvision-language models. However, since the raw captions\nfor each image are typically extracted from associated tags\nor descriptions, they often exhibit low quality, being noisy\nand suboptimal for training purposes [21, 27]. Although\nsome attempts to fix such issues have been already de-\nscribed, to some extent, in literature (e.g.\nALIP [50],\nBLIP [27]), in this work, we show that the full potential of\nimproving the quality of the training process is far from be-\ning fully realized. Specifically, by studying and addressing\nspecific issues related to noise and low data quality, in this\nwork, we show that our improved vision-language training\npipeline can achieve massive gains over the current state-\nof-the-art methods for both image recognition (∼+6% on\nFigure 1. Our approach, FFF, achieves state-of-the-art accuracy\nacross multiple datasets, largely outperforming prior methods.\naverage over 11 datasets) and image retrieval (∼+19% on\nFlickr30k [53] and ∼+15% on MSCOCO [30]).\nThe first issue we study is related to noise impacting con-\ntrastive learning: near-duplicate samples which are incor-\nrectly treated as negative pairs. Even within a batch, it is not\nuncommon to find images and/or captions that are seman-\ntically similar or even identical. Since standard contrastive\nlearning assumes one positive pair, this significantly hinders\nthe training process and the quality of the trained models.\nThe second issue we study is related to low caption qual-\nity and diversity. Captions can be short and lacking detail,\nnoisy, or even entirely irrelevant to the image. Moreover,\nsince the mapping process between image and text is one-\nto-many, more than one caption is needed to provide an ap-\nproximate description of the image.\nTo fix issue one, we propose an algorithm that mines new\npositive pairs based on image-text, image-image, and text-\ntext similarities, aiming to decrease the number of false neg-\natives in the training data arising due to semantically similar\nimages and/or captions.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n14172",
        "chosen": "FFF addresses noise and caption quality in vision-language contrastive pre-training by studying incorrect negative pair assignments and low caption quality/diversity. It proposes training with multiple true positive pairs using a sigmoid loss, achieving large gains over the state-of-the-art in image recognition and retrieval.",
        "rejected": "This study addresses two issues affecting vision-language pre-training: incorrect assignment of negative pairs and low caption quality. By introducing an algorithm that mines new positive pairs and proposes training with sigmoid loss, significant improvements in image recognition and retrieval are achieved. The approach enhances model accuracy by addressing false negatives and improving caption quality."
    },
    {
        "prompt": "summary: We consider a critical issue of false negatives in Vision-\nLanguage Pre-training (VLP), a challenge that arises from\nthe inherent many-to-many correspondence of image-text\npairs in large-scale web-crawled datasets. The presence of\nfalse negatives can impede achieving optimal performance\nand even lead to a significant performance drop. To address\nthis challenge, we propose MAFA (MAnaging FAlse nega-\ntives), which consists of two pivotal components building\nupon the recently developed GRouped mIni-baTch sampling\n(GRIT) strategy: 1) an efficient connection mining process\nthat identifies and converts false negatives into positives, and\n2) label smoothing for the image-text contrastive (ITC) loss.\nOur comprehensive experiments verify the effectiveness of\nMAFA across multiple downstream tasks, emphasizing the\ncrucial role of addressing false negatives in VLP, potentially\neven surpassing the importance of addressing false posi-\ntives. In addition, the compatibility of MAFA with the recent\nBLIP-family model is also demonstrated. Code is available\nat https://github.com/jaeseokbyun/MAFA.\n1. Introduction\nWith large-scale web-crawled datasets [3, 50–52], majorities\nof vision-language pre-training (VLP) models are trained\nin a self-supervised learning manner using the combina-\ntion of several pre-tasks and losses [2, 33, 34, 63, 65]: e.g.,\nmasked language modeling (MLM), image-text contrastive\n(ITC), and image-text matching (ITM) losses. Despite their\npromising results, one of the pressing challenges they face\nis the presence of noisy captions in image-text pairs that\noften provide incomplete or even incorrect descriptions\n[9, 13, 41, 44, 47, 59, 64]. Several recent works have fo-\ncused on addressing such issue of noisy correspondence in\nimage-text pairs [11, 18, 19, 21, 34, 47]. Notably, BLIP [34]\nintroduced a caption refinement process by leveraging an\n*Equal contribution\n†Corresponding author\nFigure 1. Examples of positives, negatives, and false negatives\namong image-text pairs.\nimage captioning model and a filter to generate synthetic\nclean captions and remove noisy captions. Such process can\nbe seen as correcting the false positives that were injected\nby the noisy captions.\nContrastively, we note that there is another type of chal-\nlenge for VLP that stems from the nature of many-to-many\ncorrespondence of image-text pairs. Namely, it is common\nfor an image (resp. text) to have additional positive con-\nnections (blue lines in Figure 1) with another texts (resp.\nimages), which are paired with their corresponding images\n(resp. texts). This is due to the fact that the existing image-\ntext datasets are constructed by only collecting paired image-\ntext instances, hence the information regarding non-paired\nbut semantically close image-text combination can be missed.\nConsequently, for each image (resp. text), the text (resp. im-\nage) that is given as the pair with the image (resp. text)\nis treated as the only positive sample during pre-training,\nwhile the other texts (resp. images) are all treated as nega-\ntives. This setup inevitably leads to the prevalence of false\nnegatives during computing ITC and ITM losses and con-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n27314",
        "chosen": "MAFA tackles false negatives in Vision-Language Pre-training (VLP) through an efficient connection mining process and label smoothing for the image-text contrastive loss, building on the GRIT strategy. It demonstrates significant improvements across downstream tasks, highlighting the importance of addressing false negatives in VLP.",
        "rejected": "The paper introduces MAFA, designed to tackle false negatives in vision-language pre-training. It utilizes a connection mining process and label smoothing for ITC loss. Experiments show MAFA's effectiveness across multiple tasks, demonstrating its compatibility with recent models like BLIP, achieving superior performance by managing false negatives."
    },
    {
        "prompt": "summary: Nonlinearities are decisive in neural representation\nlearning.\nTraditional Activation (Act) functions im-\npose fixed inductive biases on neural networks with ori-\nented biological intuitions. Recent methods leverage self-\ngated curves to compensate for the rigid traditional Act\nparadigms in fitting flexibility.\nHowever, substantial im-\nprovements are still impeded by the norm-induced mis-\nmatched feature re-calibrations (see Section 1), i.e., the ac-\ntual importance of a feature can be inconsistent with its\nexplicit intensity such that violates the basic intention of\na direct self-gated feature re-weighting.\nTo address this\nproblem, we propose to learn discriminative neural feature\nAct with a novel prototype, namely, AdaShift, which en-\nhances typical self-gated Act by incorporating an adaptive\nshift factor into the re-weighting function of Act. AdaShift\ncasts dynamic translations on the inputs of a re-weighting\nfunction by exploiting comprehensive feature-filter context\ncues of different ranges in a simple yet effective manner.\nWe obtain the new intuitions of AdaShift by rethinking the\nfeature-filter relationships from a common Softmax-based\nclassification and by generalizing the new observations to\na common learning layer that encodes features with updat-\nable filters. Our practical AdaShifts, built upon the new\nAct prototype, demonstrate significant improvements to the\npopular/SOTA Act functions on different vision benchmarks.\nBy simply replacing ReLU with AdaShifts, ResNets can\nmatch advanced Transformer counterparts (e.g., ResNet-50\nvs. Swin-T) with lower cost and fewer parameters.\n1. Introduction\nNonlinear Act functions are indispensable for the learning\nof discriminative neural features [2, 7, 11, 17, 33, 39, 39,\n42]. Neuronal behaviors [24, 40] originate traditional Act\nmodels, e.g., Softplus [15] and ReLU [34], which are fixed\nand monotonic in calculations. To realize finer rectifica-\ntions, recent works investigated self-gated-style Act func-\nTop-1 Accuracy (%)\n81.2(+2.3)\n81.2(+2.3)\n80.3(+3.1)\n80.3(+3.1)\n80.6(+1.7)\n80.6(+1.7)\n68\n70\n72\n74\n76\n78\n79\n80\n81\n82\n75\n76\n77\n78\n79\n77\n78\n79\n80\n81\n78.9\n78.9\n77.2\n79.9(+2.7)\n79.9(+2.7)\n77.5\n78.0\n78.1(+3.2)\n78.1(+3.2)\n77.2(+2.3)\n77.2(+2.3)\n74.9\n76.5\n76.1\n73.9(+5.2)\n73.9(+5.2)\n72.2(+3.5)\n72.2(+3.5)\n68.7\n70.4\n70.0\n69.4\n75.7\nN/A\nN/A\nN/A\nResNet-26\nResNet-14\nResNet-50\nResNet-101\nResNet-26\nResNet-14\nResNet-50\nResNet-101\nPserf\nSMU\nMeta-Acon\nReLU\nAdaShift-B\nAdaShift-MA\nPserf\nSMU\nMeta-Acon\nReLU\nAdaShift-B\nAdaShift-MA\nFigure 1. Comparison of our AdaShift-B and AdaShift-MA to\nthe ReLU [34] baseline and popular/SOTA Act models [4, 5, 30]\non ImageNet [13] with ResNet backbones, where the areas of the\ncircular patterns represent the relative amount of parameters com-\npared to the corresponding ReLU baselines. Our AdaShift-B and\nAdaShift-MA improve different activation functions consistently\nand remarkably on different backbones varying by size with neg-\nligible parameters added to the ReLU baselines.\ntions based on the general prototype\n \\la b e l { b ase_form} \\phi \\left (x\\right )=\\varsigma \\left (x\\right )x\\,, \n(1)\nwhere x P R is a given feature unit (i.e., scalar), ϕ : R Ñ R\ndenotes the applied Act function of x, and ς : R Ñ R\ndefines the re-weighting function of ϕ . As a special case,\nReLU can be included in this prototype by specifying ς pxq\nas a binary masking of 0 and 1 for x ď 0 and x ą 0, re-\nspectively. Despite the broad applicability, ReLU leaves\ntwo practical constraints on neural Act from (1) its rigid\nmasking on positive features, i.e., unified weight assign-\nments that possibly neutralize the discriminativeness, and\n(2) hard-zero-truncation on negative features that possibly\nleads to the “dead tensors” problem.\nRecent methods addressed these by introducing smooth\nre-weighting functions with two assumed properties:\n1. ς pxq is bounded (typically, ς pxq P p0, 1q);\n2. ς pxq is monotonically non-decreasing about x .\nThese properties theoretically ensure the stability and con-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n5947",
        "chosen": "AdaShift enhances traditional self-gated activation functions by incorporating an adaptive shift factor into the re-weighting function. This approach, which exploits comprehensive feature-filter context cues, demonstrates significant improvements over popular activation functions on various vision benchmarks when integrated with ResNet backbones.",
        "rejected": "To address feature recalibration mismatches, AdaShift is proposed, incorporating an adaptive shift factor into activation functions. This method enhances neural networks' discriminative power without significantly increasing parameters or costs, leading to improved performance on various benchmarks."
    },
    {
        "prompt": "summary: Representing signals using coordinate networks domi-\nnates the area of inverse problems recently, and is widely\napplied in various scientific computing tasks. Still, there\nexists an issue of spectral bias in coordinate networks, lim-\niting the capacity to learn high-frequency components. This\nproblem is caused by the pathological distribution of the\nneural tangent kernel’s (NTK’s) eigenvalues of coordinate\nnetworks. We find that, this pathological distribution could\nbe improved using the classical batch normalization (BN),\nwhich is a common deep learning technique but rarely used\nin coordinate networks. BN greatly reduces the maximum\nand variance of NTK’s eigenvalues while slightly modifies\nthe mean value, considering the max eigenvalue is much\nlarger than the most, this variance change results in a shift\nof eigenvalues’ distribution from a lower one to a higher\none, therefore the spectral bias could be alleviated (see\nFig. 1). This observation is substantiated by the significant\nimprovements of applying BN-based coordinate networks to\nvarious tasks, including the image compression, computed\ntomography reconstruction, shape representation, magnetic\nresonance imaging and novel view synthesis.\n1. Introduction\nCoordinate networks, which take the coordinates as inputs\nand output the signal attributes using multi-layer perceptron\n(MLP) models, have become a promising framework for\nsolving various inverse problems. Different from the clas-\nsical convolution-based networks which could only support\nup to 3D patterns as input [20, 57], the input coordinates are\norganized as 1D vectors in coordinate networks, enabling\nthe advantage of a general framework for solving inverse\nproblems with any dimensions. Therefore, coordinate net-\nworks have been widely applied in different areas of sci-\nentific computing [30], such as the hologram/tomography\n*This work was supported by the National Natural Science Foundation\nof China under Grants T2221003, 62071216 and the Leading Technology\nof Jiangsu Basic Research Plan (BK20192003).\nEigenvalues\nCount\nEigenvalues\nEigenvalues\n(a) Histograms of NTK Eigenvalues\n(b) Reconstructed images\n23.25 dB\n27.18 dB\n28.79 dB\n30.05 dB\n30.84 dB\n31.31 dB\nwithout BN\nwith BN\nMLP\nPEMLP-1\nPEMLP-5\nFigure 1. Batch normalization significantly alleviates the spectral\nbias of coordinate networks. (a) Batch normalization shifts the\nNTK’s eigenvalues distribution from a lower one to a higher one,\nthus (b) the spectral bias is alleviated and better performance is\nachieved compared with the one without batch normalization (e.g.,\nthe texture on the lion’s left paw). From left to right, each column\nrefers to the coordinate networks with ReLU activations, and po-\nsitional encoding [78] with 1 and 5 Fourier bases, respectively.\nimaging in microscopy [39, 83], 3D reconstruction and free-\nviewpoint roaming in computer vision/graphics [37, 48],\nphysical simulation in material design and hydrodynam-\nics [8, 59] and medical imaging [67, 68].\nYet, due to the spectral bias [58] of ReLU-based MLP,\ncoordinate networks prefer to learn the low-frequency com-\nponents of the signal, while the high-frequency compo-\nnents are learned at an extremely slow convergence. Sev-\neral works have been proposed to alleviate the spectral bias,\nsuch as the positional encoding [78] or frequency-related\nactivation functions [60, 72]. However, these explorations\nintroduce the ‘frequency-specified spectral bias’ [81], i.e.,\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25160",
        "chosen": "This paper shows that batch normalization can alleviate the spectral bias in coordinate networks by shifting the distribution of the neural tangent kernel's eigenvalues. This results in significant improvements in tasks like image compression and tomography reconstruction, where coordinate networks are widely applied.",
        "rejected": "Batch normalization (BN) is explored to alleviate spectral bias in coordinate networks, enhancing learning of high-frequency components. BN modifies the distribution of NTK eigenvalues, improving performance in tasks such as image compression and computed tomography reconstruction."
    },
    {
        "prompt": "summary: In this work, we present Digital Life Project, a frame-\nwork utilizing language as the universal medium to build\nautonomous 3D characters, who are capable of engaging\nin social interactions and expressing with articulated body\nmotions, thereby simulating life in a digital environment.\nOur framework comprises two primary components: 1) So-\ncioMind: a meticulously crafted digital brain that models\npersonalities with systematic few-shot exemplars, incorpo-\nrates a reflection process based on psychology principles,\nand emulates autonomy by initiating dialogue topics; 2)\nMoMat-MoGen: a text-driven motion synthesis paradigm\nfor controlling the character’s digital body. It integrates\nmotion matching, a proven industry technique to ensure mo-\ntion quality, with cutting-edge advancements in motion gen-\neration for diversity.\nExtensive experiments demonstrate\nthat each module achieves state-of-the-art performance in\nits respective domain.\nCollectively, they enable virtual\ncharacters to initiate and sustain dialogues autonomously,\nwhile evolving their socio-psychological states.\nConcur-\nrently, these characters can perform contextually relevant\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n582",
        "chosen": "The Digital Life Project presents a framework for autonomous 3D characters with social intelligence, featuring SocioMind (a digital brain modeling personalities) and MoMat-MoGen (a text-driven motion synthesis paradigm). This enables characters to engage in autonomous social interactions and express with articulated body motions.",
        "rejected": "The Digital Life Project framework uses language to build autonomous 3D characters capable of social interaction and articulated body motions. SocioMind models personalities and initiates dialogues, while MoMat-MoGen synthesizes motion. Together, they enable virtual characters to autonomously engage in dialogue and evolve socio-psychological states, demonstrating state-of-the-art performance in their domains."
    },
    {
        "prompt": "summary: Traditional 3D content creation tools empower users to\nbring their imagination to life by giving them direct control\nover a scene’s geometry, appearance, motion, and camera\npath. Creating computer-generated videos, however, is a\ntedious manual process, which can be automated by emerg-\ning text-to-video diffusion models. Despite great promise,\nvideo diffusion models are difficult to control, hindering a\nuser to apply their own creativity rather than amplifying it.\nTo address this challenge, we present a novel approach that\ncombines the controllability of dynamic 3D meshes with the\nexpressivity and editability of emerging diffusion models. For\nthis purpose, our approach takes an animated, low-fidelity\nrendered mesh as input and injects the ground truth cor-\nrespondence information obtained from the dynamic mesh\ninto various stages of a pre-trained text-to-image genera-\ntion model to output high-quality and temporally consistent\nframes. We demonstrate our approach on various examples\nwhere motion can be obtained by animating rigged assets or\nchanging the camera path. Project page: primecai.github.io/\ngenerative rendering.\n△: Part of this work was done during an internship at Adobe Research.\n* : Equal contribution.\n1. Introduction\nArtists, designers, architects, and other creators rely on\ntraditional 3D content creation tools to render computer-\ngenerated videos. Unfortunately, existing 3D workflows are\nlaborious, time consuming, and require expertise. Emerging\ngenerative artificial intelligence tools, such as text-to-image\n(T2I) and text-to-video (T2V) models, solve these issues by\nautomating many of the manual steps of traditional work-\nflows. Video generation, however, is difficult to control\nin that it is not easily possible to specify scene layout and\nmotion in a temporally consistent manner.\nRecent approaches have attempted to control diffusion\nmodels. For example, ControlNet [39] uses a pre-trained\nT2I diffusion model and finetunes an adapter network that\nis conditioned on depth, pose, or edge images to control the\nlayout. This strategy is successful for generating individual\nframes, but results in flicker for video generation. Other\napproaches aim at learning the complex types of motions\nencountered in natural videos directly [3, 9, 11–13, 21, 33,\n35,38,40]. While successful in generating smooth motions,\nthese approaches are not easily controllable. Finally, video-\nto-video diffusion models [6,10,20,37] enable video editing\nand stylization, but they require a high-fidelity video as input,\nwhich is not always available.\nIn this paper, we aim to combine the power of 3D work-\nflows with T2I models for generating 4D-guided stylized\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n7611",
        "chosen": "This novel approach combines the controllability of dynamic 3D meshes with the expressivity of text-to-image diffusion models. By injecting ground truth correspondence information from dynamic meshes into pre-trained text-to-image models, it generates high-quality, temporally consistent frames, enabling contextually relevant character motions.",
        "rejected": "A novel approach combining dynamic 3D meshes with text-to-image generation models is presented. This method injects ground truth correspondence information into stages of a pre-trained model, producing high-quality, temporally consistent frames from animated low-fidelity rendered meshes, enhancing control and creativity in video generation."
    },
    {
        "prompt": "summary: In the current era of generative AI breakthroughs, gener-\nating panoramic scenes from a single input image remains\na key challenge. Most existing methods use diffusion-based\niterative or simultaneous multi-view inpainting. However,\nthe lack of global scene layout priors leads to subpar out-\nputs with duplicated objects (e.g., multiple beds in a bed-\nroom) or requires time-consuming human text inputs for\neach view. We propose L-MAGIC, a novel method leverag-\ning large language models for guidance while diffusing mul-\ntiple coherent views of 360◦panoramic scenes. L-MAGIC\nharnesses pre-trained diffusion and language models with-\nout ﬁne-tuning, ensuring zero-shot performance. The output\nquality is further enhanced by super-resolution and multi-\nview fusion techniques. Extensive experiments demonstrate\nthat the resulting panoramic scenes feature better scene lay-\nouts and perspective view rendering quality compared to re-\nlated works, with >70% preference in human evaluations.\nCombined with conditional diffusion models, L-MAGIC can\naccept various input modalities, including but not limited\nto text, depth maps, sketches, and colored scripts. Apply-\ning depth estimation further enables 3D point cloud gen-\neration and dynamic scene exploration with ﬂuid camera\nmotion. Code is available at https://github.com/\nZhipengCai/L-MAGIC-code-release.\n1. Introduction\nDiffusion models have achieved state-of-the-art perfor-\nmance in image generation. However, generating a 360◦\npanoramic scene from a single perspective image remains\na challenge, which is an important problem in many com-\nputer vision applications, such as architecture design, movie\nscene creation, and virtual reality (VR).\nTraining a model to directly generate panoramic im-\nages is challenging due to the lack of diverse large-scale\ndatasets. Hence, most existing works separate panoramic\nscenes into multiple perspective views, and inpaint them us-\n*Corresponding author (zhipeng.cai@intel.com)\nL-MAGIC\nDepth \nestimation\nFigure 1. Teaser. L-MAGIC is a novel method to generate a 360◦\npanoramic scene from a single input image. L-MAGIC utilizes\nlarge language models to control perspective diffusion models to\ngenerate multiple views with coherent 360◦layout. L-MAGIC is\nalso compatible with images synthesized by conditional genera-\ntive models, making it capable of creating panoramic scenes from\nvarious input modalities. A set of perspective images rather than\na single panoramic image also allows our method to leverage off-\nthe-shelf monocular depth estimation models to enable immersive\nexperiences, e.g., scene ﬂy-through or 3D point cloud generation.\ning pre-trained diffusion models. To ensure generalization,\nthe diffusion model is either frozen without any architec-\nture change [11] or combined with extra modules trained on\nsmall datasets for integrating multi-view information [25].\nA common approach to encode the scene informa-\ntion during multi-view inpainting is to provide a text-\nconditioned diffusion model with the description of the in-\nput image, which is generated by a user or an image cap-\ntioning model [13]. Though effective for extending local\nscene content, such approaches suffer from incoherence\nin the overall scene layout. Speciﬁcally, using the same\ntext description for diffusing different views along the 360◦\npanorama leads to artifacts and unnecessarily repeated ob-\njects. Current inpainting methods have no mechanism to\nleverage global scene information in individual views.\nIn this work, we show that state-of-the-art (vision) lan-\nguage models, without ﬁne-tuning, can be used to control\nmulti-view diffusion and effectively address the above prob-\nlem. We propose L-MAGIC (Fig. 1), a novel framework\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n7049",
        "chosen": "L-MAGIC leverages large language models to guide the diffusion of multiple coherent views for generating 360° panoramic scenes. It ensures zero-shot performance by utilizing pre-trained models without fine-tuning and enhances output quality with super-resolution and multi-view fusion techniques.",
        "rejected": "L-MAGIC leverages large language models to guide diffusion models for generating coherent views of 360-degree panoramic scenes. It achieves enhanced scene layouts and perspective rendering quality compared to related methods. L-MAGIC supports various input modalities and enables 3D point cloud generation, offering fluid camera motion and immersive experiences."
    },
    {
        "prompt": "summary: Camera-parameter-free multi-view pose estimation is an\nemerging technique for 3D human pose estimation (HPE).\nThey can infer the camera settings implicitly or explicitly\nto mitigate the depth uncertainty impact, showcasing sig-\nnificant potential in real applications. However, due to the\nlimited camera setting diversity in the available datasets,\nthe inferred camera parameters are always simply hard-\ncoded into the model during training and not adaptable to\nthe input in inference, making the learned models cannot\ngeneralize well under unseen camera settings. A natural\nsolution is to artificially synthesize some samples, i.e., 2D-\n3D pose pairs, under massive new camera settings. Un-\nfortunately, to prevent over-fitting the existing camera set-\nting, the number of synthesized samples for each new cam-\nera setting should be comparable with that for the existing\none, which multiplies the scale of training and even makes\nit computationally prohibitive. In this paper, we propose a\nnovel HPE approach under the invariant risk minimization\n(IRM) paradigm. Precisely, we first synthesize 2D poses\nfrom myriad camera settings. We then train our model un-\nder the IRM paradigm, which targets at learning a common\noptimal model across all camera settings and thus enforces\nthe model to automatically learn the camera parameters\nbased on the input data. This allows the model to accurately\ninfer 3D poses on unseen data by training on only a hand-\nful of samples from each synthesized setting and thus avoid\nthe unbearable training cost increment. Another appealing\nfeature of our method is that benefited from the capability\nof IRM in identifying the invariant features, its performance\non the seen camera settings is enhanced as well. Compre-\nhensive experiments verify the superiority of our approach.\n1. Introduction\n3D multi-view Human Pose Estimation (HPE) leverages the\ncamera relationship between multiple viewpoint to mitigate\n*Corresponding authors\nthe impact of depth uncertainty. Existing methods primar-\nily rely on camera parameters to construct epipolar geomet-\nric constraints between camera viewpoints. Unlike camera-\nparameter-required methods, camera-parameter-free meth-\nods can explicitly or implicitly recover camera parameters\nduring training, thus making them applicable in broader\nscenarios where camera parameters are unavailable, such\nas HPE in uncontrolled environments or dynamic HPE with\nmoving cameras. HPCP [21] leverages human pose prior\nsuch as bone length to optimize potential camera parame-\nters. Flex [4] models viewpoint-consistent 3D poses by hi-\nerarchical skeletal representation. MTF-Transformer [20]\nleverages temporal information to obtain more accurate\ncamera parameters. Probabilistic Triangulation [11] adopts\nMonte Carlo sampling to select the camera parameters.\nThese methods have achieved commendable results, closely\nmatching the performance of camera-parameter-free meth-\nods under seen camera setting.\nHowever, when generalized to unseen camera settings,\ncamera-parameter-free methods exhibit a great performance\ndrop, in contrast the drop in the camera-parameter-required\nmethods is negligible [20]. We argue that this discrepancy\nprimarily stems from the reason that the inferred camera\nparameters in the camer-parameter-free methods are always\nsimply hardcoded into the models during training, i.e., they\nare not adaptable to the input in inference. To be precise,\nthe number of camera viewpoints within existing datasets\nis rather limited (typically four or eight), whereas previous\nmainstream models require at least four camera viewpoints\nas input to address the challenges such as self-occlusion\nof the human body or inaccuracies in 2D pose estimation,\nmaking the number of available camera settings no larger\nthan two. With such extremely limited diversity of cam-\nera settings, because of the training imbalance, camera-\nparameter-free methods tend to memorize the specific cam-\nera settings rather than to generalize to arbitrary settings,\nthus significantly hindering their generalization capabilities\nacross varied camera settings. Our experiments have sub-\nstantiated this conjecture. Please refer to Tab. 5 for details.\nA natural solution to address the above challenge is to ar-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n2124",
        "chosen": "PoseIRM enhances 3D human pose estimation on unseen camera settings using invariant risk minimization. It synthesizes 2D poses from various camera settings and trains a model to learn a common optimal solution, allowing accurate 3D pose inference on unseen data with minimal samples per setting.",
        "rejected": "A novel HPE approach under the invariant risk minimization (IRM) paradigm is introduced. It synthesizes 2D poses from myriad camera settings and trains the model to learn common optimal parameters across all settings. This allows accurate inference of 3D poses on unseen data, enhancing generalization capabilities and performance on both seen and unseen camera settings."
    },
    {
        "prompt": "summary: Synthesizing realistic videos of talking faces under cus-\ntom lighting conditions and viewing angles benefits various\ndownstream applications like video conferencing. However,\nmost existing relighting methods are either time-consuming\nor unable to adjust the viewpoints. In this paper, we present\nthe first real-time 3D-aware method for relighting in-the-\nwild videos of talking faces based on Neural Radiance\nFields (NeRF). Given an input portrait video, our method\ncan synthesize talking faces under both novel views and\nnovel lighting conditions with a photo-realistic and disen-\ntangled 3D representation. Specifically, we infer an albedo\ntri-plane, as well as a shading tri-plane based on a de-\nsired lighting condition for each video frame with fast dual-\nencoders. We also leverage a temporal consistency network\nto ensure smooth transitions and reduce flickering artifacts.\nOur method runs at 32.98 fps on consumer-level hardware\nand achieves state-of-the-art results in terms of reconstruc-\ntion quality, lighting error, lighting instability, temporal\nconsistency and inference speed. We demonstrate the effec-\ntiveness and interactivity of our method on various portrait\nvideos with diverse lighting and viewing conditions.\n1. Introduction\nPortrait videos are widely used in various scenarios, such as\nvideo conferencing, video editing, entertainment, virtual re-\nality, etc. However, many portrait videos are captured under\nunsatisfactory conditions, such as environments that are ei-\nther too dark or too bright, or with virtual backgrounds that\ndo not match the lighting of the foreground. These factors\ndegrade the visual quality and realism of videos and affect\nthe user experience.\nOf particular significance is the context of augmented re-\n*Corresponding author is Lin Gao\nYaw = 0.3\nFront\nAlbedo\nGeometry\nLighting\nLighting\nFigure 1. Given a portrait video shown in the leftmost column, our\nmethod reconstructs a 3D relightable face for each video frame.\nUsers can then adjust their viewpoints and lighting conditions in-\nteractively. The second column displays relighted video frames\nwith a head pose yaw of 0.3, while the third column presents faces\nrelighted under an alternative lighting condition with a frontal head\npose. The rightmost column provides the predicted albedo and ge-\nometry of the reconstructed face. Please see the supplementary\nvideo for the full results.\nality (AR) and virtual reality (VR) applications, where users\noften seek to create 3D faces that can be dynamically re-\nlighted to fit the environment. This dynamic relighting ca-\npability becomes possible only when the underlying method\nis inherently 3D-aware and operates in real time.\nHowever, 3D-aware portrait video relighting is a chal-\nlenging task, since it involves modeling the complex inter-\nactions between the light, geometry, and appearance of hu-\nman faces, as well as ensuring the temporal coherence and\nnaturalness of synthesized videos. It is even more challeng-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n6221",
        "chosen": "This real-time 3D-aware method for portrait video relighting uses Neural Radiance Fields (NeRF) to synthesize talking faces under novel views and lighting conditions. It infers albedo and shading tri-planes with fast dual-encoders and ensures temporal consistency, achieving state-of-the-art results in reconstruction quality and inference speed.",
        "rejected": "A real-time 3D-aware method for relighting talking face videos based on Neural Radiance Fields (NeRF) is proposed. It synthesizes realistic talking faces under novel views and lighting conditions, ensuring smooth transitions and reducing flickering artifacts. Running at 32.98 fps, it achieves state-of-the-art results in reconstruction quality, temporal consistency, and speed."
    },
    {
        "prompt": "summary: While existing large vision-language multimodal mod-\nels focus on whole image understanding, there is a promi-\nnent gap in achieving region-specific comprehension. Cur-\nrent approaches that use textual coordinates or spatial en-\ncodings often fail to provide a user-friendly interface for\nvisual prompting.\nTo address this challenge, we intro-\nduce a novel multimodal model capable of decoding arbi-\ntrary (free-form) visual prompts. This allows users to intu-\nitively mark images and interact with the model using nat-\nural cues like a “red bounding box” or “pointed arrow”.\nOur simple design directly overlays visual markers onto the\nRGB image, eliminating the need for complex region encod-\nings, yet achieves state-of-the-art performance on region-\nunderstanding tasks like Visual7W, PointQA, and Visual\nCommonsense Reasoning benchmark.\nFurthermore, we\npresent ViP-Bench, a comprehensive benchmark to assess\nthe capability of models in understanding visual prompts\nacross multiple dimensions, enabling future research in this\ndomain. Code, data, and model are publicly available.\n1. Introduction\nLarge language models (LLMs) like ChatGPT [21],\nGPT4 [22], and Bard [9] have recently gained significant\nattention for their strong reasoning and generalization capa-\nbilities, and their ability to chat in a human-like manner.\nIn particular, models such as GPT-4V(ision) [20], which\nincorporate visual information, have demonstrated human-\nlevel perception and reasoning capabilities [36]. This has\nspurred the development of similar open-source models that\naim to replicate or even surpass the proprietary models’ per-\nformance.\nDespite their capabilities, current models, including\nseminal ones like LLaVA [14, 15] and MiniGPT-4 [42], fo-\ncus predominantly on whole-image understanding; in other\nwords, they lack the capability to process region-specific in-\nformation in complex scenes. This limitation becomes par-\nVisual Prompt\nText Prompt\n: What is the \nperson marked \nwith the red arrow \nholding?\nLarge Multimodal Model\n: The person marked with the red arrow is holding a \ngreen flag. This flag is used for …\nFigure 1. Main Idea of ViP-LLaVA. We directly overlay diverse\nvisual prompts (e.g., arrows, boxes, circles, scribbles) onto the\noriginal image, and then feed the corresponding visual features\nalong with text embeddings into the large multimodal model for\nconversational assistance. Here we show an example using a red\narrow.\nticularly apparent when attempting to describe specific ob-\njects within an image using only language prompts, which\ncan be difficult when there is ambiguity (e.g., when there\nare multiple people in the image, and the question relates to\na specific person), as shown in Figure 1.\nTo address this issue, recent work explores spatial ref-\nerences in multimodal models. Existing efforts have pri-\nmarily focused on using textual representations of coordi-\nnates [3, 4, 7, 39], learned positional embeddings [23, 38,\n41], or ROI features [26, 38]. However, they often lack user-\nfriendliness, as they are limited to fixed-format visual ref-\nerences like bounding boxes and the spatial coordinates of\na mask contour. Most of these approaches, including those\nby Zhang et al. [38] and Chen et al. [4], only employ bound-\ning box inputs for visual referrals. While effective in struc-\ntured scenarios, this method proves less versatile in natural,\nuser-driven interactions where the visual prompts may not\nconform to clean geometric shapes.\nIn this paper, we propose a simple yet highly effective\nsolution to this problem: a large multimodal model that can\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n12914",
        "chosen": "ViP-LLaVA introduces a multimodal model capable of decoding arbitrary visual prompts, allowing users to interact with the model using natural cues like arrows or bounding boxes. This approach achieves state-of-the-art performance on region-understanding tasks and includes ViP-Bench, a benchmark for assessing visual prompt understanding.",
        "rejected": "This paper introduces a novel multimodal model capable of decoding free-form visual prompts, enabling intuitive image interaction using natural cues like \"red bounding boxes\" or \"pointed arrows.\" By overlaying visual markers directly onto RGB images, the method simplifies user interaction without complex region encodings. It achieves state-of-the-art performance on region-understanding tasks such as Visual7W, PointQA, and Visual Commonsense Reasoning benchmarks. Additionally, the authors present ViP-Bench, a benchmark for evaluating models' capabilities in understanding visual prompts across multiple dimensions. This facilitates future research and advancements in user-friendly visual prompting techniques."
    },
    {
        "prompt": "summary: Neural Radiance Fields (NeRFs) have shown great po-\ntential in novel view synthesis. However, they struggle to\nrender sharp images when the data used for training is af-\nfected by motion blur. On the other hand, event cameras ex-\ncel in dynamic scenes as they measure brightness changes\nwith microsecond resolution and are thus only marginally\naffected by blur. Recent methods attempt to enhance NeRF\nreconstructions under camera motion by fusing frames and\nevents. However, they face challenges in recovering accu-\nrate color content or constrain the NeRF to a set of pre-\ndefined camera poses, harming reconstruction quality in\nchallenging conditions. This paper proposes a novel for-\nmulation addressing these issues by leveraging both model-\nand learning-based modules. We explicitly model the blur\nformation process, exploiting the event double integral as\nan additional model-based prior. Additionally, we model\nthe event-pixel response using an end-to-end learnable re-\nsponse function, allowing our method to adapt to non-\nidealities in the real event-camera sensor.\nWe show, on\nsynthetic and real data, that the proposed approach outper-\nforms existing deblur NeRFs that use only frames as well\nas those that combine frames and events by +6.13dB and\n+2.48dB, respectively.\nMultimedial Material: For videos, datasets and more visit\nhttps://github.com/uzh-rpg/evdeblurnerf.\n1. Introduction\nNeural Radiance Fields (NeRFs) [27] have completely rev-\nolutionized the field of 3D reconstruction and novel view\nsynthesis, achieving unprecedented levels of details [2, 3,\n43]. As a result, they have quickly found applications in\nmany subfields of computer vision and robotics, such as\npose estimation and navigation [36, 53, 59], image process-\ning [12, 24, 28, 47], scene understanding [17, 22, 51], sur-\nface reconstruction [1, 48, 54], and many others.\nLeveraging multi-view consistency from calibrated im-\nages, NeRF exploits supervision from multiple view-points,\nenabling generalization to novel camera poses and the abil-\nity to render view-dependent color effects [43]. However,\nMotion-aware\nNeRF\nevent\nCRF\nEv-DeblurNeRF\nBlurry Images\nEvents\nReconstructions\nFigure 1. Ev-DeblurNeRF combines blurry images and events to\nrecover sharp NeRFs. A motion-aware NeRF recovers camera mo-\ntion and a learnable event camera response function models real\ncamera’s non-idealities, enabling high-quality reconstructions.\nakin to other methods relying on photometric consistency,\nNeRF can only deliver high-quality reconstructions when\nthe images used for training are perfectly captured and free\nfrom any artifact. Unfortunately, perfect conditions are sel-\ndom met in the real world.\nFor example, in robotics, camera motion is prevalent\nwhen capturing images, often resulting in motion blur. Un-\nder such conditions, NeRFs are unable to reconstruct sharp\nradiance fields, thereby impeding their practical use in real-\nworld scenes. Although recent works [6, 18, 24, 49] have\nshown promising results in reconstructing radiance fields\nfrom motion-blurred images by learning to infer the cam-\nera motion during the exposure time, the task of recover-\ning motion-deblurred NeRFs still remains significantly ill-\nposed. Existing image-based approaches typically fail when\ntraining images exhibit similar and consistent motion [24],\nand they are inherently limited by the presence of motion\nambiguities and loss of texture details that cannot be recov-\nered from blurry images alone.\nIn this regard, recent works have shown that event-based\ncameras can substantially aid the task of deblurring images\ncaptured with standard cameras [33, 37, 45, 56].\nThese\nsensors measure brightness changes at microseconds reso-\nlution and are practically unaffected by motion blur [11],\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n9286",
        "chosen": "Ev-DeblurNeRF addresses motion blur in NeRF reconstructions by fusing frames and events. It models the blur formation process using event double integral as a prior and employs a learnable event-pixel response function. This method outperforms existing deblur NeRFs, achieving significant improvements on synthetic and real data.",
        "rejected": "This paper addresses the issue of motion blur in Neural Radiance Fields (NeRFs) by leveraging both model- and learning-based modules, explicitly modeling the blur formation process using event cameras. The proposed method outperforms existing techniques by significant margins."
    },
    {
        "prompt": "summary: Radiologists highly desire fully automated versatile AI\nfor medical imaging interpretation. However, the lack of ex-\ntensively annotated large-scale multi-disease datasets has\nhindered the achievement of this goal. In this paper, we\nexplore the feasibility of leveraging language as a natu-\nrally high-quality supervision for chest CT imaging. In light\nof the limited availability of image-report pairs, we boot-\nstrap the understanding of 3D chest CT images by distill-\ning chest-related diagnostic knowledge from an extensively\npre-trained 2D X-ray expert model. Speciﬁcally, we pro-\npose a language-guided retrieval method to match each 3D\nCT image with its semantically closest 2D X-ray image, and\nperform pair-wise and semantic relation knowledge distil-\nlation. Subsequently, we use contrastive learning to align\nimages and reports within the same patient while distin-\nguishing them from the other patients. However, the chal-\nlenge arises when patients have similar semantic diagnoses,\nsuch as healthy patients, potentially confusing if treated as\nnegatives. We introduce a robust contrastive learning that\nidentiﬁes and corrects these false negatives. We train our\nmodel with over 12K pairs of chest CT images and radi-\nology reports. Extensive experiments across multiple sce-\nnarios, including zero-shot learning, report generation, and\nﬁne-tuning processes, demonstrate the model’s feasibility in\ninterpreting chest CT images.\n1. Introduction\nUnderstanding medical images is crucial for precise clinical\ndiagnosis, with doctors routinely engaging in multi-disease\n*Correspondence to Jianpeng Zhang and Jian Zheng. The work was\ndone during Weiwei Cao’s internship at DAMO Academy.\nModel\nSupervision\nFunctionality\nSingle-disease\nNodule\nEmphysema\nInflammation\nCalcification\n…\nOurs\nNodule\nEmphysema\nInflammation\nCalcification\n…\nNodule\nEmphysema\nInflammation\nCalcification\n…\n+\nMulti-disease\nFigure 1. Models tailored for speciﬁc diseases demand doctors\nto annotate each image. Creating a multi-disease model involves\nmore time and effort for comprehensive data annotation. In con-\ntrast, our model learns to diagnose various diseases from both im-\nages and reports, eliminating the need for additional annotations.\ndetection and diagnosis in imaging scans. Deep learning\nmodels require extensive datasets with fully annotated dis-\neases for comprehensive training [19, 24, 28]. In practice,\nthe prevailing approach to developing a diagnostic model\ntypically concentrates on a speciﬁc disease, as illustrated in\nFigure 1. This workﬂow results in models with limited us-\nage, failing to be applied to other diseases. To augment the\ncapability for a new disease, a signiﬁcant additional effort\nis needed for data collection, annotation, and model train-\ning, which is not scalable, as there might be hundreds of\ndiseases and tasks. Therefore, there is a strong anticipation\nfor a new model development paradigm beyond the current\nlabor-intensive annotation and task-speciﬁc training. Imag-\ning diagnostic reports, the core output of radiologists’ in-\ntellectual work, inherently offer disease-level labels. The\nchallenge lies in effectively leveraging this text data to en-\nhance medical image understanding.\nRecent advances in vision-language pretraining (VLP)\nhave yielded valuable insights [2, 11, 20, 22, 27]. Con-\ntrastive learning, a powerful technique in VLP, effectively\nbridges the image and text modalities. This method uniﬁes\n1\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n11238",
        "chosen": "This paper explores using language as supervision for chest CT imaging by leveraging knowledge from a 2D X-ray expert model. A language-guided retrieval method matches CT images with semantically closest X-ray images, performing knowledge distillation and contrastive learning. The model is trained on over 12K pairs of chest CT images and radiology reports, demonstrating feasibility in interpreting chest CT images across multiple scenarios.",
        "rejected": "This work explores using language as supervision for interpreting chest CT images, distilling knowledge from 2D X-ray images via contrastive learning to understand 3D chest CT scans without extensive annotations. It demonstrates feasibility across various scenarios."
    },
    {
        "prompt": "summary: Diffusion model-based image restoration (IR) aims to use\ndiffusion models to recover high-quality (HQ) images from\ndegraded images, achieving promising performance. Due\nto the inherent property of diffusion models, most existing\nmethods need long serial sampling chains to restore HQ\nimages step-by-step, resulting in expensive sampling time\nand high computation costs. Moreover, such long sampling\nchains hinder understanding the relationship between inputs\nand restoration results since it is hard to compute the gra-\ndients in the whole chains. In this work, we aim to rethink\nthe diffusion model-based IR models through a different per-\nspective, i.e., a deep equilibrium (DEQ) fixed point system,\ncalled DeqIR. Specifically, we derive an analytical solution\nby modeling the entire sampling chain in these IR models as\na joint multivariate fixed point system. Based on the analyti-\ncal solution, we can conduct parallel sampling and restore\nHQ images without training. Furthermore, we compute fast\ngradients via DEQ inversion and found that initialization\noptimization can boost image quality and control the gen-\neration direction. Extensive experiments on benchmarks\ndemonstrate the effectiveness of our method on typical IR\ntasks and real-world settings.\n1. Introduction\nImage restoration (IR) aims at recovering a high-quality\n(HQ) image from a degraded input. Recently, diffusion\nmodels [37, 61] are attracting great attention because they\ncan generate higher quality images than GANs [23] and\nlikelihood-based models [44]. Based on diffusion models\n[37, 61], many IR methods [20, 41, 68] achieve compelling\nperformance on different tasks. Directly using diffusion\nmodels in IR, however, suffers from some limitations.\nFirst, diffusion model-based image restoration (DMIR)\nmodels rely on a long sampling chain to synthesize HQ im-\nages step-by-step, as shown in Figure 2 (a). As a result,\nit will lead to expensive sampling time during the infer-\n*Corresponding author.\nDPS\nDDRM DDNM\nOurs\n(a) 4x SR (ImageNet)\n19\n22\n25\n28\nPSNR\n0.2\n0.3\n0.4\n0.5\nLPIPS\nDPS\nDDRM DDNM\nOurs\n(b) Deblurring (ImageNet)\n20\n25\n30\n35\n40\nPSNR\n0.0\n0.2\n0.4\nLPIPS\nDDRM PaletteRePaint Ours\n(c) Inpainting (CelebA)\n37\n38\n39\n40\nPSNR\n0.0\n0.1\n0.2\n0.3\nLPIPS\nDGP\nDDRM DDNM\nOurs\n(d) Colorization (CelebA)\n40\n50\n60\n70\n80\nFID\n0.1\n0.2\n0.3\nLPIPS\nFigure 1. Comparisons of different zero-shot DMIR methods in\nvarious IR applications on different datasets.\nence. For example, DPS [20] based on DDPM [37] needs\n1k sampling steps. To accelerate the sampling, some DMIR\nmethods [41, 68, 87] use DDIM [61] to make a trade-off be-\ntween computational cost and the restoration quality. Based\non this, these methods can reduce sampling steps to 100\nor even fewer. Unfortunately, it may degrade the sample\nquality when reducing the sampling steps [52]. It raises an\ninteresting question: is it possible to develop an alternative\nsampling method without sacrificing the sample quality?\nSecond, the long sampling chain makes understanding the\nrelationship between the restoration and inputs difficult. In\npractice, sampling different Gaussian noises as inputs may\nhave diverse results for some IR tasks (e.g., inpainting and\ncolorization). Such diversity is not necessary for some IR\ntasks, e.g., super-resolution (SR) or deblurring. Nevertheless,\ndifferent initializations may affect the quality of SR and\ndeblurring. It raises the second question: is it possible to\noptimize the initialization such that the generation can be\nimproved or controlled? However, it is difficult for existing\nmethods to compute the gradient along the long sampling\nchain as they require storing the entire computational graph.\nIn this paper, we rethink the sampling process in IR from a\ndeep equilibrium (DEQ) based on [57]. Specifically, we first\nderive a proposition to model the sampling chain as a fixed\npoint system, achieving parallel sampling. Then, we use a\nDEQ solver to find the fixed point of the sampling chain.\nLast, we use modern automatic differentiation packages to\ncompute the gradients with backpropagating and understand\nthe relationship between input noise and restoration.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n2824",
        "chosen": "DeqIR rethinks diffusion model-based image restoration through a deep equilibrium fixed point system. It models the sampling chain as a joint multivariate fixed point system, enabling parallel sampling and restoring high-quality images without training. The method computes fast gradients via DEQ inversion and optimizes initialization, showing effectiveness on various IR tasks.",
        "rejected": "DeqIR is introduced as a deep equilibrium fixed point system for diffusion model-based image restoration. It enables parallel sampling and restores high-quality images efficiently, showing better performance than traditional methods on multiple image restoration tasks."
    },
    {
        "prompt": "summary: We present DreamAvatar, a text-and-shape guided\nframework for generating high-quality 3D human avatars\nwith controllable poses. While encouraging results have\nbeen reported by recent methods on text-guided 3D common\nobject generation, generating high-quality human avatars\nremains an open challenge due to the complexity of the\nhuman body’s shape, pose, and appearance. We propose\nDreamAvatar to tackle this challenge, which utilizes a train-\nable NeRF for predicting density and color for 3D points\nand pretrained text-to-image diffusion models for providing\n2D self-supervision. Specifically, we leverage the SMPL\nmodel to provide shape and pose guidance for the gener-\nation. We introduce a dual-observation-space design that\ninvolves the joint optimization of a canonical space and\na posed space that are related by a learnable deforma-\ntion field.\nThis facilitates the generation of more com-\nplete textures and geometry faithful to the target pose. We\nalso jointly optimize the losses computed from the full body\nand from the zoomed-in 3D head to alleviate the common\nmulti-face “Janus” problem and improve facial details in\nthe generated avatars. Extensive evaluations demonstrate\nthat DreamAvatar significantly outperforms existing meth-\nods, establishing a new state-of-the-art for text-and-shape\nguided 3D human avatar generation.\n1. Introduction\nThe creation of 3D graphical human models has received\ngreat attention in recent years due to its wide-ranging appli-\ncations in fields such as film-making, video games, AR/VR,\nand human-robotic interaction.\nTraditional methods for\nbuilding such complex 3D models require thousands of\nman-hours of trained artists and engineers [10, 12], mak-\ning the process both time-consuming and highly expert-\n*Equal contribution\n†Corresponding authors\n‡Webpage: https:\n//yukangcao.github.io/DreamAvatar/\nAcknowlegement: This work is partially supported by Hong Kong Re-\nsearch Grant Council - Early Career Scheme (Grant No. 27208022) and\nHKU Seed Fund for Basic Research.\ndependent. With the development of deep learning meth-\nods, we have witnessed the emergence of promising meth-\nods [5, 54, 61] which can reconstruct 3D human mod-\nels from monocular images. These techniques, however,\nstill face challenges in fully recovering details from the in-\nput images and rely heavily on the training dataset.\nTo\ntackle these challenges and simplify the modeling process,\nadopting generative models for 3D human avatar modeling\nhas recently received increasing attention from the research\ncommunity. This approach has the potential to alleviate the\nneed for large 3D datasets and facilitate easier and more ac-\ncessible 3D human avatar modeling.\nTo leverage the potential of 2D generative image models\nfor 3D content generation, recent methods [8, 29, 31, 35, 46]\nhave utilized pretrained text-guided image diffusion models\nto optimize 3D implicit representations (e.g., NeRFs [37]\nand DMTet [40, 56]).\nDreamFusion [46] introduces a\nnovel Score Distillation Sampling (SDS) strategy to self-\nsupervise the optimization process and achieves promising\nresults. However, human bodies, which are the primary fo-\ncus of this paper, exhibit a complex articulated structure,\nwith head, arms, hands, trunk, legs, feet, etc., each capa-\nble of posing in various ways. As a result, while Dream-\nFusion [46] and subsequent methods (e.g., Magic3D [29],\nProlificDreamer [58], Fantasia3D [8]) produce impressive\nresults, they lack the proper constraints to enforce consis-\ntent 3D human structure and often struggle to generate de-\ntailed textures for 3D human avatars. Latent-NeRF [35] in-\ntroduces a sketch-shape loss based on the 3D shape guid-\nance, but it still faces challenges in generating reasonable\nresults for human bodies.\nIn this paper, we present DreamAvatar, a novel frame-\nwork for generating high-quality 3D human avatars from\ntext prompts and shape priors.\nInspired by previous\nworks [29, 46], DreamAvatar employs a trainable NeRF\nas the base representation for predicting density and color\nvalues for each 3D point. Coupled with pretrained text-\nto-image diffusion models [50, 68], DreamAvatar can be\ntrained to generate 3D avatars using 2D self-supervision.\nThe key innovation of DreamAvatar lies in three main as-\npects. Firstly, we leverage the SMPL model [32] to provide\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n958",
        "chosen": "DreamAvatar is a text-and-shape guided framework for generating high-quality 3D human avatars with controllable poses. It uses a trainable NeRF and pretrained text-to-image diffusion models, leveraging the SMPL model for shape and pose guidance. The dual-observation-space design and joint optimization of losses improve texture completeness and facial details, outperforming existing methods.",
        "rejected": "DreamAvatar generates high-quality 3D human avatars guided by text and shape priors, utilizing trainable NeRF and pretrained text-to-image diffusion models. It achieves complete textures and geometry faithful to target poses, significantly improving facial details."
    },
    {
        "prompt": "summary: We introduce Motion2VecSets, a 4D diffusion model for\ndynamic surface reconstruction from point cloud sequences.\nWhile existing state-of-the-art methods have demonstrated\nsuccess in reconstructing non-rigid objects using neural\nfield representations, conventional feed-forward networks\nencounter challenges with ambiguous observations from\nnoisy, partial, or sparse point clouds.\nTo address these\nchallenges, we introduce a diffusion model that explicitly\nlearns the shape and motion distribution of non-rigid ob-\njects through an iterative denoising process of compressed\nlatent representations. The diffusion-based priors enable\nmore plausible and probabilistic reconstructions when han-\ndling ambiguous inputs.\nWe parameterize 4D dynamics\nwith latent sets instead of using global latent codes. This\nnovel 4D representation allows us to learn local shape and\ndeformation patterns, leading to more accurate non-linear\nmotion capture and significantly improving generalizabil-\nity to unseen motions and identities. For more temporally-\ncoherent object tracking, we synchronously denoise defor-\nmation latent sets and exchange information across multi-\nple frames. To avoid computational overhead, we designed\n*Equal Contribution.\n†Corresponding author.\n‡Work done during master’s thesis.\nan interleaved space and time attention block to alternately\naggregate deformation latents along spatial and temporal\ndomains.\nExtensive comparisons against state-of-the-art\nmethods demonstrate the superiority of our Motion2VecSets\nin 4D reconstruction from various imperfect observations.\n1. Introduction\nOur world, dynamic in its 4D nature, demands an increas-\ningly sophisticated understanding and simulation of our\nliving environment.\nThis offers significant potential for\npractical applications, including Virtual Reality (VR), Aug-\nmented Reality (AR), and robotic simulations. There have\nbeen notable advances in 3D object modeling, particularly\nin representations through parametric models [27, 30, 36,\n45, 67]. Unfortunately, these template-based models are\nnot effectively suited to capture the 4D dynamics of gen-\neral non-rigid objects, due to the assumption of a fixed tem-\nplate mesh. Model-free approaches [25, 32, 52] represent\na significant advance by using coordinate-MLP represen-\ntations for deformable object reconstruction with arbitrary\ntopologies and non-unified structures. However, these state-\nof-the-art methods still encounter challenges when facing\nambiguous observations of noisy, sparse, or partial point\nclouds since it is an ill-posed problem where multiple pos-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n20496",
        "chosen": "Motion2VecSets is a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. It learns shape and motion distribution through an iterative denoising process of compressed latent representations, parameterized with latent sets for local shape and deformation patterns. This approach improves generalizability to unseen motions and identities, demonstrating superiority in 4D reconstruction from imperfect observations.",
        "rejected": "Motion2VecSets, a 4D diffusion model, learns shape and motion distribution through iterative denoising processes. By parameterizing dynamics with latent sets, it leads to accurate reconstructions and improved generalizability to unseen motions and identities."
    },
    {
        "prompt": "summary: Generalized Category Discovery (GCD) aims to identify\na mix of known and novel categories within unlabeled data\nsets, providing a more realistic setting for image recogni-\ntion. Essentially, GCD needs to remember existing pat-\nterns thoroughly to recognize novel categories.\nRecent\nstate-of-the-art method SimGCD transfers the knowledge\nfrom known-class data to the learning of novel classes\nthrough debiased learning.\nHowever, some patterns are\ncatastrophically forgot during adaptation and thus lead to\npoor performance in novel categories classiﬁcation. To ad-\ndress this issue, we propose a novel learning approach, Le-\ngoGCD, which is seamlessly integrated into previous meth-\nods to enhance the discrimination of novel classes while\nmaintaining performance on previously encountered known\nclasses.\nSpeciﬁcally, we design two types of techniques\ntermed as Local Entropy Regularization (LER) and Dual-\nviews Kullback–Leibler divergence constraint (DKL). The\nLER optimizes the distribution of potential known class\nsamples in unlabeled data, thus ensuring the preservation of\nknowledge related to known categories while learning novel\nclasses. Meanwhile, DKL introduces Kullback–Leibler di-\nvergence to encourage the model to produce a similar pre-\ndiction distribution of two view samples from the same im-\nage. In this way, it successfully avoids mismatched predic-\ntion and generates more reliable potential known class sam-\nples simultaneously. Extensive experiments validate that the\nproposed LegoGCD effectively addresses the known cate-\ngory forgetting issue across all datasets, e.g., delivering\na 7.74% and 2.51% accuracy boost on known and novel\nclasses in CUB, respectively.\nOur code is available at:\nhttps://github.com/Cliffia123/LegoGCD.\n*Equal Contribution.\n†Joint Corresponding Authors.\n64.44%\n57.82%\nCatastrophic\nforgetting\n(a) Baseline SimGCD [39]\n72.18% \n(↑7.74)\n60.33%\n(↑2.51)\n(b) Ours LegoGCD\nFigure 1. Visualization of the accuracy results in unlabeled dataset\non CUB dataset [37] during training. (a) shows a decrease in the\naccuracy of known (Old) classes (green) in the baseline as the ac-\ncuracy of novel (New) classes (orange) increases. (b) demonstrates\nthat LegoGCD solves the catastrophic forgetting problem and sur-\npasses the baseline by a signiﬁcant margin of 7.74.\n1. Introduction\nDeep learning have achieved superior performance on com-\nputer vision tasks [4, 11, 24, 25, 30, 34], particularly on\nimage classiﬁcation [10, 12, 27, 28, 51]. However, con-\nventional methods work in a close-world setting, where\nall training data comes with pre-deﬁned classes.\nCon-\nsequently, deploying these models in real-world scenar-\nios with potential novel classes becomes a considerable\nchallenge.\nFurthermore, these achievements rely heav-\nily on large-scale annotated dataset, which is not easily\naccessible in realistic scenarios.\nTo address these chal-\nlenges, a new paradigm of Generalized Category Discov-\nery (GCD) [1, 7, 9, 23, 36, 39, 45, 46] has been proposed\nand attracts increasing attention in recent years.\nThe goal of GCD is to train a classiﬁcation model capa-\nble of recognizing both known and novel categories within\nunlabeled data. To be clear, GCD distinguishes itself from\nthe Novel Class Discovery (NCD) [8], which relies on an\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n16880",
        "chosen": "LegoGCD addresses catastrophic forgetting in Generalized Category Discovery by integrating Local Entropy Regularization and Dual-views Kullback-Leibler divergence constraint. These techniques optimize the distribution of known class samples and encourage similar prediction distributions from different views, effectively solving the forgetting issue and improving accuracy on both known and novel classes.",
        "rejected": "LegoGCD introduces Local Entropy Regularization (LER) and Dual-views Kullback–Leibler divergence constraint (DKL) to address catastrophic forgetting in generalized category discovery. These techniques enhance novel class discrimination while preserving known class performance, boosting accuracy."
    },
    {
        "prompt": "summary: We present SuperNormal, a fast, high-ﬁdelity approach\nto multi-view 3D reconstruction using surface normal maps.\nWith a few minutes, SuperNormal produces detailed sur-\nfaces on par with 3D scanners. We harness volume ren-\ndering to optimize a neural signed distance function (SDF)\npowered by multi-resolution hash encoding. To accelerate\ntraining, we propose directional ﬁnite difference and patch-\nbased ray marching to approximate the SDF gradients nu-\nmerically. While not compromising reconstruction quality,\nthis strategy is nearly twice as efﬁcient as analytical gra-\ndients and about three times faster than axis-aligned ﬁnite\ndifference. Experiments on the benchmark dataset demon-\nstrate the superiority of SuperNormal in efﬁciency and ac-\ncuracy compared to existing multi-view photometric stereo\nmethods. On our captured objects, SuperNormal produces\nmore ﬁne-grained geometry than recent neural 3D recon-\nstruction methods. Our code is available at https://\ngithub.com/CyberAgentAILab/SuperNormal.\n1. Introduction\nRecovering high-quality 3D geometry of real-world ob-\njects from their multi-view images is a long-standing chal-\nlenge in computer vision. Recently, neural implicit surface-\nbased methods have shown remarkable reconstruction re-\nsults.\nCompared to traditional multi-view stereo (MVS)\nmethods [25], neural methods are more robust to view-\ndependent observations and textureless surfaces [29]. Fur-\nthermore, the reconstruction procedure has become highly\nefﬁcient [30] by introducing multi-resolution hash cod-\ning [20]. However, even though this spatial encoding al-\nlows ﬁne-grained geometry to be represented, shape recon-\nstruction from multi-view images tends to smooth out high-\nfrequency surface details, as shown in Fig. 1 bottom right.\nMulti-view photometric stereo (MVPS), on the other\nhand, aims to recover pixel-level high-frequency surface\ndetail by introducing additional lighting conditions during\nMulti-view posed normal maps\nOurs (51 secs)\nPS-NeRF (8 hrs)\nScanner [Rexcan CS+]\nOurs (5 mins)\nNeuS2 (5 mins)\n~100 images\n18 normal maps\nScanner \n[EinScan SE]\nFigure 1.\n(Top) From multi-view surface normal maps,\nSuperNormal recovers ﬁne-grained surface details comparable to\n3D scanners while being orders of magnitude faster than the exist-\ning MVPS method PS-NeRF [31]. (Bottom) Using normal maps\nproduces more faithful high-frequency details than the MVS-\nbased method NeuS2 [30], although both use multi-resolution hash\nencoding [20].\nthe image acquisition [9].\nIn a typical workﬂow, a sur-\nface normal map is ﬁrst recovered for each view, record-\ning per-pixel surface orientation information. The normal\nmaps estimated at different viewpoints are then fused into a\n3D model, also known as multi-view normal integration [5].\nHowever, existing normal fusion methods struggle to reﬂect\nthe details of the normal maps in the shape of the recov-\nered 3D model. Moreover, the reconstruction process takes\nhours even when only a few low-resolution normal maps are\nused [4, 31]. Due to the lack of a fast and accurate multi-\nview normal fusion method, current multi-view photometric\nstereo results remain unsatisfactory.\nThis paper presents SuperNormal to unite the best of\nboth worlds. Normal maps with pixel-level surface details\nare utilized to exploit the expressive power and efﬁciency\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n20581",
        "chosen": "SuperNormal is a fast, high-fidelity approach to multi-view 3D reconstruction using surface normal maps. It optimizes a neural signed distance function powered by multi-resolution hash encoding, using directional finite difference and patch-based ray marching. This method is more efficient and accurate than existing MVPS methods, producing detailed surfaces comparable to 3D scanners.",
        "rejected": "SuperNormal presents a fast, high-fidelity approach to multi-view 3D reconstruction using surface normal maps. It optimizes a neural signed distance function with multi-resolution hash encoding, producing detailed surfaces comparable to 3D scanners efficiently."
    },
    {
        "prompt": "summary: High-resolution wide-angle fisheye images are becom-\ning more and more important for robotics applications\nsuch as autonomous driving.\nHowever, using ordinary\nconvolutional neural networks or vision transformers on\nthis data is problematic due to projection and distor-\ntion losses introduced when projecting to a rectangular\ngrid on the plane. We introduce the HEAL-SWIN trans-\nformer, which combines the highly uniform Hierarchi-\ncal Equal Area iso-Latitude Pixelation (HEALPix) grid\nused in astrophysics and cosmology with the Hierarchical\nShifted-Window (SWIN) transformer to yield an efficient\nand flexible model capable of training on high-resolution,\ndistortion-free spherical data. In HEAL-SWIN, the nested\nstructure of the HEALPix grid is used to perform the patch-\ning and windowing operations of the SWIN transformer, en-\nabling the network to process spherical representations with\nminimal computational overhead. We demonstrate the su-\nperior performance of our model on both synthetic and real\nautomotive datasets, as well as a selection of other image\ndatasets, for semantic segmentation, depth regression and\nclassification tasks. Our code is publicly available1.\n1. Introduction\nHigh-resolution fisheye cameras are among the most com-\nmon and important sensors in modern intelligent vehi-\ncles [42]. Due to their non-rectilinear mapping functions\nand large field of view, fisheye images are highly distorted.\nMoreover, the most commonly used large-scale computer\n∗Equal contribution\naDepartment of Mathematical Sciences, Chalmers University of Tech-\nnology, University of Gothenburg, SE-412 96 Gothenburg, Sweden\nbCorresponding author, email: osccarls@chalmers.se\ncNeural Information Processing, Science of Intelligence, Technical\nUniversity Berlin, DE-10623 Berlin, Germany\ndDepartment of Mathematics and Mathematical Statistics, Ume˚a Uni-\nversity, SE-901 87 Ume˚a, Sweden\neZenseact, SE-417 56 Gothenburg, Sweden\n1https://github.com/JanEGerken/HEAL-SWIN\n<latexit sha1_base64=\"cQJewpwtZc\npJ6NEe50L9/ReM0bc=\">ADT3ichVJNTxRBEH07q8LiF8jFhMvEjcbTZsYoeCRB\nDBcTFzADEzQ+/S2fnKdC8GNvsruMKP4ugv4WIMr8vGRFHpTW9Vv6r3qrqm0zrX\nxkbRt1bQvnP3sxsZ+7+g4ePHs8vPNky1bjJVD+r8qrZSROjcl2qvtU2Vzt1o5I\nizdV2Olpz8e0j1RhdlZ/sca32i2RY6oHOEkvo857VhTJhvPxlvhv1IlnhTSf2Thd\n+bVYLrafYwEqZBijgEIJSz9HAsPfLmJEqIntY0KsoaclrjDFHLljZilmJERH/B/\nytOvRkmenaYSdsUrO3ZAZ4jn3e1FMme2qKvqG9jv3iWDf1aYiLr8Jg2pWJHFD\n8Qtzhkxm3Mwmde93I7093KYoC3chvN/mpB3D2zXzrvGmIjSQSYl0yh9RI5XzECZ\nS0fXbgpnytEMqND2gTsUpUSq+YUK+hdN3/bgef8YVvspkC7mRY04kVojGQCrk5E\n2Ju+9Y+O/2P7Yh72/sQ3Y/5RuL/3xRN52tV714ufm4+vu6gv/2maxhGd4yRe1g\nlVsYJMzyFj3FGc4Dy6Cy+BH26cGLe8s4rfV7lwB56CrDA=</latexit>⇥16\n<latexit sha1_base64=\"H68/dxKI2U\nseIb13+4XGgtKcZA=\">ADTnichVLThtBECwveRhIwiMXpFxWKCcrDWCJEck\nSMQFCSQMlgCh3WVsRt6XdsZExPJPcE0+is/khOI1DRLpITXWOPuqe6q6entqEi0\nsUFwWfPGXrx89bo+PjH5u27qemZ2V2TD8pYteM8yctOFBqV6Ey1rbaJ6hSlCtM\noUXtRf83F905VaXSe7dizQh2mYS/TXR2HlDnwOpUGX/5aLoRNANZ/n2nVTkNVGs\nrn6nN4QDHyBFjgBQKGSz9BCEMf/toIUB7BDYiU9LXGFESbIHTBLMSMk2ud/j6f\n9Cs14dpG2DFvSbhLMn0scH8TxYjZ7lZF39Becf8QrPfoDUNRdhWe0UZUHBfFTe\nIWJ8x4jplWmXe1PM90r7Lo4ou8RrO+QhD3zvivzjojJbG+RHx8lcweNSI5n7IDGW\n2bFbgu3yn48uJj2lCsEpWsUgypV9K67rt6XI23cYXv0tlUXuSYQ4mlotGVGxLyRs\nTd0yr7/YU25D3EPuE1Y84Y63/J+q+s7vUbH1qrmwvN1YXq2mr4wPm8ZET9Rmr2\nMAWe+Am6Rw/8cu78H57197NbapXqzjv8c8aq/8BTCWrzg=</latexit>⇥4\n<latexit sha1_base64=\"WGRhOxYokf\nOXVAs1tZNL6VNKYck=\">ADd3ichVLbtNAEJ3EXFpzS+EFiQciIq+kNpFXB4r\ncREvSEUibaU4qmxn4q5ir631plEb5TP4Gl7hI/gUJB4O3WQoEA32szsmTlnZ8ab\nVLmqbRB8a7W9K1evXV9b92/cvHX7Tmfj7n5dzkzKg7TMS3OYxDXnSvPAKpvzYWU\n4LpKcD5LpKxc/OGFTq1J/tKcVj4o402qi0tgCOupsRwlnSi9S1pbN0p8rPS7nUTR\n8rSyI79WZ+xHrMerhKNOL+gHsroXnbBxetSsvXKjdZ8iGlNJKc2oICZNFn5OMdX\n4DSmkgCpgI1oAM/CUxJmW5IM7QxYjIwY6xX+G07BNc5OsxZ2iltybANmlx5jvx\nXFBNnuVoZfw/7APhMs+cNC1F2FZ7CJlBcF8X3wC0dI+MyZtFkrmq5nOm6sjShl9\nKNQn2VIK7P9JfOa0QMsKlEuvRGMjNoJHI+wQ07AVuCmvFLrS8Rg2FsuiohvFGH\noG1k3f1eNqPI8zWyhXTkmAuJFaIxkRty8JbA3Xcsmu/2P3YN3t/Yx6jevbHwz\nxd10dnf6YfP+8+7PR2N5vXtkYP6BFt4UW9oF16R3uYQUqf6DN9oa/t795Db9PbO\nk9txrOPfpteFP7li5vw=</latexit>window\nsize\n<latexit sha1_base64=\"Ft6bx/o8LJ\nGZzlKi7UM7FU4VCGA=\">ADT3ichVJdaxNBFD3ZaJtU+xH7IvQlGIQiGDaFfjxG\nbKWFKhXNh8RSdreTdMl+sbuJtCG/oq/6o3z0l/SlFM9cp4JG2wmTe+fce869c3fc\nJPCz3LZ/FKzig4dz86XywqPHi0vLK5Un7SwepZ5qeXEQp13XyVTgR6qV+3mgukm\nqnNANVMcdvtbxzlilmR9H/PzRB2HziDy+7n5IQ+7e+9Onz5oXPw7mSlZtdtWdV\nZp2GcGsw6iuFp/iMU8TwMEIhQg5/QAOMv56aMBGQuwYE2IpPV/iClMskDtilmK\nGQ3TI/wFPYNGPGvNTNgeqwTcKZlVPOd+I4ous3VRT+jvea+EGzw3woTUdYdnt\nO6VCyL4lviOc6YcR8zNJm3vdzP1LfK0ceO3MZnf4kg+p7eb51dRlJiQ4lUsSeZA2\nq4ch5zAhFtix3oKd8qVOXGp7SOWCUqkVF0qJfS6unrfnSPv+IKX2SyodxIMycSC0\nWjLxUC8qbE9XcMzXe7i52R9y/2Gbuf8o01/n5Rs057o97Yqm+36g1X5jXVsIan\nmGdL2obTezjiDPwWPcSX/HN+m5dWTdFk2oVjLOKP1ax/BN7FqR</latexit>HEAL-SWIN\n<latexit sha1_base64=\"G8ES7BJ6M+\neZ5PcIQIPR60J+4Wg=\">ADSnichVLJSgNBEH0Z9327CF6CQRAPYSK4HAUX9KAo\nGiOoyMzYSYbMxsxE0ZBf8Kof5Q/4G95ED74uR8G9Q6eqX9V7XVXTduS5SWqaDzmj\no7Oru6e3r39gcGh4ZHRs/DAJm7Gjyk7ohfGRbSXKcwNVTt3U0dRrCzf9lTFbqz\nqeOVCxYkbBgfpVaROfasWuFXsVIN7Ve2ds5GC2bRlJX/7pQyp4Bs7YZjuUmc4Bw\nhHDThQyFASt+DhYS/Y5RgIiJ2ihaxmJ4rcYU2+sltMksxwyLa4H+Np+MDXjWmom\nwHd7icdk5jHDvSGKNrP1rYp+QvCfS1Y7dcbWqKsK7yitanYJ4rbxFPUmfEf08\n8y32v5n6m7SlHFsnTjsr5IEN2n86GzxkhMrCGRPNYls0YNW84XnEBAW2YFesrvCn\nnp+JzWEqtEJcgULerFtHr6uh5d41tc4VIm60tHmtmSmC8aVbnBI69NXH9HP/tuf7\nET8n5i1l9m2+s9PVFfXcO54ulxeLC3nxhZS57b2YwjRm+aKWsIJN7HIGDjVvc\nIs74954NJ6M57dUI5dxJvBpdXS+AiRpqhc=</latexit>SWIN\nFigure 1. Our HEAL-SWIN model uses the nested structure of\nthe HEALPix grid to lift the windowed self-attention of the SWIN\nmodel onto the sphere.\nvision and autonomous driving datasets do not contain fish-\neye images. For these reasons, fisheye images have received\nmuch less attention than rectilinear images in the literature.\nDespite the distortions introduced by the mapping func-\ntion, the traditional approach for dealing with this kind of\ndata is to use standard (flat) convolutional neural networks\nwhich are adjusted to the distortions and either preprocess\nthe data [16, 17, 26, 36, 46, 50] or deform the convolu-\ntion kernels [47]. However, these approaches struggle to\ncapture the inherent spherical geometry of the images since\nthey operate on a flat approximation of the sphere. Errors\nand artifacts arising from handling the strong and spatially\ninhomogeneous distortions are particularly problematic in\nsafety-critical applications such as autonomous driving.\nUtilizing spherical representations is an approach taken\nby some models [8, 10, 19] which lift convolutions to the\nsphere. These models rely on a rectangular grid in spherical\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n6067",
        "chosen": "HEAL-SWIN is a transformer model combining the Hierarchical Equal Area iso-Latitude Pixelation grid with the Hierarchical Shifted-Window transformer. This enables efficient processing of high-resolution, distortion-free spherical data, demonstrating superior performance on automotive datasets for semantic segmentation, depth regression, and classification tasks.",
        "rejected": "HEAL-SWIN transformer combines the HEALPix grid used in astrophysics with SWIN transformer to handle high-resolution fisheye images effectively. It processes spherical representations with minimal computational overhead, showing superior performance in various computer vision tasks."
    },
    {
        "prompt": "summary: Advances in camera-based physiological monitoring\nhave enabled the robust, non-contact measurement of res-\npiration and the cardiac pulse, which are known to be in-\ndicative of the sleep stage. This has led to research into\ncamera-based sleep monitoring as a promising alterna-\ntive to “gold-standard” polysomnography, which is cum-\nbersome, expensive to administer, and hence unsuitable for\nlonger-term clinical studies. In this paper, we introduce\nSleepVST, a transformer model which enables state-of-the-\nart performance in camera-based sleep stage classifica-\ntion (sleep staging). After pre-training on contact sensor\ndata, SleepVST outperforms existing methods for cardio-\nrespiratory sleep staging on the SHHS and MESA datasets,\nachieving total Cohen’s kappa scores of 0.75 and 0.77 re-\nspectively. We then show that SleepVST can be successfully\ntransferred to cardio-respiratory waveforms extracted from\nvideo, enabling fully contact-free sleep staging. Using a\nvideo dataset of 50 nights, we achieve a total accuracy of\n78.8% and a Cohen’s κ of 0.71 in four-class video-based\nsleep staging, setting a new state-of-the-art in the domain.\n1. Introduction\nAccurate sleep monitoring is critical for the diagnosis of\nsleep disorders and the discovery of novel treatments and\nbiomarkers. Poor sleep is broadly linked with a number\nof health conditions, including cardiovascular diseases such\nas diabetes and hypertension [15]. Additionally, there are\nlinks between specific sleep stages and neurodegenerative\nconditions, such as a decrease in non-rapid-eye-movement\nstage 3 (N3) sleep with Alzheimer’s disease [23].\nOvernight video polysomnography (vPSG), the “gold-\nstandard” [52] technique for sleep monitoring, requires the\n*jcarter@robots.ox.ac.uk\nuse of a large number of contact sensors including elec-\ntrodes placed on the scalp (EEG), near the eyes (EOG),\nand under the chin (EMG) of the patient. Human experts\n(polysomnographers) must then review the recorded data,\nclassifying the patient’s sleep into five stages at 30-second\nintervals (epochs) and annotating other important events\nsuch as leg movements and apnoeas. This manual process\noften takes multiple hours to complete.\nExperts primarily rely on characteristic patterns in\nbrain activity measured from the EEG to classify sleep\nstages [20]. However, sleep stage information is also en-\ncoded in measures of autonomic activity, including car-\ndiac [48] and respiratory signals [19], and body move-\nments [68]. This has led to the development of methods\nfor automatically classifying sleep stages from sensors that\nmeasure these parameters, such as smartwatches [63].\nPrior work has shown that these physiological parame-\nters can also be measured using video cameras [18, 65, 66],\nleading to the development of methods for classifying sleep\nstages entirely from video input [6, 35, 60].\nCamera-\nbased methods have particular promise as part of a multi-\npurpose sleep monitoring system. For example, within el-\nderly care settings, in addition to classifying sleep stages,\nthey could also be used to detect specific sleep movement\ndisorders [18], which are linked with conditions such as\nParkinson’s, and which are typically distinguished via man-\nual video review [51]. They could also be used to detect\nfalls [12], one of the leading causes of injury and mortality\namongst elderly individuals.\nState-of-the-art video-based sleep staging [6] has used\nthe heart rate (HR) and breathing rate (BR) derived from\nvideo to perform sleep staging. However, these rates are\ntypically measured as averages over time, losing higher\nfrequency inter-pulse and inter-breath interval information.\nEEG- [40] and wearable-based [22] sleep staging methods\nhave both been improved by using raw waveforms as inputs,\nrather than derived quantities e.g. signal features [37, 45].\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n12479",
        "chosen": "SleepVST is a transformer model for camera-based sleep stage classification. Pre-trained on contact sensor data, it outperforms existing methods for cardiorespiratory sleep staging and can be transferred to video-based staging. Using a video dataset of 50 nights, it achieves state-of-the-art performance in contact-free sleep staging.",
        "rejected": "This paper proposes Ev-DeblurNeRF, which combines blurry images and events to recover sharp NeRFs. It leverages event double integral as an additional prior and models the event-pixel response, enabling high-quality reconstructions under challenging conditions."
    },
    {
        "prompt": "summary: The landscape of deep learning research is moving to-\nwards innovative strategies to harness the true potential of\ndata. Traditionally, emphasis has been on scaling model\narchitectures, resulting in large and complex neural net-\nworks, which can be difficult to train with limited compu-\ntational resources. However, independently of the model\nsize, data quality (i.e. amount and variability) is still a ma-\njor factor that affects model generalization. In this work, we\npropose a novel technique to exploit available data through\nthe use of automatic data augmentation for the tasks of im-\nage classification and semantic segmentation.\nWe intro-\nduce the first Differentiable Augmentation Search method\n(DAS) to generate variations of images that can be pro-\ncessed as videos. Compared to previous approaches, DAS\nis extremely fast and flexible, allowing the search on very\nlarge search spaces in less than a GPU day. Our intuition\nis that the increased receptive field in the temporal dimen-\nsion provided by DAS could lead to benefits also to the spa-\ntial receptive field. More specifically, we leverage DAS to\nguide the reshaping of the spatial receptive field by selecting\ntask-dependant transformations. As a result, compared to\nstandard augmentation alternatives, we improve in terms of\naccuracy on ImageNet, Cifar10, Cifar100, Tiny-ImageNet,\nPascal-VOC-2012 and CityScapes datasets when plugging-\nin our DAS over different light-weight video backbones.\n1. Introduction\nCreating models with significantly increased capacity, in an\nattempt to achieve incremental performance improvements,\nhas been the prevailing approach in designing Convolu-\ntional Neural Network (CNN) classifiers. As a result, CNNs\nwith increased depth [16, 40, 45, 60], and Vision Trans-\nformers (ViTs) [13] were proposed over the years. Specif-\nically, ViT has demonstrated promising results on a wide\nvariety of computer vision tasks including image classifi-\n(a) Conceptual representation of the proposed approach. We reshape the\nReceptive Field (RF) by applying affine transformations optimized through\nour Differentiable Augmentation Search (DAS). On the top right you can\nsee how fusing with random transformation would not lead to benefits as,\nwhen concatenating in time, the employed shift mechanism would fuse\nfeatures related to random parts. On the bottom, the augmentations guided\nby DAS obtain specific shapes of the RF so that more context is kept.\nrotate\ntranslate\nzoom\nfound by DAS\n(b) RF visualization (ResNet-50, with GSF fusion) when different single\nor composed transformations are applied. The last column shows our DAS\nselected operation for CIFAR-10 and CIFAR-100, which combines trans-\nlation, rotation and zoom. More details in Tab. 4.\nFigure 1. 1a overviews our approach and 1b shows a real exam-\nple of obtained receptive fields. The employed transformations are\nfundamental to shape the receptive field, as shown in 1b. The aug-\nmented images with DAS (Sec. 3.1) are concatenated in time, and\nprocessed through a video network that partially shifts and fuses\nthe features (Sec. 3.2).\ncation and semantic segmentation [13, 36, 48, 56]. How-\never, while these techniques have demonstrated remarkable\nsuccess, it is noteworthy that these high-capacity models\nnecessitate increased computational resources for effective\ntraining and inference, making them economically imprac-\ntical for training and deployment within practical applica-\ntion scenarios. Moreover, the over-parametrization of ViT\nand Deep CNNs makes the networks prone to overfitting,\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n5829",
        "chosen": "DAS is a differentiable augmentation search method for image classification and semantic segmentation. It generates image variations processed as videos, guiding the reshaping of the spatial receptive field with task-dependent transformations. This improves accuracy on multiple datasets when integrated with light-weight video backbones.",
        "rejected": "Radiologists desire fully automated versatile AI for medical imaging interpretation. This paper leverages language as supervision by distilling diagnostic knowledge from pre-trained 2D X-ray models to interpret 3D chest CT images. It uses robust contrastive learning to align images and reports within patients, demonstrating feasibility in real-world applications."
    },
    {
        "prompt": "summary: Due to the high potential for abuse of GenAI systems, the\ntask of detecting synthetic images has recently become of\ngreat interest to the research community. Unfortunately, ex-\nisting image-space detectors quickly become obsolete as new\nhigh-fidelity text-to-image models are developed at blinding\nspeed. In this work, we propose a new synthetic image detec-\ntor that uses features obtained by inverting an open-source\npre-trained Stable Diffusion model. We show that these inver-\nsion features enable our detector to generalize well to unseen\ngenerators of high visual fidelity (e.g., DALL·E 3) even when\nthe detector is trained only on lower fidelity fake images\ngenerated via Stable Diffusion. This detector achieves new\nstate-of-the-art across multiple training and evaluation se-\ntups. Moreover, we introduce a new challenging evaluation\nprotocol that uses reverse image search to mitigate stylistic\nand thematic biases in the detector evaluation. We show that\nthe resulting evaluation scores align well with detectors’ in-\nthe-wild performance, and release these datasets as public\nbenchmarks for future research.\n*Work done during an internship at Google Research.\n1. Introduction\nRecent advances in text-to-image modeling have made it\neasier than ever to generate harmful or misrepresentative\ncontent at scale. Moreover, new versions of most photoreal-\nistic commercial models are being continuously updated and\nreleased behind closed APIs, making it harder to keep fake\nimage detectors up to date. In this work, we make significant\nstrides towards building a GenAI detector that can reliably\nidentify images from unseen photorealistic text-to-image\nmodels. Specifically, we propose a model that can be trained\nusing fake images only from Stable Diffusion (SD) [45] and\nreliably detect images generated by recent open (Kandin-\nsky [51], W¨uerstchen [39], PixArt-α [16], etc.) and closed-\nsource text-to-image models (Imagen [46], Midjourney [2],\nDALL·E 3 [12], etc.) of significantly higher visual fidelity.\nExisting methods [17, 37, 54] focus primarily on detect-\ning traces left by convolutional generators in a way that is ro-\nbust to re-compression, resizing and other in-the-wild trans-\nformations. While these methods worked well for GANs and\nearly diffusion models, we show that they, unfortunately, fail\nto generalize well to current photorealistic generative models,\neven when re-trained using better data. Recent diffusion de-\ntectors that rely on CLIP embeddings [37] or inversions [55]\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n10759",
        "chosen": "FakeInversion is a synthetic image detector that uses features from an inverted Stable Diffusion model. This method generalizes well to unseen high-fidelity generators, even when trained on lower-fidelity images. It achieves state-of-the-art results across multiple training and evaluation setups and introduces a new evaluation protocol using reverse image search.",
        "rejected": "Due to the rapid development of high-fidelity text-to-image models, there is an increasing need for synthetic image detection. This research introduces a detector using inversion features from a Stable Diffusion model, proving effective against unseen generators. The evaluation protocol includes reverse image search to mitigate biases, aligning with real-world performance."
    },
    {
        "prompt": "summary: Detecting edges in images suffers from the problems of\n(P1) heavy imbalance between positive and negative classes\nas well as (P2) label uncertainty owing to disagreement be-\ntween different annotators. Existing solutions address P1\nusing class-balanced cross-entropy loss and dice loss and\nP2 by only predicting edges agreed upon by most annota-\ntors. In this paper, we propose RankED, a uniﬁed ranking-\nbased approach that addresses both the imbalance problem\n(P1) and the uncertainty problem (P2). RankED tackles\nthese two problems with two components: One component\nwhich ranks positive pixels over negative pixels, and the\nsecond which promotes high conﬁdence edge pixels to have\nmore label certainty. We show that RankED outperforms\nprevious studies and sets a new state-of-the-art on NYUD-\nv2, BSDS500 and Multi-cue datasets. Code is available at\nhttps://ranked-cvpr24.github.io.\n1. Introduction\nDetecting contours of objects in a given image is a fun-\ndamental problem in Computer Vision.\nIt has been ap-\nproached as a machine learning problem since the intro-\nduction of the inﬂuential BSDS dataset [1]. As with any\nlearning-based approach, characteristics of the training data\naffects performance. One striking issue regarding ground-\ntruth contour data is that contours are rare events. For ex-\nample, in the BSDS dataset, only 7% of all pixels within an\nimage are marked as edge pixels1. This creates a signiﬁcant\nimbalance between the positive (edge) and negative (non-\nedge) classes, which hinders the training of machine learn-\ning models. Another important issue observed in edge data\nis the uncertainty regarding the ground-truth annotations.\n†Equal contribution.\n1Although some studies call such high-level, semantic edges as “con-\ntour” and low-level edges as “edge”, we follow the recent literature\n[12, 15, 39, 44, 55] and use the term “edge” for contours in the rest of\nthe paper.\n1\n1\nAnnotator 1\n1\n1\nAnnotator 2\n1\n1\n1\nAnnotator 3\n.33\n.66\n1\n.33\nPixel-wise \nAveraging\nThreshold\n1\n1\nTraining \nTarget (𝐲)\n0.4\n0.4\n0.6\n0.5\n0.7\n0.3\n0.4\n0.2\n0.2\nPrediction (𝐩)\nScore-based\nLoss\n(a) Existing Approaches\nRank Positives\nAbove Negatives\n0.4\n0.4\n0.6\n0.5\n0.5\n0.3\n0.4\n0.2\n0.2\n𝒑𝟑\n\"\n(b) RankED with its Ranking and Sorting Components\nSort Positives\nwrt Certianty\nCertainty of\nLabels (c)\n𝒑𝟐\n\"\n𝒑𝟎\n\"\n𝒑𝟏\n\"\nPrediction (𝐩)\n𝑝&\n\" > 𝑝'\n(\nfor each positive 𝑖& negative 𝑗\n𝑝&\n\" > 𝑝'\n\" ⇒𝑐&\n\" > 𝑐'\n\"\nif each positive 𝑖, 𝑗\nPos./Neg.\nAnnotation\nPositive Pred.\n𝑐)\n\"\n𝑐*\n\"\n𝑐+\n\"\n𝑐,\n\"\nNegative Pred.\nFigure 1. (a) Current approaches threshold label certainties and\nclass-balanced cross-entropy loss for training edge detectors. (b)\nWith RankED, we propose a uniﬁed approach which ranks pos-\nitives over negatives to handle the imbalance problem and sorts\npositives with respect to their certainties.\nThere exist a non-trivial amount of variation between the\nannotations produced by different human annotators, which\nessentially creates noise in the supervisory signal.\nThese two issues of edge ground-truth data, namely the\nimbalance and uncertainty, have long been known, how-\never, efforts to address them have remained limited. For\nthe imbalance problem, although there is a vast literature on\nlong-tailed and imbalance learning (see, e.g., [46, 53]), re-\nsearchers have only explored using Dice Loss [10–12] and\nweighted cross-entropy loss [20, 29, 39, 44, 49], which mit-\nigate the problem to a certain extent. However, as we show\nin this paper, they are far from ﬁnding the optimal solu-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n3239",
        "chosen": "RankED is a unified ranking-based approach for edge detection that addresses both class imbalance and label uncertainty. It ranks positive pixels over negatives and promotes high-confidence edge pixels. RankED outperforms previous methods and sets a new state-of-the-art on NYUD-v2, BSDS500, and Multi-cue datasets.",
        "rejected": "This paper presents RankED, a ranking-based method addressing class imbalance and label uncertainty in edge detection. It ranks positive pixels over negatives and promotes high-confidence edge pixels, outperforming previous studies on datasets like NYUD-v2 and BSDS500."
    },
    {
        "prompt": "summary: Detecting objects in 3D under various (normal and ad-\nverse) weather conditions is essential for safe autonomous\ndriving systems. Recent approaches have focused on em-\nploying weather-insensitive 4D radar sensors and lever-\naging them with other modalities, such as LiDAR. How-\never, they fuse multi-modal information without considering\nthe sensor characteristics and weather conditions, and lose\nsome height information which could be useful for localiz-\ning 3D objects. In this paper, we propose a novel frame-\nwork for robust LiDAR and 4D radar-based 3D object de-\ntection. Specifically, we propose a 3D-LRF module that\nconsiders the distinct patterns they exhibit in 3D space (e.g.,\nprecise 3D mapping of LiDAR and wide-range, weather-\ninsensitive measurement of 4D radar) and extract fusion\nfeatures based on their 3D spatial relationship.\nThen,\nour weather-conditional radar-flow gating network mod-\nulates the information flow of fusion features depending\non weather conditions, and obtains enhanced feature that\neffectively incorporates the strength of two domains un-\nder various weather conditions. The extensive experiments\ndemonstrate that our model achieves SoTA performance for\n3D object detection under various weather conditions.\n1. Introduction\nDetecting 3D objects, which aims to classify the objects\nand localize them in 3D coordinates, plays a crucial role in\nvarious applications such as autonomous driving, robotic,\nand drone systems [1, 8, 44]. Many attempts have been\nmade to utilize various sensors, such as camera, LiDAR,\nand radar, for 3D object detection [9, 11, 16, 24, 31, 51, 53].\nThese methods are typically trained and tested in ideal au-\ntonomous driving scenarios, demonstrating satisfactory per-\nformance under normal conditions. Since real-world driv-\ning situations have diverse weather conditions, robust mod-\nels operating in various conditions are needed.\n*Code: https://github.com/yujeong-star/RL_3DOD.\n(b) Point-based Fusion\nLiDAR\nRadar\nImage\nor\nFusion\nPoor\nPoor\n4D Radar\nImage\nLiDAR\n(d) BEV-based Fusion\nFusion\nPoor\nLiDAR\nRadar\nImage\nor\nPoor\n(c) Pseudo-Image-based Fusion\nLiDAR\nRadar\nFusion\n(e) Proposed Fusion\nLiDAR\n4D Radar\nFusion\nImage\nWeather-\ninformed\n(a) Camera, LiDAR, 4D Radar in Adverse Weather\nFigure 1. In adverse weather conditions, as depicted in (a), radar\nexhibits the highest robustness, followed by LiDAR, while the im-\nage is significantly degraded. Prior multi-modal 3D object detec-\ntion research follows the fusion method in (b)-(d). They suffer\nfrom inaccurate information from images or sub-optimal perfor-\nmance due to the compression of critical 3D information from Li-\nDAR and radar into BEV or pseudo-images for fusion. In con-\ntrast, our approach (e) effectively fuses LiDAR and 4D radar in\n3D space, taking the strengths of each sensor through weather in-\nformation, showing robust performance under adverse conditions.\nRecently, several research has focused on addressing\nthese challenges by employing radar sensors capable of\nhandling various weather conditions [28], and has re-\nleased datasets containing diverse weather environments\n[4, 28, 35]. Moreover, research on a new novel sensor, 4D\nradar, which includes height information, has been initiated\n[22, 28, 38, 43]. Since the radar relies on radio waves, it\nhas the advantages of long-range detection and robustness\nunder adverse weather conditions. However, it does not pro-\nvide precise distance or detailed 3D maps and struggles with\nstandalone deployment [12, 19, 43]. Therefore, ongoing\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n15162",
        "chosen": "This paper proposes a novel framework for robust LiDAR and 4D radar-based 3D object detection. It introduces a 3D-LRF module to extract fusion features based on the 3D spatial relationship of LiDAR and radar, and a weather-conditional radar-flow gating network to modulate information flow. The model achieves state-of-the-art performance under various weather conditions.",
        "rejected": "A novel framework for 3D object detection under diverse weather conditions is proposed, utilizing LiDAR and 4D radar. The approach, featuring a 3D-LRF module and weather-conditional radar-flow gating network, demonstrates state-of-the-art performance by effectively fusing sensor data based on their spatial relationships and weather conditions."
    },
    {
        "prompt": "summary: Group robustness strategies aim to mitigate learned bi-\nases in deep learning models that arise from spurious cor-\nrelations present in their training datasets. However, most\nexisting methods rely on the access to the label distribution\nof the groups, which is time-consuming and expensive to\nobtain. As a result, unsupervised group robustness strate-\ngies are sought. Based on the insight that a trained model’s\nclassiﬁcation strategies can be inferred accurately based on\nexplainability heatmaps, we introduce ExMap, an unsuper-\nvised two stage mechanism designed to enhance group ro-\nbustness in traditional classiﬁers. ExMap utilizes a cluster-\ning module to infer pseudo-labels based on a model’s ex-\nplainability heatmaps, which are then used during training\nin lieu of actual labels. Our empirical studies validate the\nefﬁcacy of ExMap - We demonstrate that it bridges the per-\nformance gap with its supervised counterparts and outper-\nforms existing partially supervised and unsupervised meth-\nods. Additionally, ExMap can be seamlessly integrated with\nexisting group robustness learning strategies. Finally, we\ndemonstrate its potential in tackling the emerging issue of\nmultiple shortcut mitigation1.\n1. Introduction\nDeep neural network classiﬁers trained for classiﬁcation\ntasks, have invited increased scrutiny from the research\ncommunity due to their overreliance on spurious correla-\ntions present in the training data [4, 5, 9, 31, 38]. This is\nrelated to the broader aspect of Shortcut Learning [10], or\nthe Clever Hans effect [15], where a model picks the path\nof least resistance to predict data, thus relying on shortcut\nfeatures that are not causally linked to the label. The con-\nsequence of this phenomenon is that, although such models\nmay demonstrate impressive mean accuracy on the test data,\nthey may still fail on challenging subsets of the data, i.e. the\ngroups [7, 8, 27]. As a result, group robustness is a natural\n1Code available at https://github.com/rwchakra/exmap\nobjective to be met to mitigate reliance on spurious corre-\nlations. Thus, instead of evaluating models based on mean\ntest accuracy, evaluating them on worst group accuracy has\nbeen the recent paradigm [12, 21, 25, 40], resulting in the\nemergence of group robustness techniques. By dividing a\ndataset into pre-determined groups of spurious correlations,\nclassiﬁers are then trained to maximize the worst group ac-\ncuracy - As a result, the spurious attribute that the model is\nmost susceptible to is considered the shortcut of interest.\nIn Figure 1, we illustrate the group robustness paradigm.\nGiven a dataset, a robustness strategy takes as input the\ngroup labels and retrains a base classiﬁer (such as Expected\nRisk Minimization, i.e. ERM) to improve the worst group\naccuracy (G3 in this case). GroupDRO [28] was one of the\nearly inﬂuential works that introduced the group robustness\nparadigm. Further, it demonstrated a strategy that could in-\ndeed improve worst group accuracy. One limitation of this\napproach was the reliance on group labels in the training\ndata, which was replaced with the reliance on group labels\nin the validation data in successive works [13, 19]. How-\never, while these efforts have made strides in enhancing the\naccuracy of trained classiﬁers for underperforming groups,\nmany hinge on the assumption that the underlying groups\nare known apriori and that the group labels are available,\nwhich is often impractical in real-world contexts. An unsu-\npervised approach, as illustrated in Figure 1, would ideally\nestimate pseudo-labels that could be inputs to any robust-\nness strategy, leading to improved worst group robustness.\nAn example of such a fully unsupervised worst group ro-\nbustness approach is (GEORGE) [32]. GEORGE clusters\nthe penultimate layer features in a UMAP reduced space,\ndemonstrating impressive results on multiple datasets. In\nthis work, we instead show that clustering explainability\nheatmaps instead, is more beneﬁcial in improving worst\ngroup robustness. Intuitively, this stems from the fact that a\npixel-attribution based explainability method in input space\nfocuses only on the relevant image features (pixel space) in\nthe task, discarding other entangled features irrelevant for\nthe ﬁnal prediction.\nIn our work, we circumvent the need for a group labeled\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n12017",
        "chosen": "ExMap is an unsupervised two-stage mechanism enhancing group robustness in classifiers using explainability heatmaps for pseudo-label inference. It bridges the performance gap with supervised methods and outperforms existing unsupervised approaches, demonstrating effectiveness in mitigating spurious correlations.",
        "rejected": "ExMap, an unsupervised mechanism, enhances group robustness in classifiers without relying on labeled groups. By clustering explainability heatmaps, it infers pseudo-labels used during training, bridging the performance gap with supervised methods and outperforming existing partially supervised and unsupervised techniques."
    },
    {
        "prompt": "summary: Bird’s-eye View (BeV) representations have emerged as\nthe de-facto shared space in driving applications, offer-\ning a unified space for sensor data fusion and supporting\nvarious downstream tasks. However, conventional models\nuse grids with fixed resolution and range and face compu-\ntational inefficiencies due to the uniform allocation of re-\nsources across all cells. To address this, we propose Point-\nBeV, a novel sparse BeV segmentation model operating on\nsparse BeV cells instead of dense grids. This approach of-\nfers precise control over memory usage, enabling the use\nof long temporal contexts and accommodating memory-\nconstrained platforms. PointBeV employs an efficient two-\npass strategy for training, enabling focused computation\non regions of interest. At inference time, it can be used\nwith various memory/performance trade-offs and flexibly\nadjusts to new specific use cases. PointBeV achieves state-\nof-the-art results on the nuScenes dataset for vehicle, pedes-\ntrian, and lane segmentation, showcasing superior perfor-\nmance in static and temporal settings despite being trained\nsolely with sparse signals. We release our code with two\nnew efficient modules used in the architecture: Sparse Fea-\nture Pulling, designed for the effective extraction of features\nfrom images to BeV, and Submanifold Attention, which en-\nables efficient temporal modeling. The code is available at\nhttps://github.com/valeoai/PointBeV.\n1. Introduction\nBird’s-eye View (BeV) representations are now ubiqui-\ntously in driving applications.\nIndeed, a top-view ego-\ncentric grid is not only a convenient shared space for fus-\ning inputs from multiple sensors [28, 39, 42, 49], but also\na space-aware representation relevant for many downstream\ntasks such as detection [31, 34], segmentation [15, 39], fore-\ncasting [11], tracking [48], or planning [8, 18, 40]. BeV\nsegmentation encompasses a broad family of tasks such\nas 2D instance segmentation [15], 3D instance segmenta-\ntion [20, 21, 54], occupancy forecasting [22, 35] and online\n*Work done at Valeo.ai.\n0.5\n1\n2\n4\n31\n32\nCVT\nMemory Usage (GiB) (←)\n35\n36\n37\n38\nPointBeV\n40k\n23k\n13k\n8k\n5.5k\n4.1k\n3.6k\nSimple-BEV\nBEVFormer\nLaRa\nIoU (→)\nFigure 1. BeV vehicle IoU vs. memory footprint on nuScenes\n[3] validation set. Models are evaluated without visibility filtering\n(i.e all annotated vehicles are considered) at resolution 224 × 480.\nThe memory consumption is calculated using a 40GB A100 GPU.\nThe size of a dot represents the number of BeV points being eval-\nuated, the smaller the better. PointBeV has the capacity to explore\nvarious trade-offs between efficiency and performance by varying\nthe number of points being considered. The remaining points are\nconsidered as zeros in the final prediction. Using PointBeV we\ncan achieve state-of-the-art performance with only a small portion\nof the points and without losing performance.\nmapping [27]. In this paper, we focus on BeV segmentation\nfrom multiple cameras, in scenarios with or without past\nframes, respectively referred to as temporal and static.\nBeV representations are usually implemented using\ngrids of fixed resolution and range [9, 13, 15, 26, 31].\nThis limits their efficiency in terms of compute, even more\nclearly when considering temporal tasks, where aggregat-\ning past frames for long horizons can be a very costly en-\ndeavor. Departing from these dense BeV grid approaches,\nwe present in this paper PointBeV, a camera-based BeV\nsegmentation model that operates on sparse BeV features.\nOur approach offers control over the model’s memory us-\nage by restricting the number of points considered, and en-\nables adaptive focus on specific regions of interest. No-\ntably, we develop two modules for efficient sparse opera-\ntions: the Sparse Feature Pulling module, which retrieves\nfeatures from multiple cameras using sparse coordinates,\nand the Submanifold Attention module, adapted from sub-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n15195",
        "chosen": "PointBeV is a sparse Bird’s-eye View (BeV) segmentation model operating on sparse BeV cells. It offers memory usage control, enabling long temporal contexts and efficient computation. PointBeV achieves state-of-the-art results on nuScenes for vehicle, pedestrian, and lane segmentation with sparse signals.",
        "rejected": "PointBeV is introduced as a sparse Bird’s-eye View (BeV) segmentation model that operates efficiently on sparse BeV cells instead of dense grids. This approach offers precise memory control and achieves state-of-the-art results on the nuScenes dataset, particularly in static and temporal settings, through its efficient two-pass training strategy."
    },
    {
        "prompt": "summary: Coordinate based implicit neural representations have\ngained rapid popularity in recent years as they have been\nsuccessfully used in image, geometry and scene modeling\ntasks. In this work, we present a novel use case for such\nimplicit representations in the context of learning anatomi-\ncally constrained face models. Actor speciﬁc anatomically\nconstrained face models are the state of the art in both facial\nperformance capture and performance retargeting. Despite\ntheir practical success, these anatomical models are slow to\nevaluate and often require extensive data capture to be built.\nWe propose the anatomical implicit face model; an ensem-\nble of implicit neural networks that jointly learn to model\nthe facial anatomy and the skin surface with high-ﬁdelity,\nand can readily be used as a drop in replacement to con-\nventional blendshape models. Given an arbitrary set of skin\nsurface meshes of an actor and only a neutral shape with\nestimated skull and jaw bones, our method can recover a\ndense anatomical substructure which constrains every point\non the facial surface. We demonstrate the usefulness of our\napproach in several tasks ranging from shape ﬁtting, shape\nediting, and performance retargeting.\n1. Introduction\nDeformable face models are an important tool in the arse-\nnal of visual effects artists dealing with facial animation.\nAs they are ubiquitously used both in high-end production\nworkﬂows and lightweight consumer applications, build-\ning expressive face models for various applications contin-\nues to remain an active area of research [17]. Face mod-\nels today can range from simple linear global shape mod-\nels [4, 27, 29] to highly complex local models that incorpo-\nrate the underlying facial anatomy through physical simula-\ntion [15, 44, 48] or through anatomical constraints [47].\nIn this work, we concern ourselves primarily with the\nhigh-quality facial animation workﬂow where actor spe-\nciﬁc linear blendshape models [27] continue to remain\nthe most commonly used tool for creating facial anima-\ntions [10, 33, 47]. We propose a new class of actor speciﬁc\nshape models named the Anatomical Implicit face Model\n(AIM) which provides several unique advantages over the\nexisting actor speciﬁc face models, and can be used as a\ndrop-in replacement for traditional blendshape models.\nAn actor speciﬁc blendshape model is a collection of\n3D shapes of the given actor performing a number of fa-\ncial expressions, usually created by face scanning [2] or by\nan artist. While the user-friendliness of such actor speciﬁc\nblendshape models contributes to their wide adoption, it is\na well known limitation that such models often require hun-\ndreds of shapes to accurately model complex facial defor-\nmation [27]. To address these shortcomings, local blend-\nshape models [10, 42, 47] were proposed. By splitting the\nface into regions, and allowing the individual regions to de-\nform independently, local shape models are able to capture\ncomplex deformations with a limited number of shapes.\nWhile local models address the lack of expressivity in\nglobal shape models, state-of-the-art methods in facial per-\nformance capture [47] and retargeting [10] often incorpo-\nrate anatomical constraints on the facial surface to plausibly\nrestrict the range of the skin deformations. The anatom-\nical constraints employed by these models [10, 47] pro-\nvide a few hidden advantages that end up contributing to-\nwards their practical success. For example, in the context\nof facial performance capture, Wu et al. [47] demonstrated\nthat including anatomical constraints derived from the re-\nlationship between the facial skin and underlying bones\n(skull and mandible) helps to separate the rigid and non-\nrigid components of facial deformation, leading to better\nface performance capture. In the context of facial perfor-\nmance retargeting, Chandran et al. [10] made use of such an\nanatomically constrained local face model to restrict a retar-\ngeted shape to lie within the space of anatomically plausible\nshapes of the target actor.\nDespite their practical success, anatomical constraints\nare often formulated in practice as regularization terms that\nhave to be satisﬁed as part of complex optimization prob-\nlems involving several objectives. As a result, ﬁtting these\nanatomical face models to a target scan or an image for in-\nstance, is a computationally intensive procedure taking sev-\neral minutes per frame on a CPU, or requires hand crafted\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n2220",
        "chosen": "AIM is an ensemble of implicit neural networks modeling facial anatomy and skin surface with high-fidelity. It recovers dense anatomical substructures from skin surface meshes and neutral shapes, providing advantages in shape fitting, editing, and performance retargeting.",
        "rejected": "The anatomical implicit face model uses implicit neural networks to jointly learn facial anatomy and skin surface with high fidelity. Given skin surface meshes and neutral shapes with estimated bones, it recovers dense anatomical substructures, enabling applications ranging from shape fitting to performance retargeting."
    },
    {
        "prompt": "summary: Superpixels play a crucial role in image processing by\npartitioning an image into clusters of pixels with similar vi-\nsual attributes. This facilitates subsequent image process-\ning tasks, offering computational advantages over the ma-\nnipulation of individual pixels. While numerous overseg-\nmentation techniques have emerged in recent years, many\nrely on predefined initialization and termination criteria. In\nthis paper, a novel top-down superpixel segmentation algo-\nrithm called Hierarchical Histogram Threshold Segmenta-\ntion (HHTS) is introduced. It eliminates the need for ini-\ntialization and implements auto-termination, outperform-\ning state-of-the-art methods w.r.t. boundary recall.\nThis\nis achieved by iteratively partitioning individual pixel seg-\nments into foreground and background and applying inten-\nsity thresholding across multiple color channels. The un-\nderlying iterative process constructs a superpixel hierarchy\nthat adapts to local detail distributions until color infor-\nmation exhaustion. Experimental results demonstrate the\nsuperiority of the proposed approach in terms of bound-\nary adherence, while maintaining competitive runtime per-\nformance on the BSDS500 and NYUV2 datasets. Further-\nmore, an application of HHTS in refining machine learning-\nbased semantic segmentation masks produced by the Seg-\nment Anything Foundation Model (SAM) is presented.\n1. Introduction\nSuperpixel segmentation is an important preprocessing step\nin computer vision. It groups pixels with similar proper-\nties to reduce the number of primitives and enhance ob-\nject representation. Various image processing tasks benefit\nfrom high-quality superpixels, such as semantic segmenta-\ntion [12, 33, 42, 52], object tracking [47], object categoriza-\ntion [11], simultaneous localization and mapping (SLAM)\n[6, 20], image segmentation [10, 34, 50], video segmen-\ntation [39, 40], and stereo matching [37, 44]. There ex-\nists a wide range of approaches for image oversegmention\n(a)\n(b)\n(c)\n(d)\nFigure 1. Visual comparison of 500 superpixels resulting from\n(a, c) ETPS [previous], (b, d) HHTS [proposed] segmentation.\n(a)\n(b)\n(c)\nFigure 2. Visual comparison of semantic segment masks (a) orig-\ninal image, (b) semantic segment (SAM ViT-H) [previous] and\n(c) refined semantic segment (SAM + HHTS) [proposed]\n[1, 5, 7–9, 18, 21, 28, 30, 46, 49, 51], often involving trade-\noffs between boundary adherence, regular segment sizes,\nand computational efficiency. However, there are also vari-\nous applications e.g. in computer graphics or medical imag-\ning [37, 50] requiring the maximization of boundary recall.\nIn this work, a novel hierarchical oversegmentation ap-\nproach based on auto-terminating local histogram thresh-\nolding is introduced, resulting in superpixels with signifi-\ncantly higher boundary adherence. The corresponding re-\nsults can be utilized e.g. for fine-tuning semantic segmen-\ntation masks, as will be shown for the Segment Anything\nModel (SAM) [15]. Most established superpixel algorithms\ndepend on a priori knowledge of image content and struc-\nture: Examples are seed-based methods requiring infor-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n3195",
        "chosen": "HHTS is a novel top-down superpixel segmentation algorithm eliminating initialization and implementing auto-termination. It outperforms state-of-the-art methods in boundary recall by iteratively partitioning pixel segments and applying intensity thresholding, demonstrating superior boundary adherence on BSDS500 and NYUV2 datasets.",
        "rejected": "HHTS, a top-down superpixel segmentation algorithm, automatically terminates and outperforms state-of-the-art methods in boundary recall. It iteratively partitions pixel segments into foreground and background, applying intensity thresholding across multiple channels, demonstrating superior performance on BSDS500 and NYUV2 datasets."
    },
    {
        "prompt": "summary: The study of complex human interactions and group ac-\ntivities has become a focal point in human-centric computer\nvision. However, progress in related tasks is often hindered\nby the challenges of obtaining large-scale labeled datasets\nfrom real-world scenarios. To address the limitation, we in-\ntroduce M3Act, a synthetic data generator for multi-view\nmulti-group multi-person human atomic actions and group\nactivities. Powered by Unity Engine, M3Act features mul-\ntiple semantic groups, highly diverse and photorealistic im-\nages, and a comprehensive set of annotations, which facil-\nitates the learning of human-centered tasks across single-\nperson, multi-person, and multi-group conditions.\nWe\ndemonstrate the advantages of M3Act across three core ex-\nperiments. The results suggest our synthetic dataset can sig-\nnificantly improve the performance of several downstream\nmethods and replace real-world datasets to reduce cost.\nNotably, M3Act improves the state-of-the-art MOTRv2 on\nDanceTrack dataset, leading to a hop on the leaderboard\nfrom 10th to 2nd place. Moreover, M3Act opens new re-\nsearch for controllable 3D group activity generation. We\ndefine multiple metrics and propose a competitive baseline\nfor the novel task. Our code and data are available at our\nproject page: http://cjerry1243.github.io/M3Act.\n1. Introduction\nUnderstanding collective human activities and social\ngroups carries significant implications across diverse do-\nmains, as it contributes to bolstering public safety within\n†Work done during internship at Roblox\nsurveillance systems, ensuring safe navigation for au-\ntonomous robots and vehicles amidst human crowds, and\nenriching social awareness in human-robot interactions [8,\n9, 11, 12, 21, 37, 49, 51]. However, the advancement in\nrelated tasks is often impeded by the challenges of obtain-\ning large-scale human group activity datasets in real-world\nscenarios with fine-grained multifaceted annotations.\nGenerating synthetic data is an emerging alternative to\ncollecting real-world data due to its capability of produc-\ning large-scale datasets with perfect annotations. Nonethe-\nless, most synthetic datasets [4, 20, 40, 48, 53] are pri-\nmarily designed to facilitate human pose and shape esti-\nmation. They can only provide data with independently-\nanimated persons, which is unsuitable for tasks in single-\ngroup and multi-group conditions [51]. To address the lim-\nitation, we propose M3Act, a synthetic data generator, with\nmulti-view multi-group multi-person human actions and\ngroup activities.\nAs presented in Tab. 1, M3Act stands\nout by offering comprehensive annotations including both\n2D and 3D annotations as well as fine-grained person-level\nand group-level labels, thereby making it an ideal synthetic\ndataset generator to support tasks such as human activity\nrecognition and multi-person tracking across all listed real-\nworld datasets.\nIllustrated in Fig. 1, our synthetic data generator features\nmultiple semantic groups, highly diverse and photorealistic\nimages, and a rich set of annotations. It encompasses 25\nphotometric 3D scenes, 104 HDRIs (High Dynamic Range\nImages), 5 lighting volumes, 2200 human models, 384 an-\nimations (categorized into 14 atomic action classes), and 6\ngroup activities. For our experiments, We generated two\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n21922",
        "chosen": "M3Act is a synthetic data generator for multi-view, multi-group, multi-person human actions and activities. Powered by Unity Engine, it offers diverse, photorealistic images and comprehensive annotations, improving downstream methods and enabling controllable 3D group activity generation.",
        "rejected": "M3Act, a synthetic data generator for multi-view multi-group human activities, is presented. Utilizing Unity Engine, M3Act provides photorealistic images and comprehensive annotations, enhancing learning for human-centric tasks. It improves state-of-the-art MOTRv2 on DanceTrack, opening new research avenues for controllable 3D group activity generation."
    },
    {
        "prompt": "summary: 3D visual grounding involves matching natural language\ndescriptions with their corresponding objects in 3D spaces.\nExisting methods often face challenges with accuracy in\nobject recognition and struggle in interpreting complex\nlinguistic queries,\nparticularly with descriptions that\ninvolve multiple anchors or are view-dependent.\nIn\nresponse,\nwe present the MiKASA (Multi-Key-Anchor\nScene-Aware) Transformer. Our novel end-to-end trained\nmodel\nintegrates\na\nself-attention-based\nscene-aware\nobject\nencoder\nand\nan\noriginal\nmulti-key-anchor\ntechnique, enhancing object recognition accuracy and\nthe understanding of spatial relationships. Furthermore,\nMiKASA improves the explainability of decision-making,\nfacilitating error diagnosis.\nOur model achieves the\nhighest overall accuracy in the Referit3D challenge\nfor both the Sr3D and Nr3D datasets,\nparticularly\nexcelling by a large margin in categories that require\nviewpoint-dependent descriptions.\nThe source code and\nadditional resources for this project are available on\nGitHub: https://github.com/dfki-av/MiKASA-3DVG\n1. Introduction\n3D visual grounding serves as a crucial component in the\nintersection of natural language processing and computer\nvision. This task aims to identify and localize objects within\na 3D space, using linguistic cues for spatial and semantic\ngrounding. While existing research has made significant\nstrides, challenges remain.\nKey issues include the lack\nof explainability in current models, limitations in object\nrecognition within point cloud data, and the complexity of\nhandling intricate spatial relationships.\nMost existing 3D visual grounding models [2, 15, 16,\n33, 40] consist of three parts:\n(1) object encoder, (2)\ntext encoder, and (3) fusion model.\nThe object encoder\nprocesses the provided point cloud and generates features\nin the embedding space. However, because the points in a\n(a) Target category: “chair”\n(b) “The chair in the front of the\nblue-lit monitor.”\nFigure 1. Our methodology utilizes a dual-prediction framework\nfor 3D visual grounding. First, we assign a target category score\nbased on object categorization, as detailed in Fig. 1a. Next, a\nspatial score is integrated according to the object’s alignment with\nthe textual description, as shown in Fig. 1b.\npoint cloud are unordered and inconsistent in sparsity [26],\nit is not straightforward to apply the methodology typically\nused for 2D images. An additional challenge is that 3D\npoint cloud datasets are not as extensive as those for 2D\nimages [10, 25], which makes it difficult for the models to\ncorrectly recognize object categories. While enlarging the\ndataset could conceivably improve performance, we refrain\nfrom doing so to ensure a fair comparison with existing\nstate-of-the-art methods. Existing works [6, 18, 27] mainly\nuse different techniques such as noise addition, dropping\nout colors, and transformations to expand the sample space.\nThough these techniques may increase the stability of the\nproduced object embeddings, the improvement is limited.\nInspired by previous works [20, 21, 39] which aims to\nsolve object recognition problem, we leverage the fact\nthat data availability on objects within a specific space\ncan provide valuable insights into the characteristics and\nrelationships of their surrounding entities.\nFor instance,\nwhen we come across a cuboid-shaped object in a\nkitchen, we may naturally assume that it is a dishwasher.\nConversely, if we spot the same shape in a bathroom, it is\nmore plausible that it is a washing machine. Contextual\ninformation is crucial in determining the identity of objects\nand gives us a nuanced understanding of our surroundings.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n14131",
        "chosen": "MiKASA is a transformer model for 3D visual grounding integrating a scene-aware object encoder and multi-key-anchor technique. It enhances object recognition accuracy and spatial relationship understanding, achieving the highest accuracy in the Referit3D challenge.",
        "rejected": "The paper discusses the challenges in detecting synthetic images due to advancements in generative AI systems. A new detector leveraging inversion features from a pre-trained model shows robust generalization to unseen generators. Additionally, a novel evaluation protocol using reverse image search helps mitigate biases in detector evaluations."
    },
    {
        "prompt": "summary: The spiking cameras offer the benefits of high dynamic\nrange (HDR), high temporal resolution, and low data redun-\ndancy. However, reconstructing HDR videos in high-speed\nconditions using single-bit spikings presents challenges due\nto the limited bit depth. Increasing the bit depth of the spik-\nings is advantageous for boosting HDR performance, but the\nreadout efficiency will be decreased, which is unfavorable\nfor achieving a high frame rate (HFR) video. To address\nthese challenges, we propose a readout mechanism to ob-\ntain rolling-mixed-bit (RMB) spikings, which involves inter-\nleaving multi-bit spikings within the single-bit spikings in a\nrolling manner, thereby combining the characteristics of high\nbit depth and efficient readout. Furthermore, we introduce\nRMB-Net for reconstructing HDR and HFR videos. RMB-\nNet comprises a cross-bit attention block for fusing mixed-bit\nspikings and a cross-time attention block for achieving tem-\nporal fusion. Extensive experiments conducted on synthetic\nand real-synthetic data demonstrate the superiority of our\nmethod. For instance, pure 3-bit spikings result in 3 times\nof data volume, whereas our method achieves comparable\nperformance with less than 2% increase in data volume.\n1. Introduction\nReal-world scenes possess a significantly wider dynamic\nrange that exceeds the capability of conventional sensors.\nTypical high dynamic range (HDR) video reconstruction\nmethods [3, 23, 24, 53] with conventional sensors encode\nexposure times to capture images with alternating exposures.\nAnd by fusing the low dynamic range (LDR) images taken\nunder different exposures, the pitfalls of underexposure and\n# Equal contribution. ∗Corresponding author.\n† Majority of this work was done at Peking University.\nProject page: https://github.com/yongqiye00/RMB-Net\nInput\nHDR and HFR video\n(c)\n…\n7\n1\n0\nQc\ntime\nrolling-mixed-bit spikings\n…\nN = 255\n(b)\nN = 1\nmulti-bit spikings\n…\n…\n7\n0\nQc\n(a)\nN = 1\nN = 255\nsingle-bit spikings\n…\n…\n1\n0\nQc\nFigure 1. The HDR performance of a spiking camera is closely tied\nto the bit depth of the spikings. (a) From left to right: Single-bit\nquantization (Qc is the accumulated photon electrons), diagram of\nsingle-bit spiking planes, and reconstructed image by accumulating\nN spiking planes. Increasing N to 255 significantly boosts HDR\nperformance. (b) HDR can be boosted by reading out multi-bit spik-\nings. However, multi-bit spikings decrease the readout efficiency,\nwhich is not conducive to obtaining HFR videos. (c) The proposed\nRMB spikings with time-varying quantization. We further recon-\nstruct HDR and HFR videos from RMB spikings.\noverexposure are alleviated. This kind of approach has a\ndilemma between the frame rate and exposure time [20], i.e.,\nlong exposure restricts the improvement of frame rate [2],\nwhich makes it challenging to capture high frame rate (HFR)\nvideos with conventional sensors in high-speed scenes.\nRecent advancements in the field of HDR and HFR pho-\ntography have benefited from the integration of neuromor-\nphic sensors such as event cameras [6, 29, 30] and spiking\ncameras [2, 21]. These sensors offer appealing characteris-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25117",
        "chosen": "RMB-Net reconstructs HDR and HFR videos from rolling-mixed-bit spikings. It uses a cross-bit attention block for fusing mixed-bit spikings and a cross-time attention block for temporal fusion, achieving superior performance with minimal data volume increase.",
        "rejected": "A novel approach for robust 3D object detection under various weather conditions combines LiDAR and 4D radar data. By considering distinct patterns in 3D space and modulating information flow based on weather conditions, this method enhances fusion features, achieving state-of-the-art performance in adverse weather scenarios."
    },
    {
        "prompt": "summary: In subject-driven text-to-image synthesis, the synthesis\nprocess tends to be heavily inﬂuenced by the reference im-\nages provided by users, often overlooking crucial attributes\ndetailed in the text prompt.\nIn this work, we propose\nSubject-Agnostic Guidance (SAG), a simple yet effective\nsolution to remedy the problem.\nWe show that through\nconstructing a subject-agnostic condition and applying our\nproposed dual classiﬁer-free guidance, one could obtain\noutputs consistent with both the given subject and input text\nprompts. We validate the efﬁcacy of our approach through\nboth optimization-based and encoder-based methods. Ad-\nditionally, we demonstrate its applicability in second-order\ncustomization methods, where an encoder-based model is\nﬁne-tuned with DreamBooth. Our approach is conceptually\nsimple and requires only minimal code modiﬁcations, but\nleads to substantial quality improvements, as evidenced by\nour evaluations and user studies.\n1. Introduction\nSubject-driven text-to-image synthesis focuses on generat-\ning diverse image samples, conditioned on user-given text\ndescriptions and subject images. This domain has witnessed\na surge of interest and signiﬁcant advancements in recent\nyears. Optimization-based methods [16, 37, 41] tackle the\nproblem by overﬁtting pre-trained text-to-image synthesis\nmodels [36, 38] and text tokens to the given subject. Re-\ncently, encoder-based approaches [10, 24, 49] propose to\ntrain auxiliary encoders to generate subject embeddings, by-\npassing the necessity of per-subject optimization.\nIn the aforementioned approaches, both the embeddings\nand networks are intentionally tailored to closely ﬁt the tar-\nget subject. As a consequence, these learnable conditions\ntend to dominate the synthesis process, often obscuring the\nattributes speciﬁed in the text prompt.\nFor instance, as\nshown in Fig. 1, when employing S∗1 alongside the style\n1S∗denotes a pseudo-word, where its embedding is substituted by a\nlearnable subject embedding.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n6733",
        "chosen": "Subject-Agnostic Guidance (SAG) is introduced to improve subject-driven text-to-image synthesis. SAG constructs a subject-agnostic condition and applies dual classifier-free guidance, ensuring outputs align with both the subject and text prompts. This method enhances quality in optimization-based, encoder-based, and second-order customization approaches.",
        "rejected": "In subject-driven text-to-image synthesis, existing methods often emphasize subject-specific details over attributes described in text prompts. This paper introduces Subject-Agnostic Guidance (SAG), which ensures outputs align with both the subject and input text descriptions. SAG uses dual classifier-free guidance to improve image synthesis quality significantly."
    },
    {
        "prompt": "summary: Single-photon Light Detection and Ranging (LiDAR)\nsystems are often equipped with an array of detectors for\nimproved spatial resolution and sensing speed. However,\ngiven a fixed amount of flux produced by the laser trans-\nmitter across the scene, the per-pixel Signal-to-Noise Ra-\ntio (SNR) will decrease when more pixels are packed in a\nunit space. This presents a fundamental trade-off between\nthe spatial resolution of the sensor array and the SNR re-\nceived at each pixel. Theoretical characterization of this\nfundamental limit is explored. By deriving the photon ar-\nrival statistics and introducing a series of new approxi-\nmation techniques, the Mean Squared Error (MSE) of the\nmaximum-likelihood estimator of the time delay is derived.\nThe theoretical predictions align well with simulations and\nreal data.\n1. Introduction\nSingle-photon LiDAR has a wide range of applications in\nnavigation and object identification [21, 24–26, 30, 32].\nBy actively illuminating the scene with a laser pulse of a\nknown shape, we measure the time delays of single pho-\ntons upon their return, which correspond to the distance of\nthe object [4, 19, 36]. The advancement of photo detectors\nhas significantly improved the resolution of today’s LiDAR\n[8, 15, 17, 33, 39–41]. Moreover, algorithms have shown\nhow to reconstruct both the scene reflectivity and 3D struc-\nture [2, 6, 16, 20, 22, 23, 29, 36, 38, 42, 43].\nAs an imaging device, a photodetector used in LiDAR\nfaces similar problems as any other CCD or CMOS pixels.\nPacking more pixels into a unit space decreases the SNR be-\ncause the amount of photon flux seen by each pixel dimin-\nishes [12]. This fundamental limit is linked to the stochas-\ntic nature of the underlying Poisson arrival process of the\nphotons [11, 37]. Unless noise mitigation schemes are em-\nployed [2, 14, 22, 31], there is a trade-off between the num-\nber of pixels one can pack in a unit space and the SNR we\nwill observe at each pixel. The situation can be visualized\nin Fig. 1, where we highlight the phenomenon that if we\nFigure 1. As we pack more pixels in a unit space, we gain the\nspatial resolution with a reduction in the SNR. The goal of this\npaper is to understand the trade-off between the two factors.\nuse many small pixels, the spatial resolution is good but the\nper pixel noise caused by the random fluctuation of photons\nwill be high. The bias and variance trade-off will then lead\nto a performance curve that tells us how the accuracy of the\ndepth estimate will behave as we vary the spatial resolution.\nThe goal of this paper is to rigorously derive the above\nphenomenon. In particular, we want to answer the follow-\ning question:\nCan we theoretically derive, ideally in closed-form, the\nmean squared error of the LiDAR depth estimate as a\nfunction of the number of pixels per unit space?\nThe theoretical analysis presented in this paper is unique\nfrom several perspectives:\n• Beyond Single Pixel. The majority of the computer vi-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25307",
        "chosen": "This paper explores the fundamental trade-off in single-photon LiDAR between spatial resolution and Signal-to-Noise Ratio (SNR). It derives the Mean Squared Error (MSE) of the maximum-likelihood estimator for time delay, providing theoretical insights into the sensor's performance limits.",
        "rejected": "Single-photon LiDAR systems face a trade-off between spatial resolution and per-pixel Signal-to-Noise Ratio (SNR). By analyzing photon arrival statistics, this paper derives the Mean Squared Error (MSE) of time delay estimations, providing theoretical insights into optimizing SNR versus spatial resolution in single-photon detectors."
    },
    {
        "prompt": "summary: We introduce pixelSplat, a feed-forward model that learns\nto reconstruct 3D radiance fields parameterized by 3D Gaus-\nsian primitives from pairs of images. Our model features\nreal-time and memory-efficient rendering for scalable train-\ning as well as fast 3D reconstruction at inference time. To\novercome local minima inherent to sparse and locally sup-\nported representations, we predict a dense probability dis-\ntribution over 3D and sample Gaussian means from that\nprobability distribution. We make this sampling operation\ndifferentiable via a reparameterization trick, allowing us to\nback-propagate gradients through the Gaussian splatting\nrepresentation. We benchmark our method on wide-baseline\nnovel view synthesis on the real-world RealEstate10k and\nACID datasets, where we outperform state-of-the-art light\nfield transformers and accelerate rendering by 2.5 orders\nof magnitude while reconstructing an interpretable and ed-\nitable 3D radiance field. Additional materials can be found\non the project website. 1\n1. Introduction\nWe investigate the problem of generalizable novel view\nsynthesis from sparse image observations. This line of\nwork has been revolutionized by differentiable render-\ning [29, 40, 41, 50] but has also inherited its key weak-\nness: training, reconstruction, and rendering are notoriously\nmemory- and time-intensive because differentiable render-\ning requires evaluating dozens or hundreds of points along\neach camera ray [58].\nThis has motivated light-field transformers [10, 37, 43,\n47], where a ray is rendered by embedding it into a query to-\nken and a color is obtained via cross-attention over image to-\nkens. While significantly faster than volume rendering, such\nmethods are still far from real-time. Additionally, they do\nnot reconstruct 3D scene representations that can be edited\nor exported for downstream tasks in vision and graphics.\n1dcharatan.github.io/pixelsplat\nFigure 1. Overview. Given a pair of input images, pixelSplat recon-\nstructs a 3D radiance field parameterized via 3D Gaussian primi-\ntives. This yields an explicit 3D representation that is renderable in\nreal time, remains editable, and is cheap to train.\nMeanwhile, recent work on single-scene novel view syn-\nthesis has shown that it is possible to use 3D Gaussian prim-\nitives to enable real-time rendering with little memory cost\nvia rasterization-based volume rendering [19].\nWe present pixelSplat, which brings the benefits of\na primitive-based 3D representation—fast and memory-\nefficient rendering as well as interpretable 3D structure—\nto generalizable view synthesis. This is no straightforward\ntask. First, in real-world datasets, camera poses are only\nreconstructed up to an arbitrary scale factor. We address\nthis by designing a multi-view epipolar transformer that re-\nliably infers this per-scene scale factor. Next, optimizing\nprimitive parameters directly via gradient descent suffers\nfrom local minima. In the single-scene case, this can be\naddressed via non-differentiable pruning and division heuris-\ntics [19]. In contrast, in the generalizable case, we need\nto back-propagate gradients through the representation and\nthus cannot rely on non-differentiable operations. We thus\npropose a method by which Gaussian primitives can im-\nplicitly be spawned or deleted during training, avoiding lo-\ncal minima, but which nevertheless maintains gradient flow.\nSpecifically, we parameterize the positions (i.e., means) of\nGaussians implicitly via dense probability distributions pre-\ndicted by our encoder. In each forward pass, we sample\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n19457",
        "chosen": "pixelSplat is a feed-forward model reconstructing 3D radiance fields from image pairs using 3D Gaussian primitives. It offers real-time rendering and memory-efficient training, outperforming state-of-the-art methods in novel view synthesis with faster rendering and explicit 3D representations.",
        "rejected": "PixelSplat is introduced as a feed-forward model for reconstructing 3D radiance fields from pairs of images using 3D Gaussian primitives. It offers real-time rendering and efficient memory usage, making it suitable for scalable training and fast 3D reconstruction. Experiments show superior performance compared to state-of-the-art methods."
    },
    {
        "prompt": "summary: This paper introduces the first text-guided work for\ngenerating the sequence of hand-object interaction in 3D. The\nmain challenge arises from the lack of labeled data where\nexisting ground-truth datasets are nowhere near generalizable\nin interaction type and object category, which inhibits the\nmodeling of diverse 3D hand-object interaction with the correct\nphysical implication (e.g., contacts and semantics) from text\nprompts. To address this challenge, we propose to decompose\nthe interaction generation task into two subtasks: hand-object\ncontact generation; and hand-object motion generation. For\ncontact generation, a VAE-based network takes as input a\ntext and an object mesh, and generates the probability of\ncontacts between the surfaces of hands and the object during\nthe interaction. The network learns a variety of local geometry\nstructure of diverse objects that is independent of the objects’\ncategory, and thus, it is applicable to general objects. For\nmotion generation, a Transformer-based diffusion model utilizes\nthis 3D contact map as a strong prior for generating physically\nThis research was conducted when Jihyeon Kim was a graduate student\n(Master candidate) at UNIST†. Co-last authors∗.\nplausible hand-object motion as a function of text prompts\nby learning from the augmented labeled dataset; where we\nannotate text labels from many existing 3D hand and object\nmotion data. Finally, we further introduce a hand refiner\nmodule that minimizes the distance between the object surface\nand hand joints to improve the temporal stability of the object-\nhand contacts and to suppress the penetration artifacts. In the\nexperiments, we demonstrate that our method can generate\nmore realistic and diverse interactions compared to other\nbaseline methods. We also show that our method is applicable\nto unseen objects. We will release our model and newly labeled\ndata as a strong foundation for future research. Codes and data\nare available in: https://github.com/JunukCha/Text2HOI.\n1. Introduction\nImagine handing over an apple on a table to your friends:\nyou might first grab it and convey this to them. During a social\ninteraction, the hand pose and motion are often defined as a\nfunction of object’s pose, shape, and category. While existing\nworks [3, 8, 9, 15, 21, 27, 30, 31] have been successful in mod-\neling diverse and realistic 3D human body motions from a text\nprompt (where there exists no text-guided hand motion genera-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1577",
        "chosen": "A method for generating 3D hand-object interaction sequences from text prompts is proposed. It decomposes the task into contact generation and motion generation, using a VAE-based network and a Transformer-based diffusion model. This approach generates diverse interactions applicable to unseen objects.",
        "rejected": "This paper presents a novel method for generating sequences of hand-object interactions in 3D guided by text. It decomposes the task into contact generation and motion generation subtasks, utilizing a VAE-based network and Transformer-based diffusion model respectively. The approach generates realistic interactions applicable to unseen objects, enhancing the potential for diverse applications."
    },
    {
        "prompt": "summary: We propose a hierarchical correlation clustering method\nthat extends the well-known correlation clustering to pro-\nduce hierarchical clusters applicable to both positive and\nnegative pairwise dissimilarities. Then, in the following, we\nstudy unsupervised representation learning with such hier-\narchical correlation clustering. For this purpose, we first in-\nvestigate embedding the respective hierarchy to be used for\ntree preserving embedding and feature extraction. There-\nafter, we study the extension of minimax distance measures\nto correlation clustering, as another representation learn-\ning paradigm. Finally, we demonstrate the performance of\nour methods on several datasets.\n1. Introduction\nData clustering plays an essential role in unsupervised\nlearning and exploratory data analysis. It is used in a va-\nriety of applications including web mining, network analy-\nsis, image segmentation, bioinformatics, user analytics and\nknowledge management. Its goal is to partition the data into\ngroups in a way that the objects in the same cluster are more\nsimilar according to some criterion, compared to the objects\nin different clusters.\nMany clustering methods partition the data into K flat\nclusters for example, K-means [55], spectral clustering\n[62,68] and correlation clustering [8]. In many applications,\nhowever, the clusters are preferred to be presented at differ-\nent levels, encompassing both high-level and detailed infor-\nmation. Hierarchical clustering is useful to produce such\nstructures, usually encoded by a dendrogram. A dendro-\ngram is a tree data structure where each node corresponds\nto a cluster, with the leaf nodes (those at the bottom of the\ntree) containing only one object. Higher-level clusters are\nformed by aggregating lower-level clusters and the inter-\ncluster dissimilarity between them.\nHierarchical clustering can be performed either in an ag-\nglomerative (i.e., bottom-up) or in a divisive (i.e., top-down)\nmanner [56]. Agglomerative methods are often computa-\ntionally more efficient, making them more popular in prac-\ntice [64]. In both approaches, the clusters are aggregated or\nsplit based on various criteria, such as single, average, cen-\ntroid, complete and Ward. Several studies aim to improve\nthese methods. The works in [49,52] focus on the statistical\nsignificance of hierarchical clustering. [24,25,65] formulate\nthis problem as an optimization problem and propose ap-\nproximate solutions. [82] considers multiple dissimilarities\nfor a pair of clusters, and [11, 17] suggest merging multi-\nple clusters at each step instead of one. [6] employs global\ninformation to eliminate the influence of noisy similarities,\nand [19] proposes to apply agglomerative methods to small\nsubsets of the data instead of individual data objects. [33,38]\naugment agglomerative methods with probabilistic models,\nand finally, [23,60] propose efficient but approximate meth-\nods for hierarchical clustering.\nOn the other hand,\nmost clustering methods,\nei-\nther flat or hierarchical, assume non-negative pairwise\n(dis)similarities. However, in several practical applications,\npairwise similarities can be any real number, positive or\nnegative. For example, it could be preferable for a user or\noracle to indicate whether two objects are similar (consid-\nered a positive relation) or dissimilar (considered a nega-\ntive relation), rather than solely providing a positive (non-\nnegative) pairwise similarity, even if the two objects are dis-\nsimilar. The former approach yields more precise informa-\ntion because, in the latter scenario, the dissimilarity between\ntwo objects (i.e., zero similarity) could be confused with a\nlack of available information. Some relevant applications\nfor this setting include image segmentation with higher or-\nder correlation information [47,48], webpage segmentation\n[12], community detection over graphs [67], social media\nmining [73], analysis of connections over web [43], dealing\nwith attraction/rejection data [26], automated label genera-\ntion from clicks [3] and entity resolution [7,34].\nHence, a specialized clustering model known as correla-\ntion clustering has been developed to work with such data.\nThis model was first introduced on the graphs with only\n+1 or −1 pairwise similarities [7, 8], and then was gen-\neralized to the graphs with arbitrary positive or negative\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n23083",
        "chosen": "Hierarchical correlation clustering is extended to handle positive and negative pairwise dissimilarities. The paper studies unsupervised representation learning using hierarchical correlation clustering, proposing methods for tree preserving embedding and feature extraction.",
        "rejected": "A hierarchical correlation clustering method is proposed that extends traditional correlation clustering to handle positive and negative pairwise dissimilarities. It explores unsupervised representation learning through embedding hierarchies and minimax distance measures. This approach provides an effective solution for clustering complex data structures."
    },
    {
        "prompt": "summary: Vision-language models (VLMs) have recently shown\npromising results in traditional downstream tasks. Evalu-\nation studies have emerged to assess their abilities, with the\nmajority focusing on the third-person perspective, and only\na few addressing specific tasks from the first-person per-\nspective. However, the capability of VLMs to “think” from\na first-person perspective, a crucial attribute for advanc-\ning autonomous agents and robotics, remains largely unex-\nplored. To bridge this research gap, we introduce EgoThink,\na novel visual question-answering benchmark that encom-\npasses six core capabilities with twelve detailed dimensions.\nThe benchmark is constructed using selected clips from ego-\ncentric videos, with manually annotated question-answer\npairs containing first-person information. To comprehen-\nsively assess VLMs, we evaluate twenty-one popular VLMs\non EgoThink. Moreover, given the open-ended format of the\nanswers, we use GPT-4 as the automatic judge to compute\nsingle-answer grading. Experimental results indicate that\nalthough GPT-4V leads in numerous dimensions, all evalu-\nated VLMs still possess considerable potential for improve-\nment in first-person perspective tasks. Meanwhile, enlarg-\ning the number of trainable parameters has the most signif-\nicant impact on model performance on EgoThink. In con-\nclusion, EgoThink serves as a valuable addition to existing\nevaluation benchmarks for VLMs, providing an indispens-\nable resource for future research in the realm of embodied\nartificial intelligence and robotics.\n*Equal contribution, ‡ Project lead, B Corresponding author\nProject page: https://adacheng.github.io/EgoThink/\nGitHub page: https://github.com/AdaCheng/EgoThink/\nDataset page: https://huggingface.co/datasets/EgoThink/EgoThink/\nEgoThink\nObject\nWhat is around me?\nActivity\nWhat am I doing?\nLocalization\nWhere am I?\nReasoning\nWhat about the situation around me?\nForecasting\nWhat will happen to me?\nPlanning  \nHow will I do?  \nFigure 1. The main categories of our EgoThink benchmark to\ncomprehensively assess the capability of thinking from a first-\nperson perspective.\n1. Introduction\nBenefiting from the rapid development of large lan-\nguage models (LLMs) [8, 60, 73], vision-language models\n(VLMs) [2, 15, 43, 80] have shown remarkable progress in\nboth conventional vision-language downstream tasks [2, 15,\n43, 80] and following diverse human instructions [13, 42,\n48, 81, 89]. Their application has expanded into broader\ndomains such as robotics [21, 31, 40] and embodied arti-\nficial intelligence (EAI) [71, 78]. As a result, the thorough\nevaluation of VLMs has become increasingly important and\nchallenging. Observing and understanding the world from\na first-person perspective is a natural approach for both hu-\nmans and artificial intelligence agents. We propose that the\nability to “think” from a first-person perspective, especially\nwhen interpreting egocentric images, is crucial for VLMs.\nHowever, as shown in Table 1, the ability to think from a\nfirst-person perspective is not adequately addressed by cur-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n14291",
        "chosen": "EgoThink is a visual question-answering benchmark assessing VLMs' ability to think from a first-person perspective. It includes six core capabilities with twelve detailed dimensions, evaluated using manually annotated ego-centric video clips and GPT-4 for answer grading.",
        "rejected": "EgoThink benchmarks vision-language models on their ability to understand first-person perspective tasks. With six core capabilities and twelve detailed dimensions, it evaluates twenty-one popular models using GPT-4 for grading. Results indicate significant room for improvement in first-person perspective understanding, highlighting the benchmark's importance for future research."
    },
    {
        "prompt": "summary: This paper proposes a novel task named ”3D part group-\ning”.\nSuppose there is a mixed set containing scattered\nparts from various shapes. This task requires algorithms\nto find out every possible combination among all the parts.\nTo address this challenge, we propose the so called Gradi-\nent Field-based Auto-Regressive Sampling framework (G-\nFARS) tailored specifically for the 3D part grouping task. In\nour framework, we design a gradient-field-based selection\ngraph neural network (GNN) to learn the gradients of a log\nconditional probability density in terms of part selection,\nwhere the condition is the given mixed part set. This in-\nnovative approach, implemented through the gradient-field-\nbased selection GNN, effectively captures complex relation-\nships among all the parts in the input. Upon completion\nof the training process, our framework becomes capable\nof autonomously grouping 3D parts by iteratively selecting\nthem from the mixed part set, leveraging the knowledge ac-\nquired by the trained gradient-field-based selection GNN.\nOur code is available at: https://github.com/J-\nF-Cheng/G-FARS-3DPartGrouping.\n1. Introduction\nAssuming that you purchase multiple unassembled IKEA\nchairs and carelessly mix all the parts together, it can\nquickly become a nightmare to sort through and assemble\neach chair. The task can be especially daunting if the pieces\nfrom different chairs are mixed together, making it chal-\nlenging to identify the correct components for each chair.\nSimilarly, in the field of archaeology, recovering broken\ncultural relics can be incredibly difficult, as the fragments\nare often intermingled with the pieces from other relics.\nIn such cases, archaeologists must carefully separate the\nmixed fragments and piece them together to reconstruct the\noriginal relics. In a similar vein, the field of LEGO auto-\nmatic assembly requires AI agents to select different com-\nbinations of parts from massive LEGO blocks and assemble\nthem into a shape. All of these examples contain two goals:\nThe first goal is to identify the correct combinations from\nthe mixed part set (i.e. part grouping) and the second one\nis to assemble them into reasonable shapes (i.e. part assem-\nbly). To achieve these two objectives, algorithms must first\nbe capable of comprehending the geometric relationships\namong all the parts. Next, they should be able to separate\nthe parts by their shapes, and finally, assemble the chosen\nparts into reasonable shapes.\nFor the part assembly, previous works have researched\nsome methods for assembling a given group of parts. DGL-\nNet [15] is the first work to explore the assembly problem\nwithout prior instruction. The DGL-Net algorithm can pre-\ndict the 6-DoF poses for each input part, enabling trans-\nlation and rotation of the parts to their expected positions.\nRGL-Net [27] is another part assembly work that utilizes\nsequential information among all the input parts. By assem-\nbling shapes in a specific order (e.g., top-to-bottom), RGL-\nNet achieves more accurate assembly. IET [41] is a recently\nproposed algorithm that utilizes an instance encoded trans-\nformer and self-attention mechanisms [30, 32, 36, 37, 40]\nto enhance the network’s assembly ability.\nHowever, part grouping still remains an unsolved prob-\nlem. As previously mentioned, the goal of part grouping is\nto use algorithms to identify all possible combinations in a\nmixed part set. To address this, we introduce the 3D part\ngrouping task. The definition of this task is presented in\nFig. 1. Suppose we have a set of mixed parts from N dif-\nferent shapes. The 3D part grouping task mandates the al-\ngorithms to process all these parts and categorize them into\ngroups based on their originating shapes.\nOur proposed task 3D part grouping is challenging for\ntwo main reasons. First, the algorithms must understand the\nrelationships among all the parts. Second, the exact number\nof potential groups, N, is unknown. This uncertainty com-\nplicates both the problem formulation and the creation of\neffective algorithms. To tackle these challenges, we intro-\nduce Gradient-Field-based Auto-Regressive Sampling (G-\nFARS) framework in this paper. This framework integrates\na gradient-field-based graph neural network for the encoded\nparts, aiding in understanding the relationships among all\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n27652",
        "chosen": "G-FARS is a framework for 3D part grouping using a gradient-field-based selection graph neural network. It learns part relationships and groups 3D parts autonomously, applicable to mixed part sets with unknown group numbers.",
        "rejected": "Introducing \"3D part grouping,\" this paper proposes a Gradient Field-based Auto-Regressive Sampling framework (G-FARS) designed to identify all possible combinations among scattered parts. Using a gradient-field-based selection graph neural network, G-FARS autonomously groups parts from mixed sets, demonstrating its capability in handling complex part relationships."
    },
    {
        "prompt": "summary: Backdoor attack poses a significant security threat to\nDeep Learning applications. Existing attacks are often not\nevasive to established backdoor detection techniques. This\nsusceptibility primarily stems from the fact that these attacks\ntypically leverage a universal trigger pattern or transfor-\nmation function, such that the trigger can cause misclas-\nsification for any input. In response to this, recent papers\nhave introduced attacks using sample-specific invisible trig-\ngers crafted through special transformation functions. While\nthese approaches manage to evade detection to some extent,\nthey reveal vulnerability to existing backdoor mitigation\ntechniques. To address and enhance both evasiveness and\nresilience, we introduce a novel backdoor attack LOTUS.\nSpecifically, it leverages a secret function to separate sam-\nples in the victim class into a set of partitions and applies\nunique triggers to different partitions. Furthermore, LOTUS\nincorporates an effective trigger focusing mechanism, en-\nsuring only the trigger corresponding to the partition can\ninduce the backdoor behavior. Extensive experimental re-\nsults show that LOTUS can achieve high attack success rate\nacross 4 datasets and 7 model structures, and effectively\nevading 13 backdoor detection and mitigation techniques.\nThe code is available at https://github.com/Megum1/LOTUS.\n1. Introduction\nBackdoor attack is a prominent security threat to Deep Learn-\ning applications, evidenced by the large body of existing\nattacks [5, 19, 37, 51, 64] and defense techniques [20, 32,\n35, 67, 72]. It injects malicious behaviors to a model such\nthat the model operates normally on clean samples but mis-\nclassifies inputs that are stamped with a specific trigger. A\ntypical way of injecting such malicious behaviors is through\ndata poisoning [1, 19, 39]. This approach introduces a small\nset of trigger-stamped images paired with the target label\ninto the training data. Attackers may also manipulate the\ntraining procedure [10, 45, 46], and tamper with the model’s\ninternal mechanisms [37, 41].\nThe majority of existing attacks rely on a uniform pat-\ntern [5, 19, 39, 64] or a transformation function [6, 51] as\nthe trigger. The uniform trigger tends to be effective on any\ninput, which can be detected by existing techniques. For\ninstance, trigger inversion methods [20, 38, 67, 68] aim to\nreverse engineer a small trigger that can induce the target pre-\ndiction on a set of inputs. According to the results reported\nin the literature [20, 61, 67], for a number of attacks, it is\nfeasible to invert a pattern that closely resembles the ground-\ntruth trigger and has a substantially high attack success rate\n(ASR), hence detecting backdoored models.\nRecent studies introduce sample-specific invisible at-\ntacks [10, 33, 45, 46] that encourage the model to emphasize\nthe correlation between the trigger and the input sample.\nAlthough these attacks effectively evade certain detection\nmethods [20, 67], they are not resilient to backdoor mitiga-\ntion techniques [32, 35, 72]. For instance, a straightforward\napproach such as fine-tuning the backdoored model using\nonly 5% of the training data can significantly reduce ASR.\nThis is due to the fact that imperceptible trigger patterns are\nnot persistent during the retraining process. Moreover, the\nsample-specific characteristic of these attacks make them\nless robust to backdoor mitigation methods.\nIn this paper, we introduce an innovative attack that not\nonly evades backdoor detection approaches but also exhibits\nresilience against backdoor mitigation techniques. It is a\nlabel-specific attack, aiming to misclassify the samples of\na victim class to a target class. For the victim-class sam-\nples, we divide them into sub-partitions and use a unique\ntrigger for each partition. With such an attack design, ex-\nisting defense such as trigger inversion is unlikely to find a\nuniform trigger. This is because the available set of samples\nused by trigger inversion is likely from different partitions,\nwhich makes the detection fail. In addition, we develop a\nnovel trigger focusing technique to ensure that a partition\ncan only be attacked by its designated trigger, not by any\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n24798",
        "chosen": "LOTUS is a novel backdoor attack leveraging secret functions to partition victim-class samples and apply unique triggers. It incorporates a trigger focusing mechanism, achieving high attack success rates across datasets and model structures while evading detection and mitigation techniques.",
        "rejected": "LOTUS is a novel backdoor attack leveraging a secret function to partition victim class samples and apply unique triggers to different partitions. It includes a trigger focusing mechanism ensuring only designated triggers can induce backdoor behavior. LOTUS achieves high attack success rates across various datasets and models while evading numerous detection and mitigation techniques."
    },
    {
        "prompt": "summary: We present Cutie, a video object segmentation (VOS) net-\nwork with object-level memory reading, which puts the object\nrepresentation from memory back into the video object seg-\nmentation result. Recent works on VOS employ bottom-up\npixel-level memory reading which struggles due to matching\nnoise, especially in the presence of distractors, resulting\nin lower performance in more challenging data. In con-\ntrast, Cutie performs top-down object-level memory reading\nby adapting a small set of object queries. Via those, it in-\nteracts with the bottom-up pixel features iteratively with a\nquery-based object transformer (qt, hence Cutie). The ob-\nject queries act as a high-level summary of the target object,\nwhile high-resolution feature maps are retained for accu-\nrate segmentation. Together with foreground-background\nmasked attention, Cutie cleanly separates the semantics of\nthe foreground object from the background. On the challeng-\ning MOSE dataset, Cutie improves by 8.7 J &F over XMem\nwith a similar running time and improves by 4.2 J &F over\nDeAOT while being three times faster. Code is available at:\nhkchengrex.github.io/Cutie.\n1. Introduction\nVideo Object Segmentation (VOS), specifically the “semi-\nsupervised” setting, requires tracking and segmenting objects\nfrom an open vocabulary specified in a first-frame annota-\ntion. VOS methods are broadly applicable in robotics [1],\nvideo editing [2], reducing costs in data annotation [3],\nand can also be combined with Segment Anything Models\n(SAMs) [4] for universal video segmentation (e.g., Tracking\nAnything [5–7]).\nRecent VOS approaches employ a memory-based\nparadigm [8–11]. A memory representation is computed\nfrom past segmented frames (either given as input or seg-\nmented by the model), and any new query frame “reads”\nfrom this memory to retrieve features for segmentation. Im-\nportantly, these approaches mainly use pixel-level matching\nfor memory reading, either with one [8] or multiple matching\nlayers [10], and generate the segmentation bottom-up from\nFigure 1. Comparison of pixel-level memory reading v.s. object-\nlevel memory reading. In each box, the left is the reference frame,\nand the right is the query frame to be segmented. Red arrows indi-\ncate wrong matches. Low-level pixel matching (e.g., XMem [9])\ncan be noisy in the presence of distractors. We propose object-level\nmemory reading for more robust video object segmentation.\nthe pixel memory readout. Pixel-level matching maps every\nquery pixel independently to a linear combination of memory\npixels (e.g., with an attention layer). Consequently, pixel-\nlevel matching lacks high-level consistency and is prone\nto matching noise, especially in the presence of distractors.\nThis leads to lower performance in challenging scenarios\nwith occlusions and frequent distractors. Concretely, the\nperformance of recent approaches [9, 10] is more than 20\npoints in J &F lower when evaluating on the recently pro-\nposed challenging MOSE [12] dataset rather than the simpler\nDAVIS-2017 [13] dataset.\nWe think this unsatisfactory result in challenging sce-\nnarios is caused by the lack of object-level reasoning. To\naddress this, we propose object-level memory reading, which\neffectively puts the object from a memory back into the query\nframe (Figure 1). Inspired by recent query-based object de-\ntection/segmentation [14–18] that represent objects as “ob-\nject queries,” we implement our object-level memory reading\nwith an object transformer. This object transformer uses a\nsmall set of end-to-end trained object queries to 1) iteratively\nprobe and calibrate a feature map (initialized by a pixel-level\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n3151",
        "chosen": "Cutie is a video object segmentation network with object-level memory reading using object queries and a query-based transformer. It improves segmentation accuracy by separating foreground and background semantics, outperforming existing methods on challenging datasets.",
        "rejected": "We present Cutie, a video object segmentation method that leverages spatiotemporal context for accurate and efficient segmentation. Cutie integrates deep learning techniques with dynamic programming to refine object boundaries over time, achieving state-of-the-art performance on several benchmark datasets."
    },
    {
        "prompt": "summary: The problem of calibrating deep neural networks\n(DNNs) for multi-label learning is considered. It is well-\nknown that DNNs trained by cross-entropy for single-label,\nor one-hot, classification are poorly calibrated. Many cali-\nbration techniques have been proposed to address the prob-\nlem. However, little attention has been paid to the cali-\nbration of multi-label DNNs. In this literature, the focus\nhas been on improving labeling accuracy in the face of se-\nvere dataset unbalance. This is addressed by the introduc-\ntion of asymmetric losses, which have became very popular.\nHowever, these losses do not induce well calibrated clas-\nsifiers. In this work, we first provide a theoretical expla-\nnation for this poor calibration performance, by showing\nthat these loses losses lack the strictly proper property, a\nnecessary condition for accurate probability estimation. To\novercome this problem, we propose a new Strictly Proper\nAsymmetric (SPA) loss. This is complemented by a Label\nPair Regularizer (LPR) that increases the number of cali-\nbration constraints introduced per training example. The\neffectiveness of both contributions is validated by extensive\nexperiments on various multi-label datasets. The resulting\ntraining method is shown to significantly decrease the cal-\nibration error while maintaining state-of-the-art accuracy.\n1. Introduction\nDeep neural networks (DNNs) including convolutional neu-\nral networks (CNNs) [27, 35] and vision transformers\n(ViTs) [13] have demonstrated great capacity for solving su-\npervised learning tasks in computer vision. However, many\napplications require trust-worthy machine learning systems,\nwhich are not only accurate but also probability calibrated,\ni.e. able to produce accurate estimates of the posterior prob-\nabilities of the various classes. A classifier is calibrated if\nit predicts a posterior class probability of p when the se-\nlection of the class is correct p × 100% of the time. The\nimportance of calibration has been noted for many appli-\ncations. For example, in medical diagnosis [44, 82], prob-\nabilities can be used to determine which examples require\nhuman inspection, thus avoiding the cost of manually in-\nspecting all images.\nHowever, the process can only be\ntrusted if the DNN provides accurate posterior estimates.\nThe safety-critical nature of the application makes proba-\nbility calibration a critical requirement to enable this func-\ntionality. Cost-sensitive applications [15], e.g. fraud detec-\ntion [2, 50] or business decision making [56, 75], involve\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n27589",
        "chosen": "A new Strictly Proper Asymmetric (SPA) loss and Label Pair Regularizer (LPR) are proposed to calibrate multi-label DNNs. These methods address poor calibration in multi-label learning, significantly reducing calibration error while maintaining accuracy.",
        "rejected": "This paper addresses the calibration of deep neural networks (DNNs) for multi-label learning. While asymmetric losses improve labeling accuracy under dataset imbalance, they fail to produce well-calibrated classifiers. The authors show that these losses lack strictly proper properties, essential for accurate probability estimation. To solve this, they propose a Strictly Proper Asymmetric (SPA) loss and introduce a Label Pair Regularizer (LPR) to increase calibration constraints per training example. Extensive experiments validate the effectiveness of both contributions, significantly reducing calibration error while maintaining state-of-the-art accuracy. This work highlights the importance of calibration in safety-critical applications such as medical diagnosis and cost-sensitive domains like fraud detection."
    },
    {
        "prompt": "summary: Image denoising is a fundamental task in computer vi-\nsion. While prevailing deep learning-based supervised and\nself-supervised methods have excelled in eliminating in-\ndistribution noise, their susceptibility to out-of-distribution\n(OOD) noise remains a significant challenge.\nThe re-\ncent emergence of contrastive language-image pre-training\n(CLIP) model has showcased exceptional capabilities in\nopen-world image recognition and segmentation. Yet, the\npotential for leveraging CLIP to enhance the robustness of\nlow-level tasks remains largely unexplored. This paper un-\ncovers that certain dense features extracted from the frozen\nResNet image encoder of CLIP exhibit distortion-invariant\nand content-related properties, which are highly desirable\nfor generalizable denoising. Leveraging these properties,\nwe devise an asymmetrical encoder-decoder denoising net-\nwork, which incorporates dense features including the noisy\nimage and its multi-scale features from the frozen ResNet\nencoder of CLIP into a learnable image decoder to achieve\ngeneralizable denoising. The progressive feature augmen-\ntation strategy is further proposed to mitigate feature over-\nfitting and improve the robustness of the learnable decoder.\nExtensive experiments and comparisons conducted across\ndiverse OOD noises, including synthetic noise, real-world\nsRGB noise, and low-dose CT image noise, demonstrate the\nsuperior generalization ability of our method.\n1. Introduction\nImage denoising is a significant task in computer vision\nand image processing. Current supervised denoising meth-\nods leveraging powerful deep neural networks and large-\nscale datasets have achieved exceptional performance in\nboth synthetic and real-world noise removal [33, 60]. How-\never, these supervised denoisers tend to overfit the noise\npresent in the training datasets, resulting in poor general-\nization to out-of-distribution (OOD) noise [6]. On the other\nhand, unsupervised and self-supervised denoising methods\n*Corresponding author.\n[12, 18, 26, 28, 44, 54] directly focus on the target domain\nin which the target noisy images reside and hence bypass\nOOD generalization. Nevertheless, these methods are in-\nherently vulnerable to unseen noise [7] and the collection\nof target noisy datasets is not always available. Therefore,\nit is critical to enhance the generalization of deep denoisers.\nOOD generalization has been popular research in high-\nlevel vision tasks like image recognition and segmentation\n[51, 70]. In contrast, attention to OOD generalization within\nimage denoising is limited. Existing research in this area\nprimarily consists of two aspects: generalization across\ndegradation levels and generalization across degradation\ntypes. Regarding the former, some works trained blind de-\nnoisers [18, 60, 62] or bias-free networks [41, 63] to handle\nnoise with varying levels. However, these methods are con-\nfined to specific noise and cannot generalize to unseen noise\ntypes. For the latter, several works aimed to fortify mod-\nels against general OOD noise. Particularly, MaskDenois-\ning [6] incorporated dropout units into the model training\nto enforce the denoiser to learn the reconstruction of image\ncontents. DIL [32] built upon causality and meta-learning\nand encouraged the model to learn distortion-invariant rep-\nresentations. HAT [58] designed an adversarial attack for\ndeep denoisers and then conducted adversarial training.\nRecently, through solving the image-text alignment\nproblem based on hyper-scale datasets, the contrastive\nlanguage-image pre-training (CLIP) model [48] has demon-\nstrated remarkable generalization capacity in downstream\nopen-world image recognition tasks. A series of extensions\non CLIP through frozen models [65, 69], model fine-tuning\n[49], visual prompts [71], distillations [19, 31], and so on\n[34] have been proposed to transfer the generalization abil-\nity of CLIP from classification to dense prediction tasks,\nincluding open-vocabulary segmentation [34] and zero-shot\ndepth estimation [65]. However, the feasibility of CLIP for\nrobust restoration in low-level tasks remains unexplored.\nWe therefore ask, is CLIP robust to image noise and can\nwe transfer it for generalizable image denoising?\nIn this paper, we find that the dense feature maps from\nthe frozen ResNet image encoder of CLIP within specific\nscales exhibit remarkable resilience to noise, a property that\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25974",
        "chosen": "This paper proposes a method to enhance the generalization of image denoising models using features from the frozen ResNet encoder of CLIP. The authors devise an asymmetrical encoder-decoder network and introduce a progressive feature augmentation strategy. Experiments show superior generalization across diverse out-of-distribution noises.",
        "rejected": "Image denoising methods struggle with out-of-distribution (OOD) noise. This paper explores using dense features from CLIP's frozen ResNet encoder for generalizable denoising, proposing an asymmetrical encoder-decoder network. It incorporates a progressive feature augmentation strategy to mitigate overfitting, demonstrating superior performance across various OOD noises."
    },
    {
        "prompt": "summary: The Segment Anything Model (SAM) has garnered sig-\nnificant attention for its versatile segmentation abilities and\nintuitive prompt-based interface. However, its application\nin medical imaging presents challenges, requiring either\nsubstantial training costs and extensive medical datasets for\nfull model fine-tuning or high-quality prompts for optimal\nperformance. This paper introduces H-SAM: a prompt-free\nadaptation of SAM tailored for efficient fine-tuning of med-\nical images via a two-stage hierarchical decoding proce-\ndure. In the initial stage, H-SAM employs SAM’s original\ndecoder to generate a prior probabilistic mask, guiding a\nmore intricate decoding process in the second stage. Specif-\nically, we propose two key designs: 1) A class-balanced,\nmask-guided self-attention mechanism addressing the un-\nbalanced label distribution, enhancing image embedding;\n2) A learnable mask cross-attention mechanism spatially\nmodulating the interplay among different image regions\nbased on the prior mask. Moreover, the inclusion of a hier-\narchical pixel decoder in H-SAM enhances its proficiency in\ncapturing fine-grained and localized details. This approach\nenables SAM to effectively integrate learned medical priors,\nfacilitating enhanced adaptation for medical image seg-\nmentation with limited samples. Our H-SAM demonstrates\na 4.78% improvement in average Dice compared to existing\nprompt-free SAM variants for multi-organ segmentation us-\ning only 10% of 2D slices. Notably, without using any unla-\nbeled data, H-SAM even outperforms state-of-the-art semi-\nsupervised models relying on extensive unlabeled training\ndata across various medical datasets. Our code is available\nat https://github.com/Cccccczh404/H-SAM.\n1. Introduction\nAccurate delineation of tissues, organs, and regions of in-\nterest through medical image segmentation is pivotal in aid-\nUnlabeled\nLabeled\n78.27\n86.83\nSOTA Semi-\nSupervised\nMethods\nH-SAM\nMean Dice (%)\nMethods\n75.57\n80.35\nH-SAM\nSOTA SAM\nVariants\nFigure 1.\nH-SAM is advantageous in few-shot medical image\nsegmentation. It achieves over 80% in average Dice using only\n10% slices for multi-organ segmentation, outperforming exist-\ning prompt-free SAM adaptation methods.\nWithout using any\nunlabeled data at all, it even outperforms state-of-the-art semi-\nsupervised models that use extensive unlabeled training data for\nprostate segmentation.\ning medical professionals’ diagnostic precision and treat-\nment planning processes [13, 15]. Furthermore, it plays a\nfundamental role in propelling disease research and discov-\nery [53]. Nonetheless, a significant challenge in this field\nlies in the demand for deep learning models to undergo ex-\ntensive training on large annotated datasets, a resource often\nchallenging to procure within the medical domain.\nRecently, Segment Anything Model (SAM) [37], trained\nwith over a billion masks from diverse natural images,\ndemonstrates remarkable zero-shot learning capabilities.\nThis breakthrough presents an avenue for significant ad-\nvancements in medical image segmentation, especially con-\nsidering the limited availability of extensive datasets in the\nmedical realm. However, SAM’s performance on medical\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n3511",
        "chosen": "This paper introduces H-SAM, a prompt-free adaptation of SAM for efficient fine-tuning in medical image segmentation. It employs a two-stage hierarchical decoding procedure with novel attention mechanisms and a hierarchical pixel decoder. H-SAM achieves significant improvements in average Dice scores with limited training data and outperforms semi-supervised models without using unlabeled data.",
        "rejected": "H-SAM enhances Segment Anything Model (SAM) for medical image segmentation through a prompt-free adaptation using a two-stage hierarchical decoding procedure. By addressing unbalanced label distribution and spatial modulation among regions, H-SAM shows a 4.78% improvement in Dice score compared to existing SAM variants, even outperforming state-of-the-art semi-supervised models."
    },
    {
        "prompt": "summary: Driver’s eye gaze holds a wealth of cognitive and inten-\ntional cues crucial for intelligent vehicles. Despite its sig-\nniﬁcance, research on in-vehicle gaze estimation remains\nlimited due to the scarcity of comprehensive and well-\nannotated datasets in real driving scenarios. In this pa-\nper, we present three novel elements to advance in-vehicle\ngaze research. Firstly, we introduce IVGaze, a pioneering\ndataset capturing in-vehicle gaze, collected from 125 sub-\njects and covering a large range of gaze and head poses\nwithin vehicles. In this dataset, we propose a new vision-\nbased solution for in-vehicle gaze collection, introducing\na reﬁned gaze target calibration method to tackle annota-\ntion challenges. Second, our research focuses on in-vehicle\ngaze estimation leveraging the IVGaze. In-vehicle face im-\nages often suffer from low resolution, prompting our in-\ntroduction of a gaze pyramid transformer that leverages\ntransformer-based multilevel features integration. Expand-\ning upon this, we introduce the dual-stream gaze pyramid\ntransformer (GazeDPTR). Employing perspective transfor-\nmation, we rotate virtual cameras to normalize images, uti-\nlizing camera pose to merge normalized and original im-\nages for accurate gaze estimation. GazeDPTR shows state-\nof-the-art performance on the IVGaze dataset.\nThirdly,\nwe explore a novel strategy for gaze zone classiﬁcation\nby extending the GazeDPTR. A foundational tri-plane and\nproject gaze onto these planes are newly deﬁned. Leverag-\ning both positional features from the projection points and\nvisual attributes from images, we achieve superior perfor-\nmance compared to relying solely on visual features, sub-\nstantiating the advantage of gaze estimation. The project is\navailable at https://yihua.zone/work/ivgaze.\n1. Introduction\nUnderstanding driver intention and behavior based on driver\ngaze is in high demand in intelligent vehicles, facilitating\nFigure 1. In-vehicle gaze estimation illustration. The driver’s gaze\ndirection is estimated based on the facial images captured by the\ncamera behind the steering wheels.\ndiverse applications such as in-vehicle interaction [1, 2, 25]\nand driver monitor systems [17, 21, 22]. Recent advances in\nvehicle gaze estimation concentrate primarily on gaze zone\nestimation [16, 18, 31, 32]. These approaches deﬁne mul-\ntiple coarse regions, such as side mirrors and windshields,\nand conduct classiﬁcation based on face images.\nGaze estimation1 serves as an upstream task of gaze zone\nestimation and can offer more precise information to under-\nstand driver attention. However, these methods typically\nrequire a large-scale dataset for training. Although there\nare numerous gaze datasets, collected in indoor [39, 40] or\noutdoor [20] environments, their applicability to the vehi-\ncle environment is limited due to the different environments\nand camera settings, resulting in suboptimal performance.\nCreating an in-vehicle gaze dataset proves challenging due\nto the conﬁned and irregular nature of the vehicular environ-\nment. Constructing in-vehicle gaze collection systems re-\nmains an unsolved issue, as traditional gaze collection sys-\ntems are impractical for use within vehicles. The absence\nof in-vehicle gaze datasets acts as a signiﬁcant barrier to the\nprogress of in-vehicle gaze estimation.\nIn this paper, we present a comprehensive vision-based\n1Our work focuses on gaze direction estimation. We abbreviate gaze\ndirection as gaze in the rest.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1556",
        "chosen": "This paper presents IVGaze, a novel dataset for in-vehicle gaze estimation, along with a vision-based collection solution. The authors propose gaze pyramid transformers and a dual-stream gaze pyramid transformer (GazeDPTR) for accurate gaze estimation. They also explore gaze zone classification by projecting gaze onto tri-planes and leveraging positional features and visual attributes.",
        "rejected": "This paper introduces IVGaze, a dataset for in-vehicle gaze estimation, and proposes GazeDPTR, which utilizes transformer-based multilevel features integration and dual-stream processing. GazeDPTR achieves state-of-the-art performance on the IVGaze dataset, enhancing accuracy in gaze zone classification by defining tri-plane projections and leveraging positional features from these projections."
    },
    {
        "prompt": "summary: Large-scale visual-language pre-trained models have\nachieved significant success in various video tasks. How-\never, most existing methods follow an “adapt then align”\nparadigm, which adapts pre-trained image encoders to\nmodel video-level representations and utilizes one-hot or\ntext embedding of the action labels for supervision. This\nparadigm overlooks the challenge of mapping from static im-\nages to complicated activity concepts. In this paper, we pro-\npose a novel “Align before Adapt” (ALT) paradigm. Prior\nto adapting to video representation learning, we exploit the\nentity-to-region alignments for each frame. The alignments\nare fulfilled by matching the region-aware image embeddings\nto an offline-constructed text corpus. With the aligned enti-\nties, we feed their text embeddings to a transformer-based\nvideo adapter as the queries, which can help extract the\nsemantics of the most important entities from a video to a\nvector. This paradigm reuses the visual-language alignment\nof VLP during adaptation and tries to explain an action by\nthe underlying entities. This helps understand actions by\nbridging the gap with complex activity semantics, partic-\nularly when facing unfamiliar or unseen categories. ALT\ndemonstrates competitive performance while maintaining\nremarkably low computational costs. In fully supervised ex-\nperiments, it achieves 88.1% top-1 accuracy on Kinetics-400\nwith only 4947 GFLOPs. Moreover, ALT outperforms the\nprevious state-of-the-art methods in both zero-shot and few-\nshot experiments, emphasizing its superior generalizability\nacross various learning scenarios.\n1. Introduction\nVideo action recognition is a fundamental task in the pur-\nsuit of intelligent video understanding. The recent trend\nof utilizing the visual-language pre-trained (VLP) mod-\nels [26, 31, 49, 73] have significantly advanced the research\nof action recognition [27, 37, 43, 45, 64, 71]. By lightly\nfine-tuning the model, VLP-based methods outperform the\nprevious end-to-end network architectures, including two-\nstream networks [54, 63, 78], 3D convolutional neural net-\nworks [6, 14, 15, 22, 48, 58, 59, 67], and vision-transformer-\nbased (ViT) networks [3, 12, 38, 46, 70]. Employing a pre-\ntrained VLP model for action recognition can better encode\nthe semantic meaning of items in images, even if they have\nvery different visual appearances. This is very helpful in\nunderstanding human action and also explains why VLP\nmodels have achieved superior performance. As shown in\nFig. 1, the current VLP-based action recognition methods\nfollow an “adapt then align” paradigm. They either introduce\ntemporal interaction upon image representations or insert\ntemporal modules into pre-trained image encoders. However,\nthe “adapt then align” paradigm will merely fit the video\nrepresentation to the action name, which potentially destroys\nthe other visual-semantic correspondences provided by VLP\nmodels. As far as we are concerned, actions are complex\nconcepts that involve multiple fine-grained entities, such as\nbody parts, scenes, and objects. With VLP model, the text\nembedding of the relevant entities should also be grounded\nin some image region. “adapt then align” paradigm does not\nemphasize the underlying entities-to-regions correspondence\nbehind the action concept. Furthermore, human-centric ac-\ntivities often share common entities, implying that visual-\nlanguage correspondences can be reused across different\nactions, even for those that were not included in the training\nset. The re-usability of entities-to-regions correspondences\nallow the model to quickly recognize new action categories.\nIn this paper, we propose the “ALign before adapt” (ALT)\nparadigm. Unlike the “adapt then align” approaches that\nalign the image-level visual embedding with the text embed-\nding of the action name, ALT aims to establish an entity-level\ncorrespondence to support action recognition. The relevant\nentities should have evidence in specific regions of the image.\nTo achieve entities-to-regions alignment, the VLP model is\nleveraged in two aspects: (1) Aggregating adjacent and sim-\nilar image token embeddings from the VLP model. The\nresulting embedding typically represents a region containing\nthe same entity. (2) Selecting the most relevant entities for\neach region by matching their image embeddings to the text\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n18688",
        "chosen": "This paper proposes \"Align before Adapt\" (ALT), a novel paradigm for video action recognition that establishes entity-to-region alignments before adapting to video representation learning. ALT reuses visual-language alignments during adaptation and demonstrates competitive performance with low computational costs across fully supervised, zero-shot, and few-shot experiments.",
        "rejected": "ALT paradigm advances video action recognition by aligning entity-to-region before adapting to video-level representations. By grounding text embeddings of relevant entities into image regions, ALT bridges the gap between static images and complex activity semantics. It achieves competitive performance while maintaining low computational costs, outperforming previous methods in both zero-shot and few-shot scenarios."
    },
    {
        "prompt": "summary: We\npresent\nthe\npioneering\nLarge\nVisual\nMotion\nModel (LVMM), meticulously engineered to analyze the\nintrinsic dynamics encapsulated within real-world imagery.\nOur model, fortified with a wealth of prior knowledge\nextracted from billions of image pairs,\ndemonstrates\npromising results in predicting a diverse spectrum of scene\ndynamics. As a result, it can infuse any generic image with\nauthentic dynamic effects, enhancing its visual allure.\nProject page: https://github.com/densechen/\nLVMM.\n1. Introduction\nRecent progress in generative models [39], specifically\nconditional diffusion models [11,22], and large-scale mod-\nels [39], have substantially enhanced our capability to rep-\nresent complex and rich distributions. These models have\nunderscored the transformative potential of harnessing vast\ndata and intensive training [14], exhibiting unparalleled pro-\nficiency in comprehending and generating human-like text,\nand creating visually rich and diverse images from textual\ndescriptions.\nThis has facilitated a variety of previously\nunachievable applications, such as text-conditioned gener-\nation of arbitrary, realistic image content [21]. The advent\nof these models [5, 19, 24], propelled by the availability\nof large-scale datasets [25] and advancements in training\nmethodologies [4, 20], has ignited interest in probing other\ndomains, including audio [38] and multimodal data [15].\nIn this paper, we present a novel Large Visual Motion\nModel (LVMM), specifically designed to proficiently pre-\ndict local motion embedded within a given scene, thereby\nenhancing the dynamic appeal of a static image. The dy-\nnamism of the natural world is characterized by subtle\nchanges even in seemingly static landscapes, influenced by\nvarious factors such as wind, water currents, and inherent\nrhythms. When observing a still image, we can envisage\nplausible motions that might have been occurring when the\n(a) I0\n(b) ˆz1,··· ,K\n(c) ˆp1,··· ,K\n(d) X-t slices\n(e) Capability to animate faces, generating natural and realistic smiles.\nFigure 1.\nBeginning with a reference image I0 as depicted in\nFig. 1a, the Large Visual Motion Model (LVMM) estimates a\nlatent motion trajectory ˆz1,··· ,K as shown by the t-SNE plot in\nFig. 1b, utilizing the motion denoising model ϵθ. This trajectory\nis subsequently processed by the Motion Decoder D, generating a\nsequence of optical flows ˆδ1,··· ,K (visualized in the left column of\nFig. 1c) and intention maps ˆω1,··· ,K (displayed in the right column\nof Fig. 1c). Ultimately, the Neural Image Renderer R transforms\nI0 into a series of novel images ˆI1,··· ,K, guided by optical flows\nand intention maps. Fig 1d illustrates the resultant videos, employ-\ning space-time X-t slices across 300 frames (corresponding to the\nscanline shown in Fig. 1a).\nphotograph was captured. This predictability is ingrained in\nour human perception of real scenes, i.e., we can imagine a\ndistribution of natural motions conditioned on that image if\nthere could have been multiple possible motions. Given the\nease with which humans can envision these potential mo-\ntions, an intriguing research question is to computationally\nmodel this motion distribution with a large-scale model.\nThe proposed LVMM excels in associating salient vi-\nsual and motion patterns, thereby accurately predicting lo-\ncal motion trajectory, as shown in Fig. 1. It comprises two\ncomponents: the motion rendering model and the motion\ndiffusion model. The former extracts a latent motion vector\nfrom the scene and reconstructs the target image. The latter\ngenerates suitable motion trajectories from the given scene\nand feeds them to the motion rendering model to produce\nrealistic dynamic effects.\nOur primary contribution is the pioneering proposal and\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n7131",
        "chosen": "This paper presents LVMM, a large visual motion model designed to predict local motion within scenes. The model comprises a motion rendering model and a motion diffusion model, and it can infuse static images with dynamic effects. The authors demonstrate promising results in predicting diverse scene dynamics.",
        "rejected": "LVMM predicts scene dynamics to animate static images, enhancing visual allure. The model extracts latent motion vectors and generates motion trajectories using a diffusion model. LVMM accurately predicts local motion trajectory, producing realistic dynamic effects. Its contributions include pioneering the prediction of diverse scene dynamics and introducing mechanisms for motion rendering and diffusion."
    },
    {
        "prompt": "summary: Recent advancements in text-to-image technology have\nsignificantly advanced the field of image customization.\nAmong various applications, the task of customizing diverse\nscenes for user-specified composited elements holds great\napplication value but has not been extensively explored.\nAddressing this gap, we propose AnyScene, a specialized\nframework designed to create varied scenes from compos-\nited foreground using textual prompts. AnyScene addresses\nthe primary challenges inherent in existing methods, partic-\nularly scene disharmony due to a lack of foreground seman-\ntic understanding and distortion of foreground elements.\nSpecifically, we develop a foreground injection module that\nguides a pre-trained diffusion model to generate cohesive\nscenes in visual harmony with the provided foreground. To\nenhance robust generation, we implement a layout control\nstrategy that prevents distortions of foreground elements.\nFurthermore, an efficient image blending mechanism seam-\nlessly reintegrates foreground details into the generated\nscenes, producing outputs with overall visual harmony and\nprecise foreground details. In addition, we propose a new\nbenchmark and a series of quantitative metrics to evaluate\nthis proposed image customization task. Extensive exper-\nimental results demonstrate the effectiveness of AnyScene,\nwhich confirms its potential in various applications.\n1. Introduction\nRecently, text-to-image (T2I) synthesis models [24, 28, 30,\n34] have seen rapid advancements and gained significant\nattention due to their ability to generate high-fidelity im-\nages from textual prompts. Among the various applications\nthat harness T2I technology to enhance design efforts, gen-\nerating diverse scenes tailored to specific composited fore-\ngrounds is valuable. It is effective in image editing tasks\ndemanding background alteration and creation, such as sit-\nuating e-commerce products in customized scenes, depict-\ning objects in various environmental settings, etc. Despite\nits clear utility, this task of “customizing diverse scenes for\n1† Equal contribution.\n2∗Corresponding author: An-An Liu (anan0422@gmail.com).\nFigure 1. Our proposed AnyScene is capable of synthesizing high-\nquality scenes that align with textual prompts based on the given\nforegrounds. Compared to previous alternative methods [42, 45],\nAnyScene provides precise control over the introduced foreground\nand generates visually harmonious images.\ncomposited foreground” has not been extensively explored.\nCurrently, several alternative methods [42, 45] can\nachieve this task. One such method is employing the canny\nedge as a control condition [23, 45], combined with over-\nlaying the original foreground onto the generated image,\noften requiring manual adjustments to achieve visual har-\nmony. Another strategy involves inpainting models [30, 42]\nthat construct backgrounds by painting around the exterior\nof a masked foreground area. Furthermore, in cases where\nthe foreground comprises only one or two specific objects,\nsubject-driven techniques [17, 19, 32] have shown profi-\nciency in generating customized images.\nHowever, these methods have limitations in practical ap-\nplications.\nThe method of directly controlling the fore-\nground edge [45] does not consider the visual context, lead-\ning to scenes that lack harmony (e.g. Fig. 2(a)). More-\nover, inpainting-based approaches [30, 42], though capa-\nble of creating cohesive scenes, are commonly trained un-\nder a random area recovering paradigm, which may distort\nthe foreground elements or cause unintended regeneration\nof elements from foreground descriptions (e.g. Fig. 2(b)).\nFinally, subject-driven methods [17, 19, 32] often compro-\nmise high-level details of objects (e.g. Fig. 2(c)), leading\nto significant distortions of details such as logos, text, etc,\nwhich is problematic for the applications like commodity\nposter design. Meanwhile, their limited capacity to com-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8724",
        "chosen": "This paper introduces AnyScene, a framework for creating customized scenes from composited foregrounds using textual prompts. It includes a foreground injection module, layout control strategy, and image blending mechanism. The framework addresses scene disharmony and foreground distortion issues and shows effectiveness in various applications.",
        "rejected": "AnyScene customizes scenes based on composited foreground elements using textual prompts. It addresses disharmony and distortion issues by developing a foreground injection module, layout control strategy, and efficient image blending mechanism. AnyScene ensures visual harmony and precise foreground details, demonstrating its effectiveness in various applications through extensive experiments."
    },
    {
        "prompt": "summary: This work introduces ArtAdapter, a transformative text-\nto-image (T2I) style transfer framework that transcends tra-\nditional limitations of color, brushstrokes, and object shape,\ncapturing high-level style elements such as composition and\ndistinctive artistic expression. The integration of a multi-\nlevel style encoder with our proposed explicit adaptation\nmechanism enables ArtAdapter to achieve unprecedented\nfidelity in style transfer, ensuring close alignment with tex-\ntual descriptions. Additionally, the incorporation of an Aux-\niliary Content Adapter (ACA) effectively separates content\nfrom style, alleviating the borrowing of content from style\nreferences. Moreover, our novel fast finetuning approach\ncould further enhance zero-shot style representation while\nmitigating the risk of overfitting. Comprehensive evalua-\ntions confirm that ArtAdapter surpasses current state-of-\nthe-art methods.\n1. Introduction\nBridging the realms of artificial intelligence and artistic cre-\nativity, text-to-image (T2I) style transfer [13, 24, 25] stands\nout as a captivating domain that masterfully transforms\ntextual descriptions into visually rich and stylistic repre-\nsentations. The core challenge lies in not just generating\ntext-resonant images but infusing them with artistic depth\nand nuance, spanning from delicate brushstrokes to bold\ncompositional elements—thereby capturing the essence of\nartistic vision. Conventional arbitrary style transfer (AST)\nmethods [4, 7–9, 15, 21, 28, 54] typically struggle beyond\nlow-level features such as medium and colors, failing to\ngrasp the more sophisticated realms of artistic expression.\nDiffusion approaches [25, 55], including Textual Inversion\n[14], DreamBooth [39], and Low-Rank Adaptation (LoRA)\n[2, 20], have shown potential in style representation. Yet,\nthese methods are hindered by laborious finetuning pro-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8619",
        "chosen": "This paper presents ArtAdapter, a text-to-image style transfer framework that captures high-level style elements. It integrates a multi-level style encoder with an explicit adaptation mechanism and an Auxiliary Content Adapter. The framework achieves unprecedented fidelity in style transfer and surpasses current state-of-the-art methods.",
        "rejected": "ArtAdapter transforms text descriptions into stylistically rich images, capturing high-level style elements beyond color and brushstrokes. Featuring a multi-level style encoder and explicit adaptation mechanism, it separates content from style and employs fast finetuning to enhance zero-shot style representation. Comprehensive evaluations confirm ArtAdapter's superiority over current state-of-the-art methods."
    },
    {
        "prompt": "summary: Understanding how attention varies across individuals has\nsignificant scientific and societal impacts. However, exist-\ning visual scanpath models treat attention uniformly, ne-\nglecting individual differences. To bridge this gap, this pa-\nper focuses on individualized scanpath prediction (ISP), a\nnew attention modeling task that aims to accurately predict\nhow different individuals shift their attention in diverse vi-\nsual tasks. It proposes an ISP method featuring three novel\ntechnical components: (1) an observer encoder to charac-\nterize and integrate an observer’s unique attention traits,\n(2) an observer-centric feature integration approach that\nholistically combines visual features, task guidance, and\nobserver-specific characteristics, and (3) an adaptive fix-\nation prioritization mechanism that refines scanpath pre-\ndictions by dynamically prioritizing semantic feature maps\nbased on individual observers’ attention traits. These novel\ncomponents allow scanpath models to effectively address\nthe attention variations across different observers.\nOur\nmethod is generally applicable to different datasets, model\narchitectures, and visual tasks, offering a comprehensive\ntool for transforming general scanpath models into indi-\nvidualized ones. Comprehensive evaluations using value-\nbased and ranking-based metrics verify the method’s effec-\ntiveness and generalizability.\n1. Introduction\nSaccadic eye movements, such as fixations and saccades,\nenable individuals to shift their attention quickly and redi-\nrect their focus to different points in the visual field. Study-\ning various factors driving people’s eye movements is im-\nportant for understanding human attention and develop-\ning human-like attention systems. Computational models\npredicting eye movements have broad impacts across var-\nious domains, such as assessing image and video qual-\nity [8, 27, 47], developing intuitive human-computer inter-\naction systems [33, 40, 55, 64, 67], creating immersive vir-\ntual reality experiences [1, 57, 58], enhancing the safety and\nefficiency of autonomous vehicles [28, 77, 78], and diag-\n(a)\n(b)\n(c)\nFigure 1. Understanding and predicting the distinct eye move-\nments of each observer is the key objective of individualized scan-\npath prediction. These examples reveal the variations in the scan-\npaths of different observers, showing their distinct attention pref-\nerences in (a) faces, (b) objects, and (c) background. Each dot\nrepresents a fixation, with the number and radius indicating its or-\nder and duration, respectively. The blue and red dots indicate the\nbeginning and the end of the scanpath, respectively.\nnosing neurodevelopmental conditions [11, 22, 39].\nWhile existing models of saccadic eye movements pre-\ndominantly focus on modeling generic gaze patterns man-\nifested as observer-agnostic scanpaths (i.e., a spatiotem-\nporal sequence of fixations), this work seeks to model\nthe individual variations in eye movements. As shown in\nFigure 1, there exists significant inter-observer variations\nin visual scanpaths. Such variations can be attributed to\na multitude of individual characteristics, such as gender,\nage, and neurodevelopmental conditions [56, 61]. For in-\nstance, females show more explorative gaze patterns than\nmales [53, 62, 63], older adults prefer faces [54] and ob-\njects with high color visibility [74], individuals with neu-\nrodevelopmental disorders, such as autism spectrum disor-\nder (ASD), may show a preference for repetitive patterns\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25420",
        "chosen": "This paper focuses on individualized scanpath prediction (ISP) to model individual variations in eye movements. The proposed method includes an observer encoder, observer-centric feature integration, and adaptive fixation prioritization. The approach effectively transforms general scanpath models into individualized ones and shows strong generalizability.",
        "rejected": "ISP method focuses on predicting individual scanpaths during visual tasks. It integrates observer-specific characteristics, combines task guidance with visual features, and refines predictions using an adaptive fixation prioritization mechanism. The approach effectively addresses attention variations across individuals, verified through comprehensive evaluations on various datasets and visual tasks."
    },
    {
        "prompt": "summary: How to effectively explore multi-scale representations of\nrain streaks is important for image deraining. In contrast\nto existing Transformer-based methods that depend mostly\non single-scale rain appearance, we develop an end-to-end\nmulti-scale Transformer that leverages the potentially use-\nful features in various scales to facilitate high-quality im-\nage reconstruction. To better explore the common degra-\ndation representations from spatially-varying rain streaks,\nwe incorporate intra-scale implicit neural representations\nbased on pixel coordinates with the degraded inputs in a\nclosed-loop design, enabling the learned features to facili-\ntate rain removal and improve the robustness of the model\nin complex scenarios. To ensure richer collaborative rep-\nresentation from different scales, we embed a simple yet ef-\nfective inter-scale bidirectional feedback operation into our\nmulti-scale Transformer by performing coarse-to-ﬁne and\nﬁne-to-coarse information communication. Extensive ex-\nperiments demonstrate that our approach, named as NeRD-\nRain, performs favorably against the state-of-the-art ones\non both synthetic and real-world benchmark datasets. The\nsource code and trained models are available at https:\n//github.com/cschenxiang/NeRD-Rain.\n1. Introduction\nRecent years have witnessed signiﬁcant progress in image\nderaining due to the development of numerous deep con-\nvolutional neural networks (CNNs) [15, 22, 51, 60, 63].\nHowever, as the basic operation in CNNs, the convolution\nis spatially invariant and has limited receptive ﬁelds, which\ncannot effectively model the spatially-variant property and\nnon-local structures of clear images [49, 62]. Moreover,\nsimply increasing the network depth to obtain larger recep-\ntive ﬁelds does not always lead to better performance.\nTo alleviate this problem, several recent approaches uti-\nlize Transformers to solve single image deraining [5, 8, 20,\n55, 56, 62], since Transformers can model the non-local in-\nformation for better image restoration. Although these ap-\nproaches achieve better performance than most of the CNN-\n∗Corresponding author.\nINR\nINR\n(a) Coarse-to-ﬁne\n(b) Multi-patch\n(c) Ours\nFigure 1. Illustration of the proposed approach and the currently\nexisting multi-scale solutions. (a) coarse-to-ﬁne scheme [22, 63];\n(b) multi-patch scheme [61]; (c) our method. Compared to previ-\nous approaches, the method one integrates implicit neural repre-\nsentations (INR) into our bidirectional multi-scale model to form\na closed-loop framework, which allows for better exploration of\nmulti-scale information and modeling of complex rain streaks.\nbased ones, they mostly explore feature representations at a\nﬁxed image scale (i.e., a single-input single-output archi-\ntecture), while ignoring potentially useful information from\nother scales. As the rain effect decreases signiﬁcantly at\ncoarser image scales, exploring the multi-scale representa-\ntion would facilitate the rain removal.\nTo this end, several approaches introduce the coarse-to-\nﬁne mechanism [12, 47] or multi-patch strategy [61] into\ndeep neural networks to exploit multi-scale rain features.\nAs shown in Figure 1, the decoder’s feature or derained im-\nage is initially estimated at a coarse scale and then used as\nthe input at a ﬁner scale for guidance. Although impres-\nsive performance has been achieved, these methods are less\neffective when handling complex and random rain streaks\nbecause these rain streaks cannot be removed by down-\nsampling operations and inaccurate estimation of a coarser\nscale will result in suboptimal restoration performance at\nﬁner scales. Despite the spatially-varying rain streaks ex-\nhibit a variety of scale properties (e.g., size, shape, length,\nand density), the degraded rainy images tend to share some\nsimilar visual degradation characteristics (i.e., common rain\ndegradation representation) [52, 53, 56]. However, existing\nmethods do not effectively model the common degradation\nas they usually rely on traditional representation forms that\nare sensitive to the input variation rather than capturing un-\nderlying implicit functions, which limits their performance\non complex scenarios. Thus, it is of great interest to learn\nthe underlying correlations among features to encode rain\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25627",
        "chosen": "This paper develops an end-to-end multi-scale Transformer for image deraining, incorporating intra-scale implicit neural representations and inter-scale bidirectional feedback. The method, named NeRD-Rain, explores multi-scale representations and demonstrates favorable performance against state-of-the-art approaches on various benchmark datasets.",
        "rejected": "This paper proposes an efficient data-driven solution for self-localization within a floorplan, which does not require retraining per map or location and avoids needing a large image database of the area. The method introduces a novel probabilistic model with an observation module (comprising single and multiview components) and a temporal filtering module. It operates using a ray-based representation internally and is designed to work on conventional consumer hardware, overcoming common limitations such as the need for upright images. The system meets real-time requirements and outperforms state-of-the-art methods significantly."
    },
    {
        "prompt": "summary: An effective pre-training framework with universal 3D\nrepresentations is extremely desired in perceiving large-\nscale dynamic scenes.\nHowever, establishing such an\nideal framework that is both task-generic and label-efficient\nposes a challenge in unifying the representation of the\nsame primitive across diverse scenes.\nThe current con-\ntrastive 3D pre-training methods typically follow a frame-\nlevel consistency, which focuses on the 2D-3D relationships\nin each detached image. Such inconsiderate consistency\ngreatly hampers the promising path of reaching an uni-\nversal pre-training framework: (1) The cross-scene seman-\ntic self-conflict, i.e., the intense collision between primitive\nsegments of the same semantics from different scenes; (2)\nLacking a globally unified bond that pushes the cross-scene\nsemantic consistency into 3D representation learning. To\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n19925",
        "chosen": "This paper addresses the challenge of establishing a universal pre-training framework for large-scale dynamic scene perception. The authors propose a method to unify cross-scene semantic consistency in 3D representation learning, tackling issues like cross-scene semantic self-conflict and the lack of a globally unified bond in current contrastive 3D pre-training methods.",
        "rejected": "This research explores Handwritten Mathematical Expression Generation (HMEG) from symbolic sequences, framing it as a graph-to-image (G2I) generation problem. To address the lack of real masks for supervision, the authors propose an end-to-end G2I pipeline consisting of graph to layout, layout to mask, and mask to image stages. They introduce a Less-is-More (LiM) learning strategy to enhance prediction accuracy and a differentiable layout refinement module to clarify ambiguous areas. Their model, optimized in an end-to-end manner, generates high-quality HME images, surpassing previous generative methods. Additionally, the generated images improve HME recognition models through data augmentation."
    },
    {
        "prompt": "summary: The rebroadcasting of screen-recaptured document im-\nages introduces a signiﬁcant risk to the conﬁdential docu-\nments processed in government departments and commer-\ncial companies. However, detecting recaptured document\nimages subjected to distortions from online social networks\n(OSNs) is challenging since the common forensics cues,\nsuch as moir´e pattern, are weakened during transmission.\nIn this work, we ﬁrst devise a pixel-level distortion model of\nthe screen-recaptured document image to identify the robust\nfeatures of color artifacts. Then, we extract a chromaticity\nmap from the recaptured image to highlight the presence\nof color artifacts even under low-quality samples. Based\non the prior understanding, we design a chromaticity map\nadapter (CMA) to efﬁciently extract the chromaticity map,\nand feed it into the transformer backbone as multi-modal\nprompt tokens.\nTo evaluate the performance of the pro-\nposed method, we collect a recaptured ofﬁce document im-\nage dataset with over 10K diverse samples. Experimental\nresults demonstrate that the proposed CMA method outper-\nforms a SOTA approach (with RGB modality only), reducing\nthe average EER from 26.82% to 16.78%. Robustness eval-\nuation shows that our method achieves 0.8688 and 0.7554\nAUCs under samples with JPEG compression (QF=70) and\nresolution as low as 534×503 pixels.\n1. Introduction\nDocument images, such as certiﬁcates, contracts, and iden-\ntity documents, are gaining popularity in e-business and\ne-government applications, which brings both convenience\nand threat to our applications. Traditionally, an organization\ncontrols the distribution of hard-copy documents to guard\n*B. Li is the corresponding author.\nScreen-\nrecaptured\nDocument \nImage\nGenuine\nDocument \nImage\n(a) RGB image\n(d) Chromaticity\n(ours)\n(c) Spectrum\n(previous)\n(b) LBP\n(previous)\nFigure 1. Illustrations of the low-quality genuine (top row) and\nrecaptured (bottom row) samples and their transformed domain\nrepresentations. (a) Image patches in RGB space. The samples\nare collected by Canon C3530 (4800×1200 DPI) printer, OnePlus\n5T camera (resolution at 1280×960 pixels), Dell P2418D display\n(resolution at 2560×1440 pixels, size 23.8 inches), and subject to\nJPEG compression with a quality factor of 70. (b) LBP maps with\na radius 1 containing 8 elements [40]. (c) Amplitude spectrum in\nthe Fourier domain. (d) Chromaticity map extracted by Eq. (5) in\nour work. The color artifacts highlighted in (d) show clear differ-\nences between the genuine and recaptured samples.\nagainst the leakage of conﬁdential information. Many of-\nﬁce documents with conﬁdential information can be viewed\non screen for a limited time but cannot be printed as hard\ncopies [29]. However, illegal users could acquire the gen-\nuine document image shown on display with their smart-\nphones and distribute the recaptured document image\nwithout being noticed [2, 31]. Worse still, these images are\nusually transmitted through online social networks (OSN),\ne.g., WhatsApp, and WeChat, which introduces further dis-\ntortions (compression, resizing, etc.) to the document im-\nages. Thus, there is a pressing need to develop a recaptured\ndocument detection scheme robust to various distortions.\nExisting works on recaptured image detection focus on\nface and natural images. For instance, literature exploits\nphysical traces of distortion, e.g., specularity distribution\n[52], color saturation [7], edge blurriness [45] and moir´e\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n15577",
        "chosen": "This paper presents a method to detect screen-recaptured document images using a chromaticity map adapter (CMA). The CMA extracts chromaticity maps highlighting color artifacts, which are fed into a transformer backbone. Evaluated on a dataset with over 10K samples, the method reduces average EER from 26.82% to 16.78% and shows robustness under JPEG compression and low resolution.",
        "rejected": "This paper introduces a method for detecting screen-recaptured document images by analyzing color artifacts using a chromaticity map adapter (CMA) and transformer backbone. The proposed approach improves upon existing state-of-the-art methods, reducing the average Equal Error Rate (EER) from 26.82% to 16.78%, even under JPEG compression and low-resolution conditions."
    },
    {
        "prompt": "summary: Passive depth estimation based on stereo or defocus re-\nlies on the presence of the texture on an object to resolve its\ndepth. Hence, recovering the depth of a textureless object—\nfor example, a large white wall—is not just hard but perhaps\neven impossible. Or is it? We show that spatial coherence,\na property of natural light sources, can be used to resolve\nthe depth of a scene point even when it is textureless. Our\napproach relies on the idea that natural light scattered off\na scene point is locally coherent with itself, while incoher-\nent with the light scattered from other surface points; we\nuse this insight to design an optical setup that uses self-\ninterference as a texture feature for estimating depth. Our\nlab prototype is capable of resolving depths of textureless\nobjects in sunlight as well as indoor lights.\n1. Introduction\nShape recovery is a problem of fundamental importance in\ncomputer vision, where the goal is to recover a 3D descrip-\ntion of a scene from one or more images. In passive settings,\nshape information can be recovered from disparity [13],\nshading [23, 24], focus [7], defocus [16], motion [15], or\neven polarization [19, 22]. All of these approaches, how-\never, rely on making certain assumptions about the scene,\nlimiting the generality of each approach.\nConsider, for example, the scenario shown in Fig-\nure 1(b), where a camera measures a textureless surface\nwith a uniform albedo and lit by an unknown white light\nsource. At ﬁrst glance, the 3D reconstruction problem ap-\npears to be underconstrained. It has been widely accepted\nthat it is impossible to passively reconstruct the depth of\nsuch a textureless plane through stereo imaging or depth\nfrom (de)focus; irrespective of the viewing angle and focus\nsettings of the camera, the captured images will always be\nuniform in brightness, providing no visual information that\ncan be used for 3D reconstruction.\nSo when is it possible to see the shape of a textureless\n*Corresponding author: wyharveychen@gmail.com\n(a)\n2mm\n(b)\n(c)\n(d)\nx\ny\nz\ny\nFigure 1. Passive 3D reconstruction of a textureless plane un-\nder white, incoherent illumination. (a) A photo of the capture\nsetup. A tilted plane target (enlarged in the inset) lies in front of\nthe scanning lens of our setup. (b) Front view of the target. Within\nthe sensor area, it is a uniform, textureless plane target. (c) Depth\nestimation with our approach, resolving the desired tilt. Note that\nwe can measure the explicit depth, which can not be recovered by\nother passive methods that only measure normals. (d) A side view\nof the reconstructed target reveals it is a tilted plane.\nobject, under unknown illumination? The classic aperture\nproblem [14] tells us that correspondences cannot be found\nfor textureless objects and hence, disparity across view-\npoints is unobservable. Shape from shading [23, 24] and\nshape from polarization [19, 22] permit the recovery of the\nnormal of a textureless plane, but not its depth. Sundaram\net al. [20] conclude that the depth of a textureless plane can\nbe reconstructed, but only if the plane is heavily tilted with\nrespect to the camera viewpoint. To overcome the lack of\ntexture, numerous works apply active imaging approaches\nsuch as structured light [2] or time-of-ﬂight cameras [5, 8].\nKotwal et al. [10] have demonstrated that OCT based depth\nacquisition can be implemented under sunlight without co-\nherent laser illumination, yet their approach is not fully pas-\nsive because it uses optics to control and direct the sunlight\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25058",
        "chosen": "This paper introduces a novel approach for passive 3D reconstruction of textureless objects using spatial coherence of natural light. The method leverages self-interference as a texture feature, enabling depth estimation of textureless surfaces. A lab prototype demonstrates successful depth resolution of textureless objects in various lighting conditions.",
        "rejected": "The research presents an innovative technique for depth estimation of textureless objects through spatial coherence in natural light sources. By utilizing self-interference as a texture feature, the optical setup can estimate depths accurately under both sunlight and indoor lighting conditions, overcoming traditional limitations of passive depth estimation methods."
    },
    {
        "prompt": "summary: This paper proposes ConsistDreamer – a novel frame-\nwork that lifts 2D diffusion models with 3D awareness\nand 3D consistency, thus enabling high-fidelity instruction-\nguided scene editing. To overcome the fundamental limi-\ntation of missing 3D consistency in 2D diffusion models,\nour key insight is to introduce three synergistic strategies\nthat augment the input of the 2D diffusion model to be-\ncome 3D-aware and to explicitly enforce 3D consistency\nduring the training process. Specifically, we design sur-\nrounding views as context-rich input for the 2D diffusion\nmodel, and generate 3D-consistent structured noise instead\nof image-independent noise. Moreover, we introduce self-\nsupervised consistency-enforcing training within the per-\nscene editing procedure. Extensive evaluation shows that\nour ConsistDreamer achieves state-of-the-art performance\nfor instruction-guided scene editing across various scenes\nand editing instructions, particularly in complicated large-\nscale indoor scenes from ScanNet++, with significantly im-\nproved sharpness and fine-grained textures. Notably, Con-\nsistDreamer stands as the first work capable of success-\nfully editing complex (e.g., plaid/checkered) patterns. Our\nproject page is at immortalco.github.io/ConsistDreamer.\n†Work started during an internship at Meta Reality Labs Zurich.\n1. Introduction\nWith the emergence of instruction-guided 2D generative\nmodels as in [2], it has never been easier to generate or\nedit images. Extending this success to 3D, i.e., instruction-\nguided 3D scene editing, becomes highly desirable for\nartists, designers, and the movie and game industries. Nev-\nertheless, editing 3D scenes or objects is inherently chal-\nlenging. The absence of large-scale, general 3D datasets\nmakes it difficult to create a counterpart generative model\nsimilar to [2] that can support arbitrary 3D scenes. There-\nfore, state-of-the-art solutions [8, 25] circumvent this chal-\nlenge by resorting to generalizable 2D diffusion models.\nThis approach, known as 2D diffusion distillation, renders\nthe scene into multi-view images, applies an instruction-\nconditioned diffusion model in 2D, and then distills the edit-\ning signal back to 3D, such as through a neural radiance\nfield (NeRF) [6, 8, 15].\nHowever, a fundamental limitation of this solution is the\nlack of 3D consistency: a 2D diffusion model, acting in-\ndependently across views, is likely to produce inconsistent\nedits, both in color and shape. For example, a person in\none view might be edited to be wearing a red shirt, while\nappearing in a green shirt in another view. Using these im-\nages to train a NeRF can still produce reasonable edits, but\nthe model will naturally converge towards an “averaged”\nrepresentation of the inconsistent 2D supervision, and lose\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n21071",
        "chosen": "This paper proposes ConsistDreamer, a framework enhancing 2D diffusion models with 3D awareness for high-fidelity scene editing. The approach introduces three strategies: using surrounding views as context, generating 3D-consistent noise, and self-supervised consistency training. ConsistDreamer achieves state-of-the-art performance in instruction-guided scene editing, particularly in complex indoor scenes.",
        "rejected": "ConsistDreamer is introduced as a novel framework that enhances 2D diffusion models with 3D awareness and consistency, enabling high-fidelity instruction-guided scene editing. It achieves state-of-the-art performance across various scenes, particularly improving sharpness and fine-grained textures in large-scale indoor scenes."
    },
    {
        "prompt": "summary: In this paper, we democratise caricature generation, em-\npowering individuals to effortlessly craft personalised car-\nicatures with just a photo and a conceptual sketch. Our\nobjective is to strike a delicate balance between abstraction\nand identity, while preserving the creativity and subjectiv-\nity inherent in a sketch. To achieve this, we present Explicit\nRank-1 Model Editing alongside single-image personalisa-\ntion, selectively applying nuanced edits to cross-attention\nlayers for a seamless merge of identity and style. Addition-\nally, we propose Random Mask Reconstruction to enhance\nrobustness, directing the model to focus on distinctive iden-\ntity and style features. Crucially, our aim is not to replace\nartists but to eliminate accessibility barriers, allowing en-\nthusiasts to engage in the artistry.\n1. Introduction\nEver wondered when you would finally decide to get that\npersonalised caricature created, perhaps during a holiday?\nLook no further, this paper is for you – we strive to\ndemocratise caricature [6, 26, 27] generation for everyone!\nWith a portrait of yourself and a conceptual sketch of how\nyou envision your caricature, we will automatically gen-\nerate a high-fidelity caricature that unmistakably captures\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8629",
        "chosen": "This paper presents a method for generating personalized caricatures from a photo and a conceptual sketch. The approach combines Explicit Rank-1 Model Editing and single-image personalization, focusing on cross-attention layers. Random Mask Reconstruction is used to enhance robustness, ensuring a balance between identity and style.",
        "rejected": "A new method for generating personalized caricatures from photos and conceptual sketches is proposed, balancing abstraction and identity while preserving artistic creativity. Techniques like Explicit Rank-1 Model Editing and Random Mask Reconstruction are employed to enhance robustness and ensure seamless integration of identity and style features."
    },
    {
        "prompt": "summary: We propose DiffSHEG, a Diffusion-based approach for\nSpeech-driven Holistic 3D Expression and Gesture genera-\ntion with arbitrary length. While previous works focused on\nco-speech gesture or expression generation individually, the\njoint generation of synchronized expressions and gestures\nremains barely explored.\nTo address this, our diffusion-\nbased co-speech motion generation transformer enables\nuni-directional information flow from expression to gesture,\nfacilitating improved matching of joint expression-gesture\ndistributions. Furthermore, we introduce an outpainting-\nbased sampling strategy for arbitrary long sequence gen-\neration in diffusion models, offering flexibility and compu-\ntational efficiency.\nOur method provides a practical so-\nlution that produces high-quality synchronized expression\nand gesture generation driven by speech. Evaluated on two\npublic datasets, our approach achieves state-of-the-art per-\nformance both quantitatively and qualitatively. Addition-\nally, a user study confirms the superiority of DiffSHEG over\nprior approaches. By enabling the real-time generation of\nexpressive and synchronized motions, DiffSHEG showcases\nits potential for various applications in the development of\ndigital humans and embodied agents.\n1. Introduction\nNon-verbal cues such as facial expressions, body language,\nand hand gestures play a vital role in effective communi-\ncation alongside verbal language [13, 43]. Speech-driven\ngesture and expression generation has gained significant in-\nterest in applications like the metaverse, digital human de-\nvelopment, gaming, and human-computer interaction. Gen-\nerating synchronized and realistic gestures and expressions\nbased on speech is key to bringing virtual agents and digital\nhumans to life in virtual environments.\n*The work was done during Junming’s internship at the International\nDigital Economy Academy.\n†Corresponding authors.\n…\n…\n…\n… journalist never tell lie …\n… want to be a journalist …\nExpression Generator\nGesture Generator\nDiffusion\nProcedure\nFigure 1. DiffSHEG is a unified co-speech expression and gesture\ngeneration system based on diffusion models. It captures the joint\nexpression-gesture distribution by enabling the uni-directional in-\nformation flow from expression to gesture inside the model.\nExisting research on co-speech motion synthesis has fo-\ncused on generating either expressions or gestures indepen-\ndently.\nRule-based approaches [8, 18, 22, 38, 44] were\nprevalent initially, but recent advancements have leveraged\ndata-driven techniques using deep neural networks. How-\never, co-speech gesture generation poses a challenge due\nto its inherently many-to-many mapping. State-of-the-art\nmethods have explored generative models such as nor-\nmalizing flow models [48], VQ-VAE [2], GAN [14] and\nDiffusion models [1, 3, 47, 54]. These approaches have\nmade progress in improving synchronization and diversify-\ning generated gestures. However, none of them specifically\ntarget the co-speech generation of both expressions and ges-\ntures simultaneously.\nRecently, some works have aimed to generate co-speech\nholistic 3D expressions and gestures [14, 49]. These meth-\nods either combine independent co-speech expression and\ngesture models [49] or formulate the problem as a multi-\ntask learning one [14]. However, these approaches separate\nthe generation process of expressions and gestures, neglect-\ning the potential relationship between them. This can lead\nto disharmony and deviation in the joint expression-gesture\ndistribution. Additionally, deterministic CNN-based mod-\nels [14] may not be well-suited for approximating the many-\nto-many mapping inherent in co-speech gesture generation.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n7352",
        "chosen": "This paper proposes DiffSHEG, a diffusion-based approach for speech-driven holistic 3D expression and gesture generation. The method enables uni-directional information flow from expression to gesture and introduces an outpainting-based sampling strategy for generating sequences of arbitrary length. Evaluated on public datasets, DiffSHEG achieves state-of-the-art performance.",
        "rejected": "DiffSHEG is a diffusion-based model designed for speech-driven holistic 3D expression and gesture generation, capable of producing synchronized expressions and gestures from arbitrary length audio inputs. Evaluation on public datasets shows superior qualitative and quantitative performance, offering potential applications in digital human development and embodied agents."
    },
    {
        "prompt": "summary: We present a method for large-mask pluralistic image in-\npainting based on the generative framework of discrete la-\ntent codes. Our method learns latent priors, discretized as\ntokens, by only performing computations at the visible lo-\ncations of the image. This is realized by a restrictive partial\nencoder that predicts the token label for each visible block,\na bidirectional transformer that infers the missing labels by\nonly looking at these tokens, and a dedicated synthesis net-\nwork that couples the tokens with the partial image priors\nto generate coherent and pluralistic complete image even\nunder extreme mask settings. Experiments on public bench-\nmarks validate our design choices as the proposed method\noutperforms strong baselines in both visual quality and di-\nversity metrics.\n1. Introduction\nImage inpainting is the task of filling the missing pixels of\na masked image with appropriate contents that are coher-\nent to its visible regions. As a long-studied topic in com-\nputer vision, image inpainting has evolved from a restora-\ntion technique solely relying on existing information from\nthe input image (e.g. [3]) to data-driven generative methods\n(e.g. [23, 27, 36, 41, 44, 48]) that hallucinates detailed con-\ntents from not only the observable pixels but also learned,\nrich image priors.\nPluralistic inpainting refers to the ability of a model to\ngenerate multiple plausible results that complete a partial\nimage.\nIt offers a view of image inpainting as a gener-\native method that models the smooth distributions of the\ncomplete images given the partial image as prior informa-\ntion [48]. However, modeling such distributions is challeng-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n7591",
        "chosen": "This paper presents a method for large-mask pluralistic image inpainting using discrete latent codes. The approach includes a restrictive partial encoder, a bidirectional transformer, and a synthesis network. Experiments show the method outperforms baselines in visual quality and diversity metrics.",
        "rejected": "This paper describes a pluralistic image inpainting method based on discrete latent codes, which generates coherent and diverse complete images even under extreme mask settings. The approach outperforms strong baselines in visual quality and diversity metrics, demonstrating its effectiveness in handling challenging inpainting tasks."
    },
    {
        "prompt": "summary: We present DRESS , a large vision language model\n(LVLM) that innovatively exploits Natural Language feed-\nback (NLF) from Large Language Models to enhance its\nalignment and interactions by addressing two key limitations\nin the state-of-the-art LVLMs. First, prior LVLMs gener-\nally rely only on the instruction ﬁnetuning stage to enhance\nalignment with human preferences. Without incorporating\nextra feedback, they are still prone to generate unhelpful,\nhallucinated, or harmful responses. Second, while the visual\ninstruction tuning data is generally structured in a multi-\nturn dialogue format, the connections and dependencies\namong consecutive conversational turns are weak. This re-\nduces the capacity for effective multi-turn interactions. To\ntackle these, we propose a novel categorization of the NLF\ninto two key types: critique and reﬁnement. The critique\nNLF identiﬁes the strengths and weaknesses of the responses\nand is used to align the LVLMs with human preferences.\nThe reﬁnement NLF offers concrete suggestions for improve-\nment and is adopted to improve the interaction ability of the\nLVLMs– which focuses on LVLMs’ ability to reﬁne responses\nby incorporating feedback in multi-turn interactions. To\naddress the non-differentiable nature of NLF, we generalize\nconditional reinforcement learning for training. Our experi-\nmental results demonstrate that DRESS\ncan generate more\nhelpful (9.76%), honest (11.52%), and harmless (21.03%)\nresponses, and more effectively learn from feedback during\nmulti-turn interactions compared to SOTA LVLMs.\n1. Introduction\nLarge vision-language models (LVLMs) can perceive the\nvisual world and follow the instructions to generate user-\nfriendly responses [6, 43, 90]. This is achieved by effectively\ncombining large-scale visual instruction ﬁnetuning [78] with\n*Work done during internship at SRI International.\nFigure 1. We instruct DRESS\nto improve both alignment with\nhuman preferences and interaction ability via natural language\nfeedback, which is categorized into critique and reﬁnement.\nlarge language models (LLMs) [5, 53].\nHowever, existing LVLMs solely leverage the LLMs-\ngenerated or hand-crafted datasets to achieve alignment\nvia supervised ﬁne-tuning (SFT) [6, 43, 78]. While it’s\neffective at transforming LVLMs from caption generators\nto instruction-following models, LVLMs can still generate\nresponses that are unhelpful, hallucinated, or even harm-\nful (see Figure 4). This indicates that their present level of\nalignment with human preference is still relatively low [81].\nIn addition, although existing work motivates to structure\nvisual instruction tuning samples in multi-turn formats, the\nconnection and dependencies among various turns are weak,\nwhich restricts the interaction ability of the LVLMs. Here the\ninteraction ability measures whether LVLMs can effectively\nleverage the previous context in multi-turn interactions and\nreﬁne their responses [72]. These two limitations restrict the\npotential of LVLMs to serve as visual assistants in practice.\nIn this work, we introduce DRESS , an LVLM\ndistinctively trained through the application of Natural\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n14239",
        "chosen": "This paper introduces DRESS, a large vision-language model that uses natural language feedback (NLF) for enhanced alignment and interaction. The model categorizes NLF into critique and refinement types, improving response quality and multi-turn interaction ability. Experimental results demonstrate DRESS's superiority over existing LVLMs.",
        "rejected": "DRESS, a large vision language model, leverages Natural Language Feedback (NLF) to improve alignment and multi-turn interaction capabilities. It categorizes NLF into critique and refinement types to better align responses with human preferences and improve interaction abilities. Experimental results show significant improvements in helpfulness, honesty, and harmlessness compared to state-of-the-art LVLMs."
    },
    {
        "prompt": "summary: Self-supervised denoising has attracted widespread at-\ntention due to its ability to train without clean images. How-\never, noise in real-world scenarios is often spatially cor-\nrelated, which causes many self-supervised algorithms that\nassume pixel-wise independent noise to perform poorly. Re-\ncent works have attempted to break noise correlation with\ndownsampling or neighborhood masking. However, denois-\ning on downsampled subgraphs can lead to aliasing effects\nand loss of details due to a lower sampling rate. Further-\nmore, the neighborhood masking methods either come with\nhigh computational complexity or do not consider local\nspatial preservation during inference. Through the analy-\nsis of existing methods, we point out that the key to obtain-\ning high-quality and texture-rich results in real-world self-\nsupervised denoising tasks is to train at the original input\nresolution structure and use asymmetric operations during\ntraining and inference. Based on this, we propose Asymmet-\nric Tunable Blind-Spot Network (AT-BSN), where the blind-\nspot size can be freely adjusted, thus better balancing noise\ncorrelation suppression and image local spatial destruction\nduring training and inference. In addition, we regard the\npre-trained AT-BSN as a meta-teacher network capable of\ngenerating various teacher networks by sampling different\nblind-spots. We propose a blind-spot based multi-teacher\ndistillation strategy to distill a lightweight network, signif-\nicantly improving performance.\nExperimental results on\nmultiple datasets prove that our method achieves state-of-\nthe-art, and is superior to other self-supervised algorithms\nin terms of computational overhead and visual effects.\n1. Introduction\nImage denoising is an essential low-level computer vision\nproblem. With the advancements in deep learning, an in-\n*Corresponding author.\n(a) Noisy Input\n(b) AP-BSN (R3)\n(c) LG-BPN\n \n(d) SDAP (E)\n(e) SpatiallyAdaptive\n(f) Ours  AT-BSN \nFigure 1. Comparisons of our AT-BSN with other methods. Our\nmethod recovers more high frequency texture details.\ncreasing number of studies are focused on supervised learn-\ning using clean-noisy pairs [2, 17, 32, 47–49]. Typically,\nadditive white Gaussian noise (AWGN) is introduced into\nclean datasets to synthesize clean-noisy denoising datasets.\nHowever, real-world noise is known to be spatially cor-\nrelated [7, 23, 37].\nSome generative-based methods at-\ntempt to synthesize real-world noise from existing clean\ndata [5, 8, 18, 21, 43]. However, synthesizing real-world\nnoise remains challenging, and suffers from generalization\nissues.\nTo address the issue, some researchers attempt\nto capture clean-noisy pairs in real-world scenarios [1, 4].\nHowever, in certain scenarios, such as medical imaging and\nelectron microscopy, constructing such datasets can be im-\npractical or even infeasible.\nSelf-supervised denoising algorithms, represented by\nNoise2Noise [28], have brought new life to the denoising\nfield.\nThese methods only require noisy observations to\ntrain the denoising model. However, in real-world scenar-\nios, noise often exhibits spatial correlation, which contra-\ndicts the pixel-wise independent noise assumption [25, 28]\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n2814",
        "chosen": "This paper proposes an asymmetric tunable blind-spot network (AT-BSN) for self-supervised denoising in real-world scenarios. The method balances noise correlation suppression and image local spatial preservation, using asymmetric operations during training and inference. The approach achieves state-of-the-art performance with lower computational overhead.",
        "rejected": "AT-BSN is introduced as a solution for real-world self-supervised denoising, addressing the challenge of spatially correlated noise. This asymmetric tunable blind-spot network balances noise correlation suppression and local spatial preservation, achieving state-of-the-art performance in terms of computational overhead and visual effects."
    },
    {
        "prompt": "summary: In this paper we propose an efﬁcient data-driven solu-\ntion to self-localization within a ﬂoorplan. Floorplan data\nis readily available, long-term persistent and inherently ro-\nbust to changes in the visual appearance. Our method does\nnot require retraining per map and location or demand a\nlarge database of images of the area of interest. We propose\na novel probabilistic model consisting of an observation\nand a novel temporal ﬁltering module. Operating internally\nwith an efﬁcient ray-based representation, the observation\nmodule consists of a single and a multiview module to pre-\ndict horizontal depth from images and fuses their results\nto beneﬁt from advantages offered by either methodology.\nOur method operates on conventional consumer hardware\nand overcomes a common limitation of competing meth-\nods [16, 17, 20, 28] that often demand upright images. Our\nfull system meets real-time requirements, while outperform-\ning the state-of-the-art [20, 28] by a signiﬁcant margin.\n1. Introduction\nCamera localization is an essential research topic in com-\nputer vision.\nIt is key to many AR/VR applications for\nhead-mounted or handheld mobile devices and is of great\npractical interest to the robotics community. Most existing\nworks localize the camera using a pre-collected database\n[40][1][2] or within a pre-built 3D model [24, 34, 37–\n39]. However, these representations of the environment are\ncostly in terms of storage and maintenance. In contrast, in-\ndoor environments including most commercial real estate\nsuch as warehouses, ofﬁces and apartments already possess\na ﬂoorplan. The ﬂoorplan is a generic representation for in-\ndoor environments that is easily accessible, lightweight, and\npreserves long-term scene structure independent of a chang-\ning visual appearance, such as the furnishing of the scene. It\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n18029",
        "chosen": "This paper presents F3Loc, a method for self-localization within a floorplan using a probabilistic model with observation and temporal filtering modules. The method operates on consumer hardware and meets real-time requirements, outperforming existing approaches.",
        "rejected": "An efficient data-driven solution for camera localization within floorplans is proposed, utilizing a probabilistic model with observation and temporal filtering modules. The method operates on conventional consumer hardware and meets real-time requirements, significantly outperforming current state-of-the-art methods."
    },
    {
        "prompt": "summary: In this paper, we explore a novel challenging genera-\ntion task, i.e. Handwritten Mathematical Expression Gen-\neration (HMEG) from symbolic sequences. Since symbolic\nsequences are naturally graph-structured data, we formu-\nlate HMEG as a graph-to-image (G2I) generation problem.\nUnlike the generation of natural images, HMEG requires\ncritic layout clarity for synthesizing correct and recogniz-\nable formulas, but has no real masks available to super-\nvise the learning process. To alleviate this challenge, we\npropose a novel end-to-end G2I generation pipeline (i.e.\ngraph →layout →mask →image), which requires no\nreal masks or nondifferentiable alignment between layouts\nand masks. Technically, to boost the capacity of predicting\ndetailed relations among adjacent symbols, we propose a\nLess-is-More (LiM) learning strategy. In addition, we de-\nsign a differentiable layout refinement module, which maps\nbounding boxes to pixel-level soft masks, so as to further al-\nleviate ambiguous layout areas. Our whole model, includ-\ning layout prediction, mask refinement, and image genera-\ntion, can be jointly optimized in an end-to-end manner. Ex-\nperimental results show that, our model can generate high-\nquality HME images, and outperforms previous generative\nmethods. Besides, a series of ablations study demonstrate\neffectiveness of the proposed techniques. Finally, we vali-\ndate that our generated images promisingly boosts the per-\nformance of HME recognition models, through data aug-\nmentation. Our code and results are available at: https:\n//github.com/AiArt-HDU/HMEG.\n1. Introduction\nHandwritten Mathematical Expressions (HMEs) are com-\nmon and play significant roles in our daily life, especially in\nthe research and education areas. HMEs generally present\ncomplex structures, serious deformations, and diverse writ-\n∗Equal Contributions. † Corresponding Author.\n(a)\n(b)\nBBox-to-Mask\nTransformation\nEnd-to-end Training\nLayout\nPrediction\nImage \nDecoder \nreal\nimage\nImage\nLayout\nDiscriminator\nBBox \nIndices\nInput \nGraph\nLayout\nPrediction\nMask\nPrediction\nImage \nDecoder \nreal\nlayout\nreal\nmask\nImage\nreal\nimage\nreal\nlayout\n Stage-II\nStage-I\nScene\nGraph\nhuman\nhorse\nride\nSymbol\nGraph\nx\n2\nsup\nFigure 1.\nDifferences between (a) typical two-stage graph-to-\nimage generation pipeline and (b) our end-to-end pipeline.\nIn\nprevious methods, the real masks are available as the input. In\ncontrast, we propose a novel end-to-end pipeline of graph →\nlayout →mask →image, and requires no real masks or non-\ndifferentiable alignment between layouts and masks.\ning styles. Such characteristics, along with data scarcity,\nmake HME Recognition (HMER) a grand challenge in the\nOCR community. Despite the tremendous efforts that have\nbeen made to this task, the HMER performance is still un-\nsatisfactory [14, 27, 62].\nRecently, synthetic data augmentation has shown inspir-\ning performance in various recognition tasks [23, 51]. This\nis mainly achieved by the remarkable progress of condi-\ntional image generation [21] in the past few years [58]. The\nadvances allow for creating realistic images through texts\n[39, 57, 66], scene graphs [24, 60], layouts [28, 69], seman-\ntic masks [37, 72], sketches [61, 67], and more [12, 65].\nHowever, there has been no effective generative model for\ngenerating high-quality HME images.\nSome previous works synthesize HMEs by recompos-\ning real online HMEs [46, 59].\nBesides, FormulaGAN\n[41] generates HMEs from rendered images, via an Image-\nto-Image (I2I) Translation model. However, their gener-\nated images are limited in either diversity or realism. Be-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n15675",
        "chosen": "This paper explores generating handwritten mathematical expressions from symbolic sequences, formulating it as a graph-to-image generation problem. The proposed end-to-end pipeline includes layout prediction, mask refinement, and image generation. Experimental results show the model generates high-quality images and boosts recognition performance through data augmentation.",
        "rejected": "The paper explores Handwritten Mathematical Expression Generation (HMEG) as a graph-to-image generation problem, proposing an end-to-end pipeline with layout prediction, mask refinement, and image generation stages. The Less-is-More (LiM) learning strategy and differentiable layout refinement module improve detailed relation prediction among symbols, resulting in high-quality HME images."
    },
    {
        "prompt": "summary: In recent years, Neural Radiance Field (NeRF) has\ndemonstrated remarkable capabilities in representing 3D\nscenes.\nTo expedite the rendering process, learnable\nexplicit\nrepresentations\nhave\nbeen\nintroduced\nfor\ncombination with implicit NeRF representation, which\nhowever results in a large storage space requirement.\nIn this paper, we introduce the Context-based NeRF\nCompression (CNC) framework, which leverages highly\nefﬁcient context models to provide a storage-friendly NeRF\nrepresentation.\nSpeciﬁcally, we excavate both level-wise\nand\ndimension-wise\ncontext\ndependencies\nto\nenable\nprobability prediction for information entropy reduction.\nAdditionally, we exploit hash collision and occupancy grids\nas strong prior knowledge for better context modeling. To\nthe best of our knowledge, we are the ﬁrst to construct and\nexploit context models for NeRF compression. We achieve\na size reduction of 100⇥and 70⇥with improved ﬁdelity\nagainst the baseline Instant-NGP on Synthesic-NeRF and\nTanks and Temples datasets, respectively. Additionally, we\nattain 86.7% and 82.3% storage size reduction against\nthe SOTA NeRF compression method BiRF. Our code is\navailable here: https://github.com/YihangChen-ee/CNC.\n1. Introduction\nHigh-quality photo-realistic rendering at novel viewpoints\nremains a pivotal challenge in both computer vision and\ncomputer graphics. Traditional explicit 3D representations,\nsuch as voxel grids [17, 25, 34, 37], have earned their\nplace due to their efﬁciency across numerous applications.\nHowever, their discrete nature makes them susceptible to\nthe limitations imposed by the Nyquist sampling theorem,\noften necessitating exponentially increased memory for\ncapturing detailed nuances.\nIn the past few years, Neural Radiance Field (NeRF) [28]\nhas emerged as a game-changer for novel view synthesis.\nNeRF deﬁnes both density and radiance at a 3D point as\n1The size of INGP is calculated under 16 levels with resolution from\n16 to 2048. The feature vector dimension is 2 and represented with FP32.\nσ\nc\nRendering MLP\nOccupancy Grid\nFeature Embeddings:\nContext Models\n3D Feature Embeddings\nRendering MLP\nOccupancy Grid\n45.56MB \n33.76dB\n3D\n!!\n…\n(x, d)\nconcat\nIndexing\n45.56MB \n36.98dB\n103x\nsmaller\n103x\nsmaller\n…\n∈−1; +1\n…\n3D=>2D\n3D\n2D\n…\nDimension-wise\nContext Models\nLevel-wise\nContext Models\nHash \nFusion\n2D\n2D Feature Embeddings\n0.444MB \n34.35dB\n0.442MB \n37.31dB\nContext Models\nFigure 1.\nMotivation of our work.\nInstant-NGP represents\n3D scenes using 3D hash feature embeddings along with a\nrendering MLP, which takes a non-negligible storage size with the\nembeddings accounting for over 99% of storage size (upper-left).\nTo tackle this, we introduce context models to substantially\ncompress feature embeddings, with the three key technical\ncomponents (bottom-left). Our approach achieves a size reduction\nof over 100⇥while simultaneously improving ﬁdelity.1\nfunctions of the 3D coordinates. Its implicit representation,\nencapsulated within a Multi-Layer Perceptron (MLP),\ncaptures continuous signals of a 3D scene seamlessly.\nLeveraging frequency-based positional embeddings of 3D\ncoordinates [28, 41, 49], NeRF has showcased superior\nnovel view synthesis quality in comparison to traditional\nexplicit 3D representations.\nWhile NeRF exhibits good\ncharacteristics in memory efﬁciency and image quality, its\ncomplex queries of the MLP slow down its rendering speed.\nTo boost NeRF’s rendering speed, recent approaches\nhave converged towards a hybrid representation, merging\nexplicit voxelized feature encoding with implicit neural\nnetworks.\nThis combination promises faster rendering\nwithout compromising on quality. These methods include\nvaried data structures such as dense grids [7, 38–40],\noctrees [26, 50],\nsparse voxel grids [24],\nand hash\ntables [29]. Among them, Instant-NGP (INGP) [29] which\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n20321",
        "chosen": "This paper presents CNC, a framework for compressing NeRF representations using context models to reduce storage requirements. The approach leverages level-wise and dimension-wise context dependencies and utilizes hash collisions and occupancy grids for better context modeling. Experiments show significant size reductions and improved fidelity compared to baseline methods.",
        "rejected": "In recent years, Neural Radiance Field (NeRF) has shown remarkable 3D scene representation abilities but faced challenges with storage space due to explicit representations. This paper presents Context-based NeRF Compression (CNC), which utilizes context models for efficient compression while improving fidelity. CNC achieves significant size reduction compared to Instant-NGP and BiRF methods."
    },
    {
        "prompt": "summary: Diffusion models have shown an impressive ability to\nmodel complex data distributions, with several key advan-\ntages over GANs, such as stable training, better coverage\nof the training distribution’s modes, and the ability to solve\ninverse problems without extra training. However, most dif-\nfusion models learn the distribution of fixed-resolution im-\nages. We propose to learn the distribution of continuous\nimages by training diffusion models on image neural fields,\nwhich can be rendered at any resolution, and show its ad-\nvantages over fixed-resolution models. To achieve this, a\nkey challenge is to obtain a latent space that represents pho-\ntorealistic image neural fields. We propose a simple and\neffective method, inspired by several recent techniques but\nwith key changes to make the image neural fields photo-\nrealistic. Our method can be used to convert existing la-\ntent diffusion autoencoders into image neural field autoen-\ncoders.\nWe show that image neural field diffusion mod-\nels can be trained using mixed-resolution image datasets,\noutperform fixed-resolution diffusion models followed by\nsuper-resolution models, and can solve inverse problems\nwith conditions applied at different scales efficiently.\n1. Introduction\nDiffusion models [16, 34, 50] have recently become attrac-\ntive alternatives to GANs. These likelihood-based models\n†Equal advising.\nexhibit fewer artifacts, stable training, can model complex\ndata distributions, do not suffer from mode collapse, and\ncan solve inverse problems using the score function with-\nout extra training. Since diffusion typically requires many\niterations at a fixed dimension, directly modeling the dif-\nfusion process in the pixel space [17, 40, 43] can be in-\nefficient for high-resolution image synthesis. Latent diffu-\nsion models (LDMs) [41, 57] were proposed as a more ef-\nficient alternative. The key idea is to learn an autoencoder\nto map images to a latent representation from which the im-\nage can be decoded back, and train a diffusion model on\nthe lower-dimensional latent representation. Despite their\nsuccess, LDMs’ latent space still represents images at fixed\nresolution (for example, 256 in LDM [41] and 512 in Stable\nDiffusion). To generate higher-resolution images (e.g., 2K),\nLDMs usually first generate a low-resolution image and up-\nsample it using a separate super-resolution model.\nIn this work, we propose Image Neural Field Diffusion\nmodels (INFD). Our method is based on the latent diffusion\nframework, where we first learn a latent representation that\nrepresents an image neural field (which can be rendered at\nany resolution), then learn a diffusion model on this latent\nrepresentation. A key challenge of our approach is to learn a\nlatent space of photorealistic image neural fields where the\ndiffusion model is applied. We propose a simple and effec-\ntive method that can convert an existing autoencoder of la-\ntent diffusion models to a neural field autoencoder. We find\nthat directly implementing an autoencoder with LIIF [8]\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8007",
        "chosen": "This paper introduces Image Neural Field Diffusion models (INFD), which learn the distribution of continuous images by training diffusion models on image neural fields. The method converts existing latent diffusion autoencoders into image neural field autoencoders and demonstrates advantages over fixed-resolution models, including better performance on mixed-resolution datasets and efficient solving of inverse problems.",
        "rejected": "Diffusion models have demonstrated advantages over GANs in modeling complex data distributions. This work proposes Image Neural Field Diffusion models (INFD), capable of rendering images at any resolution and solving inverse problems efficiently without extra training. INFD outperforms fixed-resolution diffusion models followed by super-resolution models."
    },
    {
        "prompt": "summary: The exponential growth of large language models\n(LLMs) has opened up numerous possibilities for multi-\nmodal AGI systems. However, the progress in vision and\nvision-language foundation models, which are also critical\nelements of multi-modal AGI, has not kept pace with LLMs.\nIn this work, we design a large-scale vision-language foun-\ndation model (InternVL), which scales up the vision foun-\ndation model to 6 billion parameters and progressively\naligns it with the LLM, using web-scale image-text data\nfrom various sources. This model can be broadly applied\nto and achieve state-of-the-art performance on 32 generic\nvisual-linguistic benchmarks including visual perception\ntasks such as image-level or pixel-level recognition, vision-\nlanguage tasks such as zero-shot image/video classification,\nzero-shot image/video-text retrieval, and link with LLMs to\ncreate multi-modal dialogue systems. It has powerful visual\ncapabilities and can be a good alternative to the ViT-22B.\nWe hope that our research could contribute to the develop-\nment of multi-modal large models.\n† This work is done when they are interns at Shanghai AI Laboratory;\nB corresponding author (daijifeng@tsinghua.edu.cn)\n1. Introduction\nLarge language models (LLMs) largely promote the de-\nvelopment of artificial general intelligence (AGI) systems\nwith their impressive capabilities in open-world language\ntasks, and their model scale and performance are still in-\ncreasing at a fast pace.\nVision large language models\n(VLLMs) [3, 5, 19, 21, 28, 69, 87, 113, 147], which leverage\nLLMs, have also achieved significant breakthroughs, en-\nabling sophisticated vision-language dialogues and interac-\ntions. However, the progress of vision and vision-language\nfoundation models, which are also crucial for VLLMs, has\nlagged behind the rapid growth of LLMs.\nTo bridge vision models with LLMs, existing VLLMs\n[5, 61, 100, 138, 147] commonly employ lightweight “glue”\nlayers, such as QFormer [61] or linear projection [69], to\nalign features of vision and language models. Such align-\nment contains several limitations: (1) Disparity in param-\neter scales. The large LLMs [38] now boosts up to 1000\nbillion parameters, while the widely-used vision encoders\nof VLLMs are still around one billion. This gap may lead\nto the under-use of LLM’s capacity. (2) Inconsistent rep-\nresentation. Vision models, trained on pure-vision data or\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n24185",
        "chosen": "This paper presents InternVL, a large-scale vision-language foundation model with 6 billion parameters. The model is progressively aligned with large language models (LLMs) using web-scale image-text data and achieves state-of-the-art performance on 32 visual-linguistic benchmarks. It offers powerful visual capabilities and can serve as an alternative to ViT-22B.",
        "rejected": "This paper introduces a large-scale vision-language foundation model named InternVL, aligning with large language models using web-scale image-text data. It performs well on various visual-linguistic benchmarks, offering a promising alternative to ViT-22B. The model bridges the gap between vision models and language models."
    },
    {
        "prompt": "summary: Speech-preserving\nfacial\nexpression\nmanipulation\n(SPFEM) aims to modify facial emotions while meticu-\nlously maintaining the mouth animation associated with\nspoken content.\nCurrent works depend on inaccessible\npaired training samples for the person, where two aligned\nframes exhibit the same speech content yet differ in emo-\ntional expression, limiting the SPFEM applications in\nreal-world scenarios. In this work, we discover that speak-\ners who convey the same content with different emotions\nexhibit highly correlated local facial animations, providing\nvaluable supervision for SPFEM. To capitalize on this\ninsight, we propose a novel adaptive spatial coherent\ncorrelation learning (ASCCL) algorithm, which models\nthe aforementioned correlation as an explicit metric and\nintegrates the metric to supervise manipulating facial\nexpression and meanwhile better preserving the facial\nanimation of spoken contents. To this end, it first learns\na spatial coherent correlation metric, ensuring the visual\ndisparities of adjacent local regions of the image belonging\nto one emotion are similar to those of the corresponding\ncounterpart of the image belonging to another emotion.\nRecognizing that visual disparities are not uniform across\nall regions,\nwe have also crafted a disparity-aware\nadaptive strategy that prioritizes regions that present\ngreater challenges.\nDuring SPFEM model training, we\nconstruct the adaptive spatial coherent correlation metric\nbetween corresponding local regions of the input and\noutput images as addition loss to supervise the generation\n*Zhijing Yang is the corresponding author.\nTianshui Chen, Jian-\nman Lin, and Zhijing Yang are with Guangdong University of Tech-\nnology.\nChunmei Qing is with South China University of Technol-\nogy.\nLiang Lin is with Sun Yat-Sen University.\nThis work was\nsupported in part by National Natural Science Foundation of China\n(NSFC) under Grant No. 62206060, in Part by Natural Science Foun-\ndation of Guangdong Province (2022A1515011555, 2023A1515012568,\n2023A1515012561), Guangdong Provincial Key Laboratory of Human\nDigital Twin (2022B1212010004), and in part by Guangzhou Basic and\nApplied Basic Research Foundation under Grant No. SL2022A04J01626.\nReference\nSource\nNED\nASCCL\nFigure 1. Several examples are generated by the current advanced\nNED with and without the proposed ASCCL algorithm. Incor-\nporating the ASCCL can better manipulate the expressions and\nmeanwhile preserve mouth shapes.\nprocess.\nWe conduct extensive experiments on variant\ndatasets, and the results demonstrate the effectiveness of\nthe proposed ASCCL algorithm. Code is publicly available\nat https://github.com/jianmanlincjx/ASCCL\n1. Introduction\nSpeech-preserving\nfacial\nexpression\nmanipulation\n(SPFEM), which aims to manipulate facial emotions\nwhile preserving the mouth animations in static images\nor dynamic videos, can enhance human expressiveness\nand thus benefit variant applications including virtual\navatars and film & television production. For example, it\nrequires lots of effects and repeated remakes to capture\nan expected actor’s emotions in a movie & shooting. In\ncontrast, a robust SPFEM system can easily modify the\nfacial emotions to achieve comparable performance in the\npost-production stage and thus is urgently expected.\nCurrent SPFEM literature either predominantly previous\nface reenactment algorithms [10, 28] or harnesses decou-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n7267",
        "chosen": "This paper proposes a novel adaptive spatial coherent correlation learning (ASCCL) algorithm for speech-preserving facial expression manipulation (SPFEM). The algorithm models the correlation of local facial animations and integrates it as a metric to supervise the manipulation process, ensuring better preservation of mouth animations associated with spoken content.",
        "rejected": "Speech-preserving facial expression manipulation aims to modify facial emotions while preserving mouth animations. This paper proposes an adaptive spatial coherent correlation learning (ASCCL) algorithm that improves expression manipulation while better preserving speech-related facial animations. Extensive experiments validate the effectiveness of ASCCL."
    },
    {
        "prompt": "summary: Multimodal Large Language Models (MLLMs) have\nendowed LLMs with the ability to perceive and under-\nstand multi-modal signals.\nHowever, most of the exist-\ning MLLMs mainly adopt vision encoders pretrained on\ncoarsely aligned image-text pairs, leading to insufficient ex-\ntraction and reasoning of visual knowledge. To address this\nissue, we devise a dual-Level vIsual knOwledge eNhanced\nMultimodal Large Language Model (LION), which empow-\ners the MLLM by injecting visual knowledge in two lev-\nels. 1) Progressive incorporation of fine-grained spatial-\naware visual knowledge. We design a vision aggregator\ncooperated with region-level vision-language (VL) tasks to\nincorporate fine-grained spatial-aware visual knowledge\ninto the MLLM. To alleviate the conflict between image-\nlevel and region-level VL tasks during incorporation, we\ndevise a dedicated stage-wise instruction-tuning strategy\nwith mixture-of-adapters. This progressive incorporation\nscheme contributes to the mutual promotion between these\ntwo kinds of VL tasks. 2) Soft prompting of high-level se-\nmantic visual evidence. We facilitate the MLLM with high-\nlevel semantic visual evidence by leveraging diverse image\ntags. To mitigate the potential influence caused by imper-\nfect predicted tags, we propose a soft prompting method by\nembedding a learnable token into the tailored text instruc-\ntion. Comprehensive experiments on several multi-modal\nbenchmarks demonstrate the superiority of our model (e.g.,\nimprovement of 5% accuracy on VSR and 3% CIDEr on\nTextCaps over InstructBLIP, 5% accuracy on RefCOCOg\nover Kosmos-2).\n1. Introduction\nRecently, Large Language Models (LLMs) have demon-\nstrated remarkable zero-shot abilities on various linguistic\ntasks.\nAssisted by LLMs, several multimodal large lan-\nguage models (MLLMs), such as MiniGPT-4 [54], Otter\n†Corresponding author\nInstruction:\nWhat is in the photo?\nLLM\nVision\nEncoder\nLION\nchicken, potatoes\nand cauliflower on a\nwhite plate with a\nknife and fork in\nthe middle of the\ntable next to a red\nwine bottle that\nsits on the table\nnext to it.\nInstruction:\nWhat is in the photo?\nLLM\nGrilled chicken breast [                      ]\nSpatial-aware Visual Knowledge:\nSemantic Visual Evidence:\n cauliflower, chicken, table,\nplate, food, meat,  potato, white\nChicken, potatoes,\nand cauliflower on a\nwhite plate with a\nwooden table in\nthe foreground.\nThe plate is sitting\non a wooden table.\nResponse:\nResponse:\nExsiting MLLMs\nBridge\nModule\nBridge\nModule\nVision\nEncoder\nA piece of steamed cauliflower [                      ] \nRoasted baby yellow potato [                      ]  \nMixture\nof\nAdapters\nFigure 1. Comparison between existing MLLMs and LION\n.\nThe existing MLLM generates a vague and inaccurate response,\nwhile LION provides a more precise and contextually accurate de-\nscription by progressively incorporating spatial-aware knowledge\nand softly prompting semantic visual evidence.\n[19], and InstructBLIP [7], achieve significant improve-\nments in reasoning abilities to deal with various vision-\nlanguage (VL) tasks.\nIn most of the existing MLLMs, the visual informa-\ntion is mainly extracted from a vision encoder pretrained\nwith image-level supervision (e.g., CLIP [32]), and then\nare adapted to a LLM by using a tiny bridge module. This\nmakes these MLLMs inherently possess limited image un-\nderstanding capabilities [21]. As shown in Fig. 1, the in-\nsufficient visual information misleads MLLMs to provide\nerroneous and hallucinated responses. An intuitive solution\nto this problem is to replace or tune the vision encoder [41].\nHowever, it requires pretraining on massive data or suffers\nfrom the catastrophic forgetting issue [35, 36, 50], which\ndiminishes the practical efficacy of this strategy.\nThese\npredicaments highlight that the insufficient extraction of vi-\nsual knowledge has become a central obstacle impeding the\ndevelopment of MLLMs.\nTo overcome this dilemma, as depicted in Fig. 1, we de-\nvise a dual-Level vIsual knOwledge eNhanced Multimodal\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n26540",
        "chosen": "This paper introduces LION, a dual-Level vIsual knOwledge eNhanced Multimodal Large Language Model. LION incorporates fine-grained spatial-aware visual knowledge and soft prompts of high-level semantic visual evidence into the multimodal large language model (MLLM), improving performance on various multi-modal benchmarks.",
        "rejected": "Multimodal Large Language Models (MLLMs) face challenges in extracting and reasoning visual knowledge. This work presents LION, enhancing MLLMs by injecting visual knowledge at two levels: progressive incorporation of fine-grained spatial-aware knowledge and soft prompting of high-level semantic visual evidence. LION shows superior performance on several multi-modal benchmarks."
    },
    {
        "prompt": "summary: Recent progress in Large Multimodal Models (LMM)\nhas opened up great possibilities for various applications in\nthe ﬁeld of human-machine interactions. However, develop-\ning LMMs that can comprehend, reason, and plan in com-\nplex and diverse 3D environments remains a challenging\ntopic, especially considering the demand for understanding\npermutation-invariant point cloud representations of the 3D\nscene. Existing works seek help from multi-view images by\nprojecting 2D features to 3D space, which inevitably leads\nto huge computational overhead and performance degrada-\ntion. In this paper, we present LL3DA, a Large Language\n3D Assistant that takes point cloud as the direct input and\nresponds to both text instructions and visual interactions.\nThe additional visual interaction enables LMMs to better\ncomprehend human interactions with the 3D environment\nand further remove the ambiguities within plain texts. Ex-\nperiments show that LL3DA achieves remarkable results\nand surpasses various 3D vision-language models on both\n3D Dense Captioning and 3D Question Answering.\n1. Introduction\nThe recent surge in Large Language Model (LLM) fam-\nilies [14, 31, 46, 52, 63] opens up great opportunities for\naddressing various machine learning tasks in a generalized\nway [30, 32, 40, 60]. During this LLM carnival, researchers\nare also seeking generalized LLM solutions for various vi-\nsion language tasks [37, 50, 62]. Among these, LLM-based\n3D scene understanding is a valuable topic that would ben-\neﬁt the development of autonomous driving [9, 23] and em-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n26428",
        "chosen": "This paper presents LL3DA, a Large Language 3D Assistant that takes point cloud as direct input and responds to both text instructions and visual interactions. The model enables better comprehension of human interactions with 3D environments and removes ambiguities in plain texts, achieving remarkable results on 3D Dense Captioning and 3D Question Answering.",
        "rejected": "Large Multimodal Models (LMM) struggle with understanding permutation-invariant point cloud representations of 3D scenes. LL3DA is introduced as a solution, taking point clouds directly as input and responding to text instructions and visual interactions. Experiments demonstrate LL3DA's superior performance in 3D Dense Captioning and 3D Question Answering tasks."
    },
    {
        "prompt": "summary: For image super-resolution (SR), bridging the gap be-\ntween the performance on synthetic datasets and real-world\ndegradation scenarios remains a challenge. This work in-\ntroduces a novel ”Low-Res Leads the Way” (LWay) train-\ning framework, merging Supervised Pre-training with Self-\nsupervised Learning to enhance the adaptability of SR mod-\nels to real-world images.\nOur approach utilizes a low-\nresolution (LR) reconstruction network to extract degrada-\ntion embeddings from LR images, merging them with super-\nresolved outputs for LR reconstruction. Leveraging unseen\nLR images for self-supervised learning guides the model\nto adapt its modeling space to the target domain, facili-\ntating fine-tuning of SR models without requiring paired\nhigh-resolution (HR) images. The integration of Discrete\nWavelet Transform (DWT) further refines the focus on high-\nfrequency details.\nExtensive evaluations show that our\nmethod significantly improves the generalization and de-\ntail restoration capabilities of SR models on unseen real-\nworld datasets, outperforming existing methods. Our train-\ning regime is universally compatible, requiring no network\narchitecture modifications, making it a practical solution\nfor real-world SR applications.\n1. Introduction\nImage super-resolution (SR) aims to restore high-resolution\n(HR) images from their low-resolution (LR) or degraded\ncounterparts. The inception of the deep-learning-based SR\nmodel can be traced back to SRCNN [14]. Recently, ad-\nvancements in deep learning models have substantially en-\nhanced SR performance [1, 6, 8–10, 12, 25–27, 39, 51, 52,\n54, 56], particularly in addressing specific degradation types\nlike bicubic downsampling. Nevertheless, the efficacy of\nSR models is generally restricted by the degradation strate-\ngies employed during the training phase, posing great chal-\nlenges in complex real-world applications.\n*Lei Zhu (leizhu@ust.hk) is the corresponding author.\nSL space on \nsynthetic data\nSSL space on \nreal test data\nUnseen \nReal-world Image\nHigh quality\nLow fidelity\nLow quality\nHigh fidelity\nGround Truth\nHigh quality\nHigh fidelity\nOurs\npull\nSL space on \nsynthetic data\nSSL space on \nreal test data\nZoom In\nZoom In\nZoom In\nZoom In\nZoom In\nFigure 1.\nOur proposed training method combine the benefits\nof supervised learning (SL) on synthetic data and self-supervised\nlearning (SSL) on the unseen test images, achieve high quality and\nhigh fidelity SR results.\nIn the realm of real-world SR, as shown in Figure 2,\ntraining approaches can primarily be categorized into three\nmain paradigms.\n(a) Unsupervised Learning with Un-\npaired Data: Methods within this paradigm [2, 3, 15, 38,\n40, 45, 46, 55] commonly utilize Generative Adversarial\nNetworks (GAN) architecture to learn target distributions\nwithout paired data. Using one or multiple discriminators,\nthey distinguish between generated images and actual sam-\nples, guiding the generator to model accurately. However,\nas this approach heavily relies on external data, it encoun-\nters significant challenges when facing scarce target domain\ndata, particularly in real-world scenarios. The GAN frame-\nwork for unsupervised learning also has some drawbacks.\nFirstly, it inherently struggles with stability during training,\nleading to noticeable artifacts in SR outputs. Secondly, it\nis difficult for a single 0/1 plane modelled by a discrimina-\ntor to accurately separate the target domain [31]. This can\nresult in imprecise distribution learning. (b) Supervised\nLearning with Paired Synthetic Data: BSRGAN [47] and\nReal-ESRGAN [42] have largely enhanced the SR model’s\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25857",
        "chosen": "This paper introduces a novel \"Low-Res Leads the Way\" (LWay) training framework for image super-resolution (SR). The framework combines Supervised Pre-training with Self-supervised Learning, utilizing a low-resolution reconstruction network to extract degradation embeddings and guide model adaptation to real-world images without requiring paired high-resolution images.",
        "rejected": "Image super-resolution (SR) faces challenges bridging synthetic datasets' performance and real-world degradation scenarios. A novel \"Low-Res Leads the Way\" (LWay) training framework merges supervised pre-training with self-supervised learning, significantly improving SR models' generalization and detail restoration capabilities on unseen real-world datasets."
    },
    {
        "prompt": "summary: Pose regression networks predict the camera pose of a\nquery image relative to a known environment. Within this\nfamily of methods, absolute pose regression (APR) has re-\ncently shown promising accuracy in the range of a few cen-\ntimeters in position error. APR networks encode the scene\ngeometry implicitly in their weights. To achieve high ac-\ncuracy, they require vast amounts of training data that, re-\nalistically, can only be created using novel view synthesis\nin a days-long process. This process has to be repeated\nfor each new scene again and again. We present a new\napproach to pose regression, map-relative pose regression\n(marepo), that satisfies the data hunger of the pose re-\ngression network in a scene-agnostic fashion. We condi-\ntion the pose regressor on a scene-specific map representa-\ntion such that its pose predictions are relative to the scene\nmap.\nThis allows us to train the pose regressor across\nhundreds of scenes to learn the generic relation between\na scene-specific map representation and the camera pose.\nOur map-relative pose regressor can be applied to new\nmap representations immediately or after mere minutes of\nfine-tuning for the highest accuracy. Our approach out-\nperforms previous pose regression methods by far on two\npublic datasets, indoor and outdoor.\nCode is available:\nhttps://nianticlabs.github.io/marepo.\n1. Introduction\nToday, neural networks have conquered virtually all sectors\nof computer vision, but there is still at least one task that\nthey struggle with: visual relocalization. What is visual\nrelocalization? Given a set of mapping images and their\nposes, expressed in a common coordinate system, build a\nscene representation. Later, given a query image, estimate\nits pose, i.e. position and orientation, relative to the scene.\nSuccessful approaches to visual relocalization rely\non predicting image-to-scene correspondences, either via\nmatching [8, 21, 38–40, 42, 57] or direct regression [4–\n6, 14, 56], then solving for the pose using traditional and\nrobust algorithms like PnP [18] and RANSAC [17].\nAdopting a different perspective, approaches based on\n20x\nFigure 1. Camera pose estimation performance vs. mapping\ntime. The figure shows the median translation error of several\npose regression relocalization methods on the 7-Scenes dataset and\nthe time required (proportional to the bubble size) to train each\nrelocalizer on the target scenes. Our proposed approach, marepo,\nachieves superior performance – by far – on both metrics, thanks\nto its integration of scene-specific geometric map priors within an\naccurate, map-relative, pose regression framework.\npose regression [12, 25, 32, 46] attempt to perform visual\nrelocalization without resorting to traditional pose solving,\nby using a single feed-forward neural network to infer poses\nfrom single images.\nThe mapping data is treated as a\ntraining set where the camera extrinsics serve as supervi-\nsion. Generally, pose regression approaches come in two\nflavors, but they both struggle with accuracy compared to\ncorrespondence-based methods.\nAbsolute pose regression (APR) methods [7, 24, 25] in-\nvolve training a dedicated pose regressor for each individual\nscene, enabling the prediction of camera poses to that par-\nticular scene. Though the scene coordinate space can be im-\nplicitly encoded in the weights of the neural networks, ab-\nsolute pose regressors exhibit low pose estimation accuracy,\nprimarily due to the often limited training data available for\neach scene, and struggle to generalize to unseen views [43].\nRelative pose regression is a second flavor of pose re-\ngression methods [10, 16, 26, 51, 54].\nThe regressor is\ntrained to predict the relative pose between two images. In a\ntypical inference scenario, the regressor is applied to a pair\nformed by an unseen query and an image from the mapping\nset (typically selected via a nearest neighbor-type match-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n20665",
        "chosen": "This paper presents map-relative pose regression (marepo), a new approach to pose regression for visual re-localization. The method conditions the pose regressor on a scene-specific map representation, allowing training across hundreds of scenes and immediate application to new map representations. The approach outperforms previous methods on public datasets.",
        "rejected": "Pose regression networks predict camera poses from query images but require vast amounts of training data. This paper introduces map-relative pose regression (marepo), conditioning pose regressors on scene-specific maps, enabling training across multiple scenes. Marepo outperforms previous methods on public indoor and outdoor datasets."
    },
    {
        "prompt": "summary: We introduce Mind Artist (MindArt), a novel and efficient\nneural decoding architecture to snap artistic photographs\nfrom our mind in a controllable manner. Recently, progress\nhas been made in image reconstruction with non-invasive\nbrain recordings, but it’s still difficult to generate realis-\ntic images with high semantic fidelity due to the scarcity of\ndata annotations. Unlike previous methods, this work casts\nthe neural decoding into optimal transport (OT) and rep-\nresentation decoupling problems. Specifically, under dis-\ncrete OT theory, we design a graph matching-guided neu-\nral representation learning framework to seek the under-\nlying correspondences between conceptual semantics and\nneural signals, which yields a natural and meaningful self-\nsupervisory task. Moreover, the proposed MindArt, struc-\ntured with multiple stand-alone modal branches, enables\nthe seamless incorporation of semantic representation into\nany visual style information, thus leaving it to have multi-\nmodal reconstruction and training-free semantic editing ca-\n†Corresponding author: Yu Qi.\npabilities. By doing so, the reconstructed images of Min-\ndArt have phenomenal realism both in terms of semantics\nand appearance. We compare our MindArt with leading al-\nternatives, and achieve SOTA performance in different de-\ncoding tasks. Importantly, our approach can directly gen-\nerate a series of stylized “mind snapshots” w/o extra opti-\nmizations, which may open up more potential applications.\nCode is available at https://github.com/JxuanC/\nMindArt.\n1. Introduction\nAs we venture into the frontiers of human creativity, one\ncaptivating question has surfaced: Can our brains serve as\nhidden “artistic cameras”? Let’s envision a world where\npainting tools, and photographic skills are going out of fash-\nion. All you need is to think or gaze upon a visual object\nor immerse yourself in the wonders of nature. Then, your\nbrain will be turned into an artistic lens transforming your\nperceptions into astonishing works of art, perhaps resem-\nbling a Van Gogh-style snapshot with vibrant yellow and\norange sunflowers. To actualize aforesaid goals, we present\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n27207",
        "chosen": "This paper introduces Mind Artist (MindArt), a neural decoding architecture for generating artistic photographs from human thoughts. The method casts neural decoding into optimal transport and representation decoupling problems, enabling multimodal reconstruction and training-free semantic editing capabilities. MindArt achieves state-of-the-art performance in different decoding tasks.",
        "rejected": "Mind Artist (MindArt) proposes a novel neural decoding architecture for generating artistic photographs from brain recordings. Utilizing discrete optimal transport theory, MindArt designs a graph matching-guided framework, achieving phenomenal realism in reconstructed images. It supports multimodal reconstruction and training-free semantic editing capabilities."
    },
    {
        "prompt": "summary: Learning-based stereo matching techniques have made\nsignificant progress. However, existing methods inevitably\nlose geometrical structure information during the feature\nchannel generation process, resulting in edge detail mis-\nmatches. In this paper, the Motif Channel Attention Stereo\nMatching Network (MoCha-Stereo) is designed to address\nthis problem. We provide the Motif Channel Correlation\nVolume (MCCV) to determine more accurate edge match-\ning costs.\nMCCV is achieved by projecting motif chan-\nnels, which capture common geometric structures in fea-\nture channels, onto feature maps and cost volumes. In ad-\ndition, edge variations in the reconstruction error map also\naffect details matching, we propose the Reconstruction Er-\nror Motif Penalty (REMP) module to further refine the full-\nresolution disparity estimation. REMP integrates the fre-\nquency information of typical channel features from the re-\nconstruction error. MoCha-Stereo ranks 1st on the KITTI-\n2015 and KITTI-2012 Reflective leaderboards. Our struc-\nture also shows excellent performance in Multi-View Stereo.\n*Co-first author.\n†Corresponding author.\nCode is avaliable at MoCha-Stereo.\n1. Introduction\nStereo matching remains a foundational challenge in\ncomputer vision, bearing significant relevance to au-\ntonomous driving, virtualization, rendering, and related sec-\ntors [40]. The primary goal of the assignment is to establish\na pixel-wise displacement map, or disparity, which can be\nused to identify the depth of the pixels in the scene. Edge\nperformance of disparity maps is particularly vital in tech-\nniques requiring pixel-level rendering, such as virtual real-\nity and augmented reality, where precise fitting between the\nscene model and image mapping is essential [23]. This un-\nderscores the need for a close alignment between the edges\nof the disparity map and the original RGB image.\nTraditional stereo matching relies on global [8], semi-\nglobal [13], or local [2] grayscale relationships between left\nand right view pixels. These methods struggle to fully lever-\nage scene-specific prior knowledge. Achieving optimal re-\nsults often involves human observation, this tuning process\ncan be resource-intensive in scenes with complex images\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n27768",
        "chosen": "This paper proposes MoCha-Stereo, a motif channel attention stereo matching network. The approach introduces the Motif Channel Correlation Volume (MCCV) and Reconstruction Error Motif Penalty (REMP) module to improve edge matching and full-resolution disparity estimation. MoCha-Stereo ranks 1st on KITTI-2015 and KITTI-2012 Reflective leaderboards and shows excellent performance in Multi-View Stereo.",
        "rejected": "For image super-resolution, this study develops a new training framework combining supervised learning on synthetic data with self-supervised learning on real test data, termed \"Low-Res Leads the Way\". This approach enhances adaptability and fine-tuning of SR models without paired HR images, showing improved generalization on real-world datasets."
    },
    {
        "prompt": "summary: Recent advances in generative diffusion models have en-\nabled the previously unfeasible capability of generating 3D\nassets from a single input image or a text prompt. In this\nwork, we aim to enhance the quality and functionality of\nthese models for the task of creating controllable, photo-\nrealistic human avatars. We achieve this by integrating a\n3D morphable model into the state-of-the-art multi-view-\nconsistent diffusion approach. We demonstrate that accu-\nrate conditioning of a generative pipeline on the articulated\n3D model enhances the baseline model performance on the\ntask of novel view synthesis from a single image. More im-\nportantly, this integration facilitates a seamless and accu-\nrate incorporation of facial expression and body pose con-\ntrol into the generation process. To the best of our knowl-\nedge, our proposed framework is the first diffusion model to\nenable the creation of fully 3D-consistent, animatable, and\nphotorealistic human avatars from a single image of an un-\nseen subject; extensive quantitative and qualitative evalua-\ntions demonstrate the advantages of our approach over ex-\nisting state-of-the-art avatar creation models on both novel\nview and novel expression synthesis tasks. The code for our\nproject is publicly available.\n1. Introduction\nThe field of photorealistic controllable human avatar gener-\nation has been subject to several technological leaps in the\nrecent decade. The introduction of large 3D scan collec-\ntions has facilitated the construction of expressive, articu-\nlated models of 3D human bodies [47, 55], faces [5, 39, 56],\nand hands [65]. From the outset, one of the primary appli-\ncations of these models was to reconstruct a 3D avatar from\nhighly under-constrained setups, such as monocular video\nor a single image [5, 18, 37, 55].\nWhile allowing for rich semantic information to be in-\nferred, these 3D morphable models were limited in the level\nof photorealism due to their focus on minimally clothed\nbodies and face regions, as well as their reliance on the stan-\ndard mesh-based computer graphics pipelines for rendering.\nRecently, the task of generating photorealistic avatars\n[57] gained significant attention from the research commu-\nnity due to its potential to revolutionize our ways of digital\ncommunication. Here, combining novel neural rendering\ntechniques [50, 86] with articulated human models allowed\nfor a new level of generated image quality. However, the\nbest-performing models here still require a significant vi-\nsual input, such as calibrated multi-view images [21, 78] or\nmonocular video sequences of the subject [61, 82, 95, 99].\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n10359",
        "chosen": "This paper enhances the quality and functionality of generative diffusion models for creating controllable, photorealistic human avatars from a single image. By integrating a 3D morphable model into a multi-view-consistent diffusion approach, the framework improves novel view synthesis and enables accurate facial expression and body pose control. Extensive evaluations show advantages over existing avatar creation models.",
        "rejected": "Recent advances have enabled the generation of 3D human avatars from single images using generative diffusion models. This work integrates a 3D morphable model into a multi-view-consistent diffusion approach, enhancing photorealism and controllability of facial expressions and body poses. The proposed method creates fully 3D-consistent, animatable avatars with superior performance compared to existing avatar creation models."
    },
    {
        "prompt": "summary: Neural Radiance Field (NeRF) has been widely recog-\nnized for its excellence in novel view synthesis and 3D\nscene reconstruction.\nHowever, their effectiveness is in-\nherently tied to the assumption of static scenes, rendering\nthem susceptible to undesirable artifacts when confronted\nwith transient distractors such as moving objects or shad-\nows. In this work, we propose a novel paradigm, namely\n“Heuristics-Guided Segmentation” (HuGS), which signifi-\ncantly enhances the separation of static scenes from tran-\nsient distractors by harmoniously combining the strengths\nof hand-crafted heuristics and state-of-the-art segmentation\nmodels, thus significantly transcending the limitations of\nprevious solutions. Furthermore, we delve into the metic-\nulous design of heuristics, introducing a seamless fusion\nof Structure-from-Motion (SfM)-based heuristics and color\nresidual heuristics, catering to a diverse range of texture\nprofiles. Extensive experiments demonstrate the superiority\nand robustness of our method in mitigating transient dis-\ntractors for NeRFs trained in non-static scenes. Project\npage: https://cnhaox.github.io/NeRF-HuGS/\n1. Introduction\nNeural Radiance Fields (NeRF) [29] have garnered signif-\nicant attention for their remarkable achievements in novel\nview synthesis. Utilizing multiple-view images, NeRF con-\nceptualizes the 3D scene as a neural field [54] and produces\nhighly realistic renderings through advanced volume ren-\ndering techniques. This capability has opened the door to\na wide array of downstream applications including 3D re-\nconstruction [22, 43, 48], content generation [23, 33, 36],\n*Corresponding author is Guanbin Li. This work was supported in part\nby the National Natural Science Foundation of China (NO. 62322608),\nin part by the CAAI-MindSpore Open Fund, developed on OpenI Com-\nmunity, in part by the Open Project Program of State Key Labora-\ntory of Virtual Reality Technology and Systems, Beihang University\n(No.VRLAB2023A01), in part by Shenzhen Science and Technology Pro-\ngram KQTD20210811090149095 and also the Pearl River Talent Recruit-\nment Program 2019QN01X226.\nSeg. w/ Prior (“carton, bottle, car…”)\nHeuristics (Color Residual)\nHuGS (SfM + Color Residual)\nStatic Map\n(a) Segmentation-based Method\n(b) Heuristics-based Method\n(c) NeRF-HuGS (Ours)\nNeRF Training and Rendering\nStatic Map\nNeRF Training and Rendering\nStatic Map\nNeRF Training and Rendering\nTrain\nImages\n···\nFigure 1.\nComparison between previous methods and the\nproposed Heuristics-Guided Segmentation (HuGS) paradigm.\nWhen training NeRF with static scenes disturbed by transient\ndistractors, (a) segmentation-based methods rely on prior knowl-\nedge and cannot identify unexpected transient objects (e.g., pizza);\n(b) heuristics-based methods are more generalizable but inaccu-\nrate (e.g., tablecloth textures); (c) our method combines their\nstrengths and produces highly accurate transient vs. static sepa-\nrations, thereby significantly improving NeRF results.\nsemantic understanding [14, 42, 58], etc.\nHowever, the images used as NeRF training data must\nmeet several strict conditions, one of which is the require-\nment for content consistency and stability. In other words,\nthe native NeRF model operates under the assumption of a\nstatic scene. Any elements that exhibit motion or inconsis-\ntency throughout the entire data capture session, which we\nrefer to as “transient distractors”, can introduce undesirable\nartifacts into the reconstructed 3D geometry. However, the\npresence of transient distractors is nearly inevitable in real-\nworld scenarios. For instance, in outdoor settings, random\nappearances of pedestrians and vehicles may occur during\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n19436",
        "chosen": "This paper proposes NeRF-HuGS, a novel paradigm combining heuristics and segmentation models to improve NeRF's performance in non-static scenes. The method fuses Structure-from-Motion (SfM)-based heuristics and color residual heuristics, effectively separating static scenes from transient distractors. Experiments demonstrate the method's superiority in mitigating artifacts caused by moving objects or shadows.",
        "rejected": "NeRF has shown excellence in novel view synthesis but struggles with transient distractors. A new paradigm named Heuristics-Guided Segmentation (HuGS) combines hand-crafted heuristics and state-of-the-art segmentation models to improve separation between static scenes and transient distractors, significantly mitigating artifacts in NeRFs trained in non-static scenes."
    },
    {
        "prompt": "summary: Absolute Pose Regression (APR) methods use deep neu-\nral networks to directly regress camera poses from RGB\nimages. However, the predominant APR architectures only\nrely on 2D operations during inference, resulting in limited\naccuracy of pose estimation due to the lack of 3D geometry\nconstraints or priors. In this work, we propose a test-time\nrefinement pipeline that leverages implicit geometric con-\nstraints using a robust feature field to enhance the ability of\nAPR methods to use 3D information during inference. We\nalso introduce a novel Neural Feature Synthesizer (NeFeS)\nmodel, which encodes 3D geometric features during train-\ning and directly renders dense novel view features at test\ntime to refine APR methods. To enhance the robustness of\nour model, we introduce a feature fusion module and a pro-\ngressive training strategy. Our proposed method achieves\nstate-of-the-art single-image APR accuracy on indoor and\noutdoor datasets.\nCode will be released at https://\ngithub.com/ActiveVisionLab/NeFeS.\n1. Introduction\nCamera relocalization is a crucial task that allows machines\nto understand their position and orientation in 3D space.\nIt is an essential prerequisite for applications such as aug-\nmented reality, robotics, and autonomous driving, where the\naccuracy and efficiency of pose estimation are important.\nRecently, Absolute Pose Regression (APR) methods [21–\n23] have been shown to be effective in directly estimating\ncamera pose from RGB images using convolutional neu-\nral networks. The simplicity of APR’s architecture offers\nseveral potential advantages over classical geometry-based\nmethods [5, 43, 45], involving end-to-end training, cheap\ncomputation cost, and low memory demand.\nLatest advances in APR, particularly the use of novel\nview synthesis (NVS) [10, 11, 29, 32, 33, 49] to generate\nnew images from random viewpoints as data augmentation\nduring training, have significantly improved the pose re-\ngression performance. Despite this, state-of-the-art (SOTA)\n(a) Before Pose Refinement\n(b) After Pose Refinement\nFigure 1. Our pose refinement (R) improves (coarse) pose predic-\ntions from other methods using novel feature synthesis to achieve\npixel-wise alignment.\nTop left / right: 3D plots of predicted\n(green) and ground-truth (red) camera positions. Bottom left /\nright: alignment between rendered features and query image.\nAPRs still have the following limitations: (i) They predict\nthe pose of a query image by passing it through a CNN,\nwhich typically disregards geometry at inference time. This\ncauses APR networks to struggle to generalize to view-\npoints that the training data fails to cover [46]; (ii) The un-\nlabeled data, often sampled from the validation/testing set,\nused for finetuning the APR network [8, 10, 11] may not\nbe universally available in real-life circumstances, and this\nsemi-supervised finetuning is also time-consuming.\nTo address these limitations, we propose a novel test-\ntime refinement pipeline for APR methods. Unlike prior\nworks that explore extended Kalman filters [34], pose graph\noptimization [8], or pose auto-encoders [49], our method\nintegrates an implicit representation based geometric refine-\nment into an end-to-end learning framework, where gradi-\nents can be backpropagated to the APR network. We test\nour proposed method across different APR architectures to\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n20987",
        "chosen": "This paper presents a test-time refinement pipeline for Absolute Pose Regression (APR) methods, enhancing their ability to use 3D information during inference. The proposed Neural Feature Synthesizer (NeFeS) model encodes 3D geometric features during training and renders dense novel view features at test time. The method achieves state-of-the-art single-image APR accuracy on indoor and outdoor datasets.",
        "rejected": "APR methods for camera pose estimation face limitations due to lack of 3D geometry constraints during inference. This work proposes a test-time refinement pipeline that leverages implicit geometric constraints through a robust feature field, introducing a Neural Feature Synthesizer (NeFeS) to encode 3D features and refine APR accuracy."
    },
    {
        "prompt": "summary: We explore the boundaries of scaling up a multilingual\nvision and language model, both in terms of size of the com-\nponents and the breadth of its training task mixture. Our\nmodel achieves new levels of performance on a wide-range\nof varied and complex tasks, including multiple image-based\ncaptioning and question-answering tasks, image-based doc-\nument understanding and few-shot (in-context) learning, as\nwell as object detection, video question answering, and video\ncaptioning. Our model advances the state-of-the-art on most\nvision-and-language benchmarks considered (20+ of them).\nFinally, we observe emerging capabilities, such as complex\ncounting and multilingual object detection, tasks that are\nnot explicitly in the training mix.\n1. Introduction\nThe success of scaling language models [1–4] makes it ap-\npealing to similarly scale Vision-Language (V&L) models,\nand investigate the improvements, capabilities, and emergent\nproperties of such models. Inspired by the work in [5], we\npresent PaLI-X, a multilingual vision and language model\nwith reusable scaled-up components, consisting of a pre-\ntrained large-capacity visual encoder (using [6] as the start-\ning point) and a pretrained language-only encoder-decoder\n(using [7] as the starting point), further trained at-scale on\na vision-and-language data mixture using a combination of\nself-supervision and full-supervision signals.\nOne clear pattern that emerges from the combination\nof results from PaLI [5] and the work we present in this\npaper is that scaling both V&L components together brings\nincreases in performance across a wide range of tasks. We\nshow this by comparing against the same benchmarks used\nfor PaLI (Fig. 1, Left), and also against new benchmarks\nfor which the new capabilities of PaLI-X are evaluated (e.g.,\nChartQA, AI2D, DocVQA, InfographicVQA, as well as\nvideo understanding tasks). We observe that scaling leads\nto large improvements over the results of the PaLI model,\nand also over specialized large-scale models that are trained\nspeciﬁcally to solve certain tasks, often with the help of\n(often much larger) text-only LLMs [8]. In particular, we\nﬁnd that increasing both the effective capacity of the vision\ncomponent (which [9] does more unilaterally) and of the\nlanguage component (which [10] also does unilaterally) is\nbeneﬁcial; the new PaLI-X model provides more balanced\nparameter allocation than any other prior work (roughly\n40%-60% split of the total capacity).\nAside from demonstrating the consistent impact of scale,\nthe original contribution of PaLI-X consists in leveraging\nthe mixture-of-objectives proposed in [7] for vision-and-\nlanguage modeling, and showing that it results in a model\nthat improves both state-of-the-art results and the Pareto\nfrontier for ﬁne-tuning and few-shot (Fig. 1, Right).\nWe also observe emergent properties based on PaLI-X’s\nresults compared to previous models with similar architec-\nture but smaller sizes. For instance, we report drastically im-\nproved performance on the counting ability (See Table 1 and\nAppendix B), both for the plain variety (count all instances\nof a class) and the complex variety (count instances based\non a natural language description), that are not attributable\nto training design1. Additionally, we present qualitative in-\nsights into the model’s performance (Appendix A), with an\n1Plain counting is usually achievable via good object detection, while\ncomplex counting requires a ﬁne-grained understanding of the alignment\nbetween language-based speciﬁcations and visually-based occurrences.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n14432",
        "chosen": "This paper explores scaling up a multilingual vision and language model, achieving new performance levels on various complex tasks. The model advances the state-of-the-art on most vision-and-language benchmarks and shows emerging capabilities like complex counting and multilingual object detection.",
        "rejected": "Scaling up multilingual vision and language models achieves improved performance across various tasks, including image captioning, question answering, and object detection. PaLI-X demonstrates enhanced capabilities like complex counting and multilingual object detection by scaling both visual and language components, resulting in balanced parameter allocation and state-of-the-art results on multiple benchmarks."
    },
    {
        "prompt": "summary: Due to the resource-intensive nature of training vision-\nlanguage models on expansive video data, a majority\nof studies have centered on adapting pre-trained image-\nlanguage models to the video domain. Dominant pipelines\npropose to tackle the visual discrepancies with additional\ntemporal learners while overlooking the substantial dis-\ncrepancy for web-scaled descriptive narratives and con-\ncise action category names, leading to less distinct semantic\nspace and potential performance limitations. In this work,\nwe prioritize the refinement of text knowledge to facilitate\ngeneralizable video recognition. To address the limitations\nof the less distinct semantic space of category names, we\nprompt a large language model (LLM) to augment action\nclass names into Spatio-Temporal Descriptors thus bridg-\ning the textual discrepancy and serving as a knowledge base\nfor general recognition. Moreover, to assign the best de-\nscriptors with different video instances, we propose Optimal\nDescriptor Solver, forming the video recognition problem as\nsolving the optimal matching flow across frame-level repre-\nsentations and descriptors. Comprehensive evaluations in\nzero-shot, few-shot, and fully supervised video recognition\nhighlight the effectiveness of our approach. Our best model\nachieves a state-of-the-art zero-shot accuracy of 75.1% on\nKinetics-600.\n1. Introduction\nLarge-scale contrastive language-image pre-training [25,\n46, 65] have shown remarkable performance in various\ncomputer vision tasks.\nThe visual-semantic joint space\nnot only serves powerful visual representation but also en-\nables few/zero-shot transferring to downstream tasks with\nthe reference of natural language. However, training a sim-\nilar model for video recognition can be costly since large-\nscale video-language datasets are exponentially more mas-\nsive [57] due to the extra temporal dimension.\nHence,\na feasible solution is to adapt the pre-trained image-text\nmodels for the task of video recognition. As depicted in\nSki jumping competitions. Jumper in \nmid air with extreme speed. The \nbackground of the forest and slope.\nCLIP Pre-training Data\nVisual \nDiscrepancy\nTextual\nDiscrepancy\nA video of a person\n Ski jumping.\nVideo Recognition Data\nWeb-scaled Descriptive Narratives\nConcise Category Names\nStatic Images\nVideo Sequences\nLow semantic distinction\nHigh semantic distinction\nSpatio-Temporal\nSpatial Inductive\nText Encoder\nVisual Encoder\nTemporal Learner\nMatching\nDominant Pipeline\n: The textual discrepancy is overlooked, which may introduce ambiguity in matching\n: Dominant pipelines tackle the visual discrepancy via additional temporal learners \n(Settings, common objects, certain steps, etc.)\n(Short phrases with constant hard-prompt)\nFigure 1. Motivation of our method. Dominant pipelines propose\nto tackle the visual discrepancies with additional temporal learn-\ners while overlooking the textual discrepancy between descriptive\nnarratives and concise category names. This oversight results in a\nless separable latent space, which may hinder video recognition.\nFig. 1, current methods devise a range of temporal learn-\ners to address the visual discrepancy while preserving text-\ndomain knowledge in the semantic space of action cat-\negory names, often by merging the category name with\nCLIP-style hard-prompts (e.g., “a video of a person {ski\njumping}”) [41, 45, 53, 56, 60]. Despite providing essen-\ntial inter-class correlations that can benefit general recog-\nnition, we speculate this paradigm overlooks the textual\ndiscrepancy between web-scaled descriptive narratives in\nCLIP pre-training and concise category names in down-\nstream video recognition.\nGiven that category names of\nvideo datasets generally consist of verbs and nouns, the\nnouns exhibit variability while the verbs tend to remain con-\nsistent. For instance, playing cello, playing organ & play-\ning violin are distinct actions related to playing instruments.\nThe sole differentiation between these category names lies\nin the noun itself, resulting in low discriminative text em-\nbeddings. This may lead to a less separable semantic space,\npotentially introducing ambiguity in recognition tasks [5].\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n18888",
        "chosen": "This paper proposes refining text knowledge with optimal spatio-temporal descriptors for generalizable video recognition. The method prompts a large language model to augment action class names and proposes an Optimal Descriptor Solver. Evaluations show effectiveness in zero-shot, few-shot, and fully supervised video recognition.",
        "rejected": "To address discrepancies in adapting pre-trained image-language models to video recognition, this paper proposes augmenting action class names into Spatio-Temporal Descriptors using a large language model (LLM). An Optimal Descriptor Solver is introduced to match frame-level representations with descriptors, leading to significant improvements in zero-shot, few-shot, and fully supervised video recognition."
    },
    {
        "prompt": "summary: Recently, some large kernel convnets strike back with\nappealing performance and efﬁciency.\nHowever, given\nthe square complexity of convolution, scaling up kernels\ncan bring about an enormous amount of parameters and\nthe proliferated parameters can induce severe optimization\nproblem. Due to these issues, current CNNs compromise\nto scale up to 51 × 51 in the form of stripe convolution\n(i.e., 51 × 5 + 5 × 51) and start to saturate as the ker-\nnel size continues growing. In this paper, we delve into\naddressing these vital issues and explore whether we can\ncontinue scaling up kernels for more performance gains.\nInspired by human vision, we propose a human-like periph-\neral convolution that efﬁciently reduces over 90% parame-\nter count of dense grid convolution through parameter shar-\ning, and manage to scale up kernel size to extremely large.\nOur peripheral convolution behaves highly similar to hu-\nman, reducing the complexity of convolution from O(K2) to\nO(log K) without backﬁring performance. Built on this, we\npropose Parameter-efﬁcient Large Kernel Network (PeLK).\nOur PeLK outperforms modern vision Transformers and\nConvNet architectures like Swin, ConvNeXt, RepLKNet and\nSLaK on various vision tasks including ImageNet classiﬁca-\ntion, semantic segmentation on ADE20K and object detec-\ntion on MS COCO. For the ﬁrst time, we successfully scale\nup the kernel size of CNNs to an unprecedented 101 × 101\nand demonstrate consistent improvements.\n1. Introduction\nConvolutional Neural Networks (CNNs) have played a piv-\notal role in machine learning for decades [16, 19, 20, 35].\nHowever, their dominance has been greatly challenged by\nVision Transformers (ViTs) [6, 12, 24, 42, 47] over re-\ncent years.\nSome works [32, 44] attribute the powerful\nperformance of ViTs to their large receptive ﬁelds: Facil-\n*Work done during internship at Meituan Inc.\n†Corresponding author.\nitated by self-attention mechanism, ViTs can capture con-\ntext information from a large spatial scope and model long-\nrange dependencies. Inspired by this, recent advances in\nCNNs [11, 23, 25] have revealed that when equipped with\nlarge kernel size (e.g., 31 × 31), pure CNN architecture can\nperform on par with or even better than state-of-the-art ViTs\non various vision tasks.\nAlthough large kernel convnets exhibit strong perfor-\nmance and appealing efﬁciency, a fatal problem exists: the\nsquare complexity O(K2) with respect to kernel size K.\nDue to this problem, directly scaling up kernels will bring\nabout a huge amount of parameters. For instance, the pa-\nrameter of a 31 × 31 kernel is more than 100× larger than\nthat of a typical 3 × 3 counterpart in ResNet [16] and about\n20× as many as that of the 7 × 7 kernel used in Con-\nvNeXt [25]. The proliferated parameters subsequently in-\nduce severe optimization problem, making it useless or even\nharmful to directly scale up kernel size [11, 23, 25]. To\nsolve, RepLKNet [11] re-parameterize a 5×5 kernel par-\nallel to the large one to make up the optimization issue,\nSLaK [23] compromise to use stripe convolution to reduce\nthe complexity to linear and scales up to 51 × 51 (i.e.,\n51 × 5 + 5 × 51). However, this is still a limited inter-\naction range for the resolution of downstream tasks (e.g.,\n2048 × 512 on ADE20K) and more importantly, stripe con-\nvolution lacks the range perception of dense convolution,\nthus we conjecture it may undermine the model’s spatial\nperception capacity.\nIn this paper, we ﬁrst conduct a comprehensive dissec-\ntion of convolution forms under a uniﬁed modern frame-\nwork (i.e., SLaK [23]). We empirically verify our conjec-\nture that dense grid convolution outperforms stripe convo-\nlution with consistent improvements across multiple kernel\nsizes. This phenomenon holds not only for classiﬁcation\ntask, but even more pronounced for downstream tasks, in-\ndicating the essential advantage of dense convolution over\nstripe form. Nevertheless, as mentioned above, the square\ncomplexity of large dense convolution leads to the prolif-\nerated parameters, causing rapidly increasing model size,\ngreater optimization difﬁculty and thus preventing it from\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n5557",
        "chosen": "This paper introduces Parameter-efficient Large Kernel Networks (PeLK) with peripheral convolution, inspired by human vision. The network efficiently reduces parameters, scales up kernel size, and outperforms modern vision Transformers and ConvNet architectures on various vision tasks.",
        "rejected": "Large kernel convnets can achieve high performance but suffer from optimization issues as kernel sizes increase. Inspired by human vision, a peripheral convolution method reduces parameter counts by over 90% while scaling up kernel size to extremely large dimensions. The Parameter-efficient Large Kernel Network (PeLK) outperforms modern vision transformers and ConvNet architectures on various vision tasks, successfully scaling kernels to an unprecedented 101x101 size."
    },
    {
        "prompt": "summary: Domain Generalization (DG) aims to resolve distribu-\ntion shifts between source and target domains, and cur-\nrent DG methods are default to the setting that data from\nsource and target domains share identical categories. Nev-\nertheless, there exists unseen classes from target domains\nin practical scenarios.\nTo address this issue, Open Set\nDomain Generalization (OSDG) has emerged and several\nmethods have been exclusively proposed. However, most ex-\nisting methods adopt complex architectures with slight im-\nprovement compared with DG methods. Recently, vision-\nlanguage models (VLMs) have been introduced in DG fol-\nlowing the fine-tuning paradigm, but consume huge train-\ning overhead with large vision models. Therefore, in this\npaper, we innovate to transfer knowledge from VLMs to\nlightweight vision models and improve the robustness by\nintroducing Perturbation Distillation (PD) from three per-\nspectives, including Score, Class and Instance (SCI), named\nSCI-PD. Moreover, previous methods are oriented by the\nbenchmarks with identical and fixed splits, ignoring the di-\nvergence between source domains. These methods are re-\nvealed to suffer from sharp performance decay with our\nproposed new benchmark Hybrid Domain Generalization\n(HDG) and a novel metric H2-CV, which construct var-\nious splits to comprehensively assess the robustness of\nalgorithms.\nExtensive experiments demonstrate that our\nmethod outperforms state-of-the-art algorithms on multi-\nple datasets, especially improving the robustness when con-\nfronting data scarcity.\n1. Introduction\nDeep learning has attained remarkable success on var-\nious downstream tasks in computer vision, typically un-\n*Corresponding author\n†Source code is available at https://github.com/znchen666/HDG.\nFigure 1. The balance between model performance and training\ntime consumption. Model performance is evaluated on the average\nH-score of different splits based on the proposed HDG benchmark.\nOur method achieves superior performance with less training time\ncompared with state-of-the-art (SOTA) methods in OSDG.\nder the presumption that both training and test samples\nare Independent and Identically Distributed (IID) with the\nsame label space. However, real-world data often exhibits\nunpredictable distributions, leading to the failure of deep\nneural networks. To address such distribution shifts, Do-\nmain Generalization (DG) is first introduced to leverage\ndata from multiple source domains to achieve generaliza-\ntion on unseen target domains, from the perspective of\ndomain-invariant learning [16, 27, 32, 37, 39, 43], data\naugmentation [8, 23, 52, 58, 59], and learning strategies\n[2, 6, 20, 30, 48, 56].\nHowever, it has been observed\nthat most existing domain generalization methods assume\na closed-set distribution, where the label space remains\nidentical across the source and target domain. To address\nthis limitation, Open Set Domain Generalization (OSDG)\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n23501",
        "chosen": "This paper addresses Open Set Domain Generalization (OSDG) by transferring knowledge from vision-language models to lightweight vision models and introducing Perturbation Distillation (PD) from three perspectives. The method improves robustness and outperforms state-of-the-art algorithms on multiple datasets.",
        "rejected": "Domain Generalization (DG) faces challenges when dealing with unseen classes in target domains. Open Set Domain Generalization (OSDG) addresses this issue, but existing methods often require complex architectures. This paper introduces Perturbation Distillation (PD) from three perspectives (Score, Class, Instance), named SCI-PD, which transfers knowledge from vision-language models to lightweight vision models, improving robustness and performance on multiple datasets, especially under data scarcity."
    },
    {
        "prompt": "summary: Weakly-supervised Video Anomaly Detection (wVAD)\naims to detect frame-level anomalies using only video-\nlevel labels in training. Due to the limitation of coarse-\ngrained labels, Multi-Instance Learning (MIL) is prevail-\ning in wVAD. However, MIL suffers from insufficiency of\nbinary supervision to model diverse abnormal patterns. Be-\nsides, the coupling between abnormality and its context hin-\nders the learning of clear abnormal event boundary.\nIn\nthis paper, we propose prompt-enhanced MIL to detect var-\nious abnormal events while ensuring clear event bound-\naries. Concretely, we design the abnormal-aware prompts\nby using abnormal class annotations together with learn-\nable prompt, which can incorporate semantic priors into\nvideo features dynamically.\nThe detector can utilize the\nsemantic-rich features to capture diverse abnormal pat-\nterns. In addition, normal context prompt is introduced to\namplify the distinction between abnormality and its context,\nfacilitating the generation of clear boundary. With the mu-\ntual enhancement of abnormal-aware and normal context\nprompt, the model can construct discriminative represen-\ntations to detect divergent anomalies without ambiguous\nevent boundaries. Extensive experiments demonstrate our\nmethod achieves SOTA performance on three public bench-\nmarks.\nThe code is available at https://github.\ncom/Junxi-Chen/PE-MIL.\n1. Introduction\nTo identify anomaly at frame level in video, Video Anomaly\nDetection (VAD) has become vital in critical areas, e.g.,\nsurveillance systems [19], medical imaging [33] and au-\ntonomous driving [1].\nFor great generalization ability\nacross diverse scenes, researchers [6, 8, 29, 31, 32, 43, 45]\n*Corresponding authors\nFigure 1. (a) Illustration of prompt-enhanced MIL. In multi-modal\nfeature space, text prompts integrate abnormal-aware semantic pri-\nors into visual features. NCP incorporates normal semantic into\nambiguous context feature. In such manner, our method learns\na more discriminative representation to deliver a precise anomaly\ndetection. (b) Anomaly detection examples of our method.\nturn to weakly-supervised VAD (wVAD) which only lever-\nages video-level labels. Primarily, wVAD faces two key\nchallenges: 1) detecting complex anomalous patterns in var-\nied scenarios where temporal relationship and visual ap-\npearance of anomalies exhibit substantial discrepancies; 2)\ngenerating clear abnormal event boundaries in the absence\nof fine-grained boundary annotations.\nTo tackle wVAD, prior works [52, 55] generate noisy\nframe-level abnormal labels and reduce noise subsequently,\nbut such manner limits the generalization ability to un-\nseen scenarios. Recently, Multi-Instance Learning (MIL) is\nleveraged by most methods [5, 6, 8, 23, 29, 45] to tackle\nwVAD due to its ability to model patterns under coarse-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n18319",
        "chosen": "This paper proposes prompt-enhanced Multi-Instance Learning (MIL) for weakly supervised video anomaly detection. The method uses abnormal-aware prompts and normal context prompts to capture diverse abnormal patterns and generate clear event boundaries. Experiments show state-of-the-art performance on public benchmarks.",
        "rejected": "Weakly-supervised Video Anomaly Detection (wVAD) aims to detect anomalies using only video-level labels. This paper proposes prompt-enhanced MIL, incorporating abnormal-aware prompts and normal context prompts to generate clear event boundaries and capture diverse abnormal patterns. Extensive experiments show that this method achieves state-of-the-art performance on three public benchmarks."
    },
    {
        "prompt": "summary: Segment Anything Model (SAM) has emerged as a trans-\nformative approach in image segmentation, acclaimed for\nits robust zero-shot segmentation capabilities and flexible\nprompting system. Nonetheless, its performance is chal-\nlenged by images with degraded quality. Addressing this\nlimitation, we propose the Robust Segment Anything Model\n(RobustSAM), which enhances SAM’s performance on low-\nquality images while preserving its promptability and zero-\nshot generalization. Our method leverages the pre-trained\nSAM model with only marginal parameter increments and\ncomputational requirements.\nThe additional parameters\nof RobustSAM can be optimized within 30 hours on eight\nGPUs, demonstrating its feasibility and practicality for typ-\nical research laboratories. We also introduce the Robust-\nSeg dataset, a collection of 688K image-mask pairs with\ndifferent degradations designed to train and evaluate our\nmodel optimally. Extensive experiments across various seg-\nmentation tasks and datasets confirm RobustSAM’s supe-\nrior performance, especially under zero-shot conditions,\nunderscoring its potential for extensive real-world applica-\ntion. Additionally, our method has been shown to effectively\nimprove the performance of SAM-based downstream tasks\nsuch as single image dehazing and deblurring.\nProject Page: https://robustsam.github.io\n† Part of the work done during internship at Snap Research.\n* Co-corresponding authors\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n4081",
        "chosen": "This paper introduces RobustSAM, enhancing Segment Anything Model (SAM) for better performance on low-quality images. The method preserves SAM's promptability and zero-shot generalization with minimal parameter increments. Experiments confirm superior performance across various segmentation tasks.",
        "rejected": "The Segment Anything Model (SAM) excels in zero-shot segmentation but struggles with low-quality images. RobustSAM enhances SAM’s performance on such images with minimal additional parameters, preserving its promptability and zero-shot capabilities. It uses a pre-trained SAM model and can be optimized within 30 hours on eight GPUs. The RobustSeg dataset, comprising 688K image-mask pairs with various degradations, supports training and evaluation. Experiments confirm RobustSAM's superior performance, especially under zero-shot conditions, making it ideal for real-world applications like image dehazing and deblurring."
    },
    {
        "prompt": "summary: We propose SceneTex, a novel method for effectively gen-\nerating high-quality and style-consistent textures for indoor\nscenes using depth-to-image diffusion priors. Unlike pre-\nvious methods that either iteratively warp 2D views onto\na mesh surface or distillate diffusion latent features with-\nout accurate geometric and style cues, SceneTex formulates\nthe texture synthesis task as an optimization problem in the\nRGB space where style and geometry consistency are prop-\nerly reflected. At its core, SceneTex proposes a multires-\nolution texture field to implicitly encode the mesh appear-\nance. We optimize the target texture via a score-distillation-\nbased objective function in respective RGB renderings. To\nfurther secure the style consistency across views, we intro-\nduce a cross-attention decoder to predict the RGB values\nby cross-attending to the pre-sampled reference locations\nin each instance. SceneTex enables various and accurate\ntexture synthesis for 3D-FRONT scenes, demonstrating sig-\nnificant improvements in visual quality and prompt fidelity\nover the prior texture generation methods.\n1. Introduction\nSynthesizing high-quality 3D contents is an essential yet\nhighly demanding task for numerous applications, includ-\ning gaming, film making, robotic simulation, autonomous\ndriving, and upcoming VR/AR scenarios. With an increas-\ning number of 3D content datasets, the computer vision and\ngraphics community has witnessed a soaring research inter-\nest in the field of 3D geometry generation [2, 12, 36, 38, 40,\n60, 68, 73]. Despite achieving a remarkable success in 3D\ngeometry modeling, generating the object appearance, i.e.\ntextures, is still bottlenecked by laborious human efforts.\nIt typically requires a substantially long time for designing\nand adjustment, and immense 3D modelling expertise with\ntools such as Blender. As such, automatic designing and\naugmenting the textures has not yet been fully industrial-\nized due to a huge demand for human expertise and finan-\ncial expenses.\nLeveraging the recent advances of 2D diffusion mod-\nels, tremendous progress has been made for text-to-3D\ngeneration, especially for synthesizing textures of given\nshapes [8, 39, 50]. Seminal work such as Text2Tex [8] and\nLatent-Paint [39] have achieved great success in generat-\ning high-quality appearances for objects, facilitating high-\nfidelity texture synthesis from input prompts. Despite the\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n21081",
        "chosen": "This paper proposes SceneTex, a method for generating high-quality, style-consistent textures for indoor scenes using depth-to-image diffusion priors. The method formulates texture synthesis as an optimization problem and introduces a cross-attention decoder for style consistency. Results show significant improvements in visual quality and prompt fidelity.",
        "rejected": "SceneTex is a novel method for generating high-quality, style-consistent textures for indoor scenes using depth-to-image diffusion priors. Unlike previous methods, SceneTex formulates texture synthesis as an RGB-space optimization problem, ensuring geometry and style consistency. It employs a multi-resolution texture field and score-distillation-based objective function for optimization, and a cross-attention decoder for style consistency across views. SceneTex shows significant improvements in visual quality and prompt fidelity over prior methods, enabling accurate texture synthesis for 3D-FRONT scenes."
    },
    {
        "prompt": "summary: Recent works on text-to-3d generation show that using\nonly 2D diffusion supervision for 3D generation tends to\nproduce results with inconsistent appearances (e.g., faces\non the back view) and inaccurate shapes (e.g., animals\nwith extra legs). Existing methods mainly address this is-\nsue by retraining diffusion models with images rendered\nfrom 3D data to ensure multi-view consistency while strug-\ngling to balance 2D generation quality with 3D consistency.\nIn this paper, we present a new framework Sculpt3D that\nequips the current pipeline with explicit injection of 3D\npriors from retrieved reference objects without re-training\nthe 2D diffusion model. Specifically, we demonstrate that\nhigh-quality and diverse 3D geometry can be guaran-\nteed by keypoints supervision through a sparse ray sam-\npling approach.\nMoreover, to ensure accurate appear-\nances of different views, we further modulate the output of\nthe 2D diffusion model to the correct patterns of the tem-\nplate views without altering the generated object’s style.\nThese two decoupled designs effectively harness 3D in-\nformation from reference objects to generate 3D objects\nwhile preserving the generation quality of the 2D diffu-\nsion model. Extensive experiments show our method can\nlargely improve the multi-view consistency while retaining\nfidelity and diversity.\nOur project page is available at:\nhttps://stellarcheng.github.io/Sculpt3D/.\n1. Introduction\nThere has been growing research attention towards text-to-\n3d generation.\nCompared to image generation, the data\navailable for 3D generation is less in quantity and lower\nin quality. Thus, many studies [19, 28, 41] have begun to\ngenerate 3D objects using 2D text-to-image models [9, 30]\nas supervision to leverage their strong priors learned from\nbillions of real images.\n∗Corresponding authors.\nLifelike tiger with fierce expression\nAn antique glass perfume bottle\nProlificdreamer\nOurs\nReference Sets\nFigure 1. Comparison of objects generated by our method and\nProlificDreamer. We retain the 2D model’s capability to produce\nhigh-fidelity objects and adaptively learn 3D information from ref-\nerence templates retrieved from external datasets.\nThese methods mainly contain two steps: the first step\nis to continuously sample images from different views of\na randomly initialized 3D representation (e.g. NeRF [26],\nDMTet [33]). The second step uses a 2D diffusion model\nto individually judge whether each image is a high-quality\nimage that conforms to the text description. Compared to\n2D image generation, 3D generation not only requires pro-\nducing high-quality images for each individual viewpoint\nbut also needs to create plausible shapes and appearances\nas a whole 3D object. Thus, a high-quality 2D generative\nmodel and a mechanism that can accurately provide 3D pri-\nors are two keys to achieving decent 3D generation results.\nSince early works [19, 28, 41] mainly use the sole 2D diffu-\nsion model as supervision, they tend to produce inaccurate\nshapes (shape ambiguity) and appearances that are incon-\nsistent across viewpoints (appearance ambiguity), as shown\nin Figure 1 left, where examples include incomplete bottles,\ntigers with multiple legs, and tails.\nRecently, some efforts have been made to expand the\n3D datasets [10]. Following this, there were immediate at-\ntempts to retrain 2D diffusion models on these 3D datasets\nto learn 3D information [21, 22, 35]. Although these meth-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n10228",
        "chosen": "This paper presents Sculpt3D, a framework for text-to-3D generation that improves multi-view consistency by injecting 3D priors from reference objects without retraining the 2D diffusion model. The method uses keypoints supervision and modulates the 2D diffusion model's output to ensure accurate appearances across different views. Experiments show significant improvements in multi-view consistency while maintaining fidelity and diversity.",
        "rejected": "Recent research in text-to-3D generation has struggled with multi-view consistency and shape accuracy. Sculpt3D proposes a novel framework that integrates 3D priors from reference objects into the 2D diffusion model without retraining, enhancing the quality and diversity of generated 3D objects."
    },
    {
        "prompt": "summary: Recently, transformer-based methods have achieved\nstate-of-the-art prediction quality on human pose esti-\nmation(HPE). Nonetheless, most of these top-performing\ntransformer-based models are too computation-consuming\nand storage-demanding to deploy on edge computing plat-\nforms. Those transformer-based models that require fewer\nresources are prone to under-fitting due to their smaller\nscale and thus perform notably worse than their larger\ncounterparts.\nGiven this conundrum, we introduce SD-\nPose, a new self-distillation method for improving the per-\nformance of small transformer-based models.\nTo miti-\ngate the problem of under-fitting, we design a transformer\nmodule named Multi-Cycled Transformer(MCT) based on\nmultiple-cycled forwards to more fully exploit the poten-\ntial of small model parameters. Further, in order to pre-\nvent the additional inference compute-consuming brought\nby MCT, we introduce a self-distillation scheme, extracting\nthe knowledge from the MCT module to a naive forward\nmodel. Specifically, on the MSCOCO validation dataset,\nSDPose-T obtains 69.7% mAP with 4.4M parameters and\n1.8 GFLOPs. Furthermore, SDPose-S-V2 obtains 73.5%\nmAP on the MSCOCO validation dataset with 6.2M pa-\nrameters and 4.7 GFLOPs, achieving a new state-of-the-art\namong predominant tiny neural network methods.\n1. Introduction\nHuman Pose Estimation (HPE) aims to estimate the posi-\ntion of each joint point of the human body in a given im-\nage. HPE tasks support a wide range of downstream tasks\nsuch as activity recognition[1], motion capture[2], etc. Re-\ncently with the ViT model being proven effective on many\nvisual tasks, many transformer-based methods[3–5] have\nachieved excellent performance on HPE tasks. Compared\nwith past CNN-based methods[6], transformer-based mod-\n*Equal Contribution.\n†Corresponding Authors.\nFigure 1. Comparsions between other small models and our meth-\nods on MSCOCO validation dataset. Compared to other methods,\nour approach can significantly reduce the scale while maintaining\nthe same performance, or greatly improve performance under the\nsame scale.\nels are much more powerful in capturing the relationship\nbetween visual elements. However, most of them are large\nand computationally expensive. The state-of-the-art(SOTA)\ntransformer-based model[5] has 632 million parameters and\nrequires 122.9 billion floating-point operations.\nSuch a\nlarge-scale model is difficult to deploy on edge computing\ndevices and cannot accommodate the growing development\nof embodied intelligence. However, when the CNN or ViT\nused as a backbone is not of sufficient scale, transformer-\nbased models are not able to learn the relationship between\nkeypoints and visual elements well resulting in poor per-\nformance.\nStacking more transformer layers is a viable\napproach[4], but this also increases the scale of the network\nresulting in larger parameters and the difficulty of edge de-\nployment.\nTo help small models learn better, one possible way is\nto distill knowledge from big model to small model[7, 8].\nHowever, previous distillation methods have the following\ndrawbacks: (1) To align the vector space, an additional\nmanipulation is required during feature distillation[9] and\nleads to a potential performance decrease. (2) A huge extra\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1082",
        "chosen": "This paper introduces SD-Pose, a self-distillation method for improving the performance of small transformer-based models in human pose estimation. The method features a Multi-Cycled Transformer module and a self-distillation scheme, achieving state-of-the-art performance among small models on the MSCOCO validation dataset.",
        "rejected": "SDPose introduces a self-distillation method to improve small transformer-based models' performance on human pose estimation tasks. By employing a Multi-Cycled Transformer module and self-distillation scheme, SDPose significantly boosts model efficiency and accuracy, achieving state-of-the-art results among tiny neural network methods."
    },
    {
        "prompt": "summary: Category-level object pose estimation, aiming to predict\nthe 6D pose and 3D size of objects from known categories,\ntypically struggles with large intra-class shape variation.\nExisting works utilizing mean shapes often fall short of cap-\nturing this variation. To address this issue, we present Sec-\nondPose, a novel approach integrating object-specific ge-\nometric features with semantic category priors from DI-\nNOv2.\nLeveraging the advantage of DINOv2 in provid-\ning SE(3)-consistent semantic features, we hierarchically\nextract two types of SE(3)-invariant geometric features to\nfurther encapsulate local-to-global object-specific informa-\ntion. These geometric features are then point-aligned with\nDINOv2 features to establish a consistent object represen-\ntation under SE(3) transformations, facilitating the map-\nping from camera space to the pre-defined canonical space,\nthus further enhancing pose estimation. Extensive exper-\niments on NOCS-REAL275 demonstrate that SecondPose\nachieves a 12.4% leap forward over the state-of-the-art.\nMoreover, on a more complex dataset HouseCat6D which\nprovides photometrically challenging objects, SecondPose\nstill surpasses other competitors by a large margin.\n1. Introduction\nCategory-level pose estimation involves estimating the\ncomplete 9 degrees-of-freedom (DoF) object pose, encom-\npassing 3D rotation, 3D translation, and 3D metric size, for\narbitrary objects within a known set of categories. This task\nhas garnered significant research interest due to its essen-\ntial role in various applications, including the AR/VR in-\ndustry [30, 38, 40, 55], robotics [51, 52, 54], and scene un-\n∗Equal contributions.\n† Corresponding author (e-mail: guangyao.zhai@tum.de).\nFigure 1. Categorical SE(3)-consistent features. We visualize\nour fused features by PCA. Colored points highlight the most cor-\nresponding parts, where our proposed feature achieves consistent\nalignment cross instances (left vs. middle) and maintains consis-\ntency on the same instance of different poses (middle vs. right).\nderstanding [1, 7, 53]. In contrast to traditional instance-\nlevel pose estimation methods [6,21], which rely on specific\n3D CAD models for each target object, the category-level\napproach necessitates greater adaptability to accommodate\ninherent shape diversity within each category. Effectively\naddressing intra-class shape variations has thus become a\ncentral focus, crucial for real-world applications where ob-\njects within a category may exhibit significant differences\nin shape while sharing the same general category label.\nMean Shape vs. Semantic Priors. One common ap-\nproach to handle intra-class shape variation involves us-\ning explicit mean shapes as prior knowledge [24, 39, 57].\nThese methods typically consist of two functional modules:\none for reconstructing the target object by slightly deform-\ning the mean shape and another for regressing the 9D pose\nbased on the reconstructed object [24,39] or enhanced inter-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n9959",
        "chosen": "This paper presents SecondPose, a novel approach for category-level object pose estimation that integrates object-specific geometric features with semantic category priors from DINOv2. The method extracts SE(3)-invariant geometric features and aligns them with DINOv2 features, achieving significant performance improvements on multiple datasets.",
        "rejected": "SecondPose addresses intra-class shape variation in category-level object pose estimation by integrating geometric features with semantic category priors from DINOv2. This approach enhances the consistency and accuracy of pose predictions, demonstrating superior performance on NOCS-REAL275 and HouseCat6D datasets."
    },
    {
        "prompt": "summary: We propose a novel self-supervised embedding to learn\nhow actions sound from narrated in-the-wild egocentric\nvideos. Whereas existing methods rely on curated data\nwith known audio-visual correspondence, our multimodal\ncontrastive-consensus coding (MC3) embedding reinforces\nthe associations between audio, language, and vision when\nall modality pairs agree, while diminishing those associa-\ntions when any one pair does not. We show our approach\ncan successfully discover how the long tail of human ac-\ntions sound from egocentric video, outperforming an array\nof recent multimodal embedding techniques on two datasets\n(Ego4D and EPIC-Sounds) and multiple cross-modal tasks.\n1. Introduction\nHuman activity often produces sounds. Closing a door, chop-\nping vegetables, typing on a keyboard, talking with a friend—\nour interactions with the objects and people around us gener-\nate audio that reveals our physical behaviors. These sounds\ncan be strongly associated with the subjects of our activity\nand how we perform it. For example, opening a water bottle\nsounds different than opening a cabinet; chopping sweet\npotatoes sounds different than chopping onions; chopping\nonions sounds different than mincing onions (the same ob-\nject). Understanding the link between sounds and actions is\nvaluable for a number of applications, such as multimodal ac-\ntivity recognition, cross-modal retrieval, content generation,\nor forecasting the physical effects of a person’s actions.\nHow should AI learn about sounding actions? Exist-\ning work typically curates annotated datasets for supervised\nlearning [9, 22, 28, 45], taking care to select events or actions\nthat have associated sounds (e.g., lawnmowing, chopping),\nwhile others deliberately collect videos of object collisions\n(e.g., striking objects with a drumstick [43] or crashing into\nthem with a robot [10, 17]), or develop physics-based simula-\ntions [16]. On the one hand, these approaches are appealing\nfor their ability to focus on meaningful audio-visual corre-\nspondences. On the other hand, their curated nature risks\nlimiting the scope of sounding actions that can be learned.\nAction: C digs the \nsoil with a hoe.\nAction: C moves a \nmetal cutting machine \nwith hands.\nDoes the action sound?\nDoes the action sound?\nFigure 1. We aim to distinguish sounds that are directly caused\nby human actions (bottom) from those that are not (top). Given\negocentric training videos with language descriptions of the camera\nwearer’s (“C\") current action, we learn an embedding where the\naudio and visual features of any given clip are best aligned only\nwhen both are also consistent with the language. This allows\ndiscerning clips where the audio and vision may be correlated (e.g.,\nthe cutting machine running making loud noise in top row) versus\nthose where the sounds are driven by human action (digging in\nbottom row)—importantly, without language at inference time.\nInstead, we aim to learn how human actions sound from\nnarrated in-the-wild egocentric videos. See Figure 1. Given\na pool of videos of everyday human activity, the goal is to\nlearn a cross-modal representation where sounding actions\ncluster together based on how they look and sound. By sam-\npling the videos freely, we can broaden the scope to discover\nthe breadth of sounding actions without having to rely on a\nclosed, pre-defined set of action categories. In particular, by\nfocusing on unscripted egocentric video from wearable cam-\neras in daily-life settings [11, 26], we aim to include subtle\nand long-tail scenarios unavailable in curated datasets, such\nas sounds of keys jangling when unlocking a door, scissors\nsnipping when cutting the dog’s fur, or fingernails scratch-\ning on one’s own arm. Egocentric video is a particularly\nattractive source here because 1) human interaction sounds\nare more audible in near-field egocentric recordings and 2)\npassively captured long-form ego-video simply covers more\n1\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n27252",
        "chosen": "This paper proposes SoundingActions, a self-supervised embedding method for learning how actions sound from narrated egocentric videos. The method reinforces audio-visual-language associations when all modalities agree, outperforming recent techniques on multiple datasets and tasks.",
        "rejected": "This paper presents MC3 embedding, a self-supervised multimodal learning method for associating actions with sounds in egocentric videos. MC3 outperforms existing techniques in discovering how sounding actions cluster based on visual and audio cues, advancing cross-modal tasks like activity recognition and content generation."
    },
    {
        "prompt": "summary: Learning generalizable visual representations from Inter-\nnet data has yielded promising results for robotics. Yet, pre-\nvailing approaches focus on pre-training 2D representations,\nbeing sub-optimal to deal with occlusions and accurately\nlocalize objects in complex 3D scenes. Meanwhile, 3D rep-\nresentation learning has been limited to single-object under-\nstanding. To address these limitations, we introduce a novel\n3D pre-training framework for robotics named SUGAR that\ncaptures semantic, geometric and affordance properties of\nobjects through 3D point clouds. We underscore the impor-\ntance of cluttered scenes in 3D representation learning, and\nautomatically construct a multi-object dataset benefiting\nfrom cost-free supervision in simulation. SUGAR employs\na versatile transformer-based model to jointly address five\npre-training tasks, namely cross-modal knowledge distilla-\ntion for semantic learning, masked point modeling to un-\nderstand geometry structures, grasping pose synthesis for\nobject affordance, 3D instance segmentation and referring\nexpression grounding to analyze cluttered scenes. We evalu-\nate our learned representation on three robotic-related tasks,\nnamely, zero-shot 3D object recognition, referring expres-\nsion grounding, and language-driven robotic manipulation.\nExperimental results show that SUGAR’s 3D representation\noutperforms state-of-the-art 2D and 3D representations.\n1. Introduction\nVisual perception plays an essential role in robotics and en-\nables autonomous agents to understand and interact with\ntheir physical environment. Nevertheless, learning general-\nizable visual representations for robotics is challenging due\nto the scarcity of real robot data and the large variety of\nreal-world scenes. Despite substantial efforts in robot data\naccumulation [2, 76, 77] and augmentation [39, 85], it re-\nmains prohibitively expensive to collect large-scale datasets\ncomprising a broad range of robotic tasks.\nTo alleviate the burden of data collection, recent endeav-\nors [36, 37, 48, 49, 51, 62] have sought to leverage large-\nscale internet data to pre-train 2D visual representations for\nPre-training\nRobotics tasks\nSemantic\na blue plastic \nwater bottle\nwith a black lid\nmUlti-object scenes\nRGB\nDepth\nLabel\nGeometry\nAffordance\npick up the knife and leave it\non the chopping board\ngrasp the red cup and lift it\nFigure 1. We introduce SUGAR\n, a pre-training framework for\nrobotic-related tasks, which learns semantic, geometry and affor-\ndance on both single- and multi-object scenes.\nrobotics. For example, MVP [62], VIP [48] and VC-1 [49]\nuse self-supervised learning on image or video datasets,\nwhile EmbCLIP [37], R3M [51] and Voltron [36] further per-\nform cross-modal pre-training based on videos with aligned\nlanguage descriptions. While these 2D representations have\ndemonstrated promising performance, they still fall short in\naddressing occlusions in complex cluttered scenes [79] and\naccurately predicting robotic actions [7] in the 3D world.\nRecently, growing research attention has been paid to 3D\nvisual representations for robotics. A majority of approaches\ntrain 3D-based models from scratch [7, 43, 69], potentially\nlosing the generalization ability. Several recent works lift pre-\ntrained 2D features to the 3D space [20, 34, 56, 89], which\ncompromise efficiency due to processing multi-view images\nand do not fully take advantage of 3D data. To improve\n3D representation learning, prior endeavours propose self-\nsupervised pre-training of 3D models [44, 53, 59, 86]. Pre-\ntraining in existing work, however, is typically limited to\nsingle objects and complete point clouds, hence, ignoring\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n18049",
        "chosen": "This paper introduces SUGAR, a 3D pre-training framework for robotics that captures semantic, geometric, and affordance properties of objects through 3D point clouds. The framework includes five pre-training tasks and demonstrates superior performance on robotic-related tasks compared to state-of-the-art 2D and 3D representations.",
        "rejected": "SUGAR is a 3D pre-training framework designed to learn semantic, geometric, and affordance properties of objects through point clouds. It excels in handling occlusions and accurately localizing objects in cluttered scenes, showing improved performance over state-of-the-art 2D and 3D representations in various robotic tasks."
    },
    {
        "prompt": "summary: Automatic text-to-3D generation that combines Score Dis-\ntillation Sampling (SDS) with the optimization of volume\nrendering has achieved remarkable progress in synthesizing\nrealistic 3D objects. Yet most existing text-to-3D methods by\nSDS and volume rendering suffer from inaccurate geometry,\ne.g., the Janus issue, since it is hard to explicitly integrate 3D\npriors into implicit 3D representations. Besides, it is usually\ntime-consuming for them to generate elaborate 3D models\nwith rich colors. In response, this paper proposes GSGEN, a\nnovel method that adopts Gaussian Splatting, a recent state-\nof-the-art representation, to text-to-3D generation. GSGEN\naims at generating high-quality 3D objects and address-\ning existing shortcomings by exploiting the explicit nature\nof Gaussian Splatting that enables the incorporation of 3D\nprior. Specifically, our method adopts a progressive optimiza-\ntion strategy, which includes a geometry optimization stage\nand an appearance refinement stage. In geometry optimiza-\n†Corresponding author\ntion, a coarse representation is established under 3D point\ncloud diffusion prior along with the ordinary 2D SDS opti-\nmization, ensuring a sensible and 3D-consistent rough shape.\nSubsequently, the obtained Gaussians undergo an iterative\nappearance refinement to enrich texture details. In this stage,\nwe increase the number of Gaussians by compactness-based\ndensification to enhance continuity and improve fidelity. With\nthese designs, our approach can generate 3D assets with del-\nicate details and accurate geometry. Extensive evaluations\ndemonstrate the effectiveness of our method, especially for\ncapturing high-frequency components. Our code is available\nat https://github.com/gsgen3d/gsgen.\n1. Introduction\nDiffusion model based text-to-image generation [1, 55, 57,\n58] has achieved remarkable success in synthesizing photo-\nrealistic images from textual prompts. Nevertheless, for high-\nquality text-to-3D content generation, the advancements lag\nbehind that of image generation due to the inherent complex-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n21401",
        "chosen": "This paper proposes GSGEN, a text-to-3D generation method using Gaussian Splatting. The method includes geometry optimization and appearance refinement stages, generating high-quality 3D objects with accurate geometry and rich details. Evaluations show its effectiveness, particularly in capturing high-frequency components.",
        "rejected": "GSGEN proposes using Gaussian Splatting for text-to-3D generation to address geometry inaccuracies and time consumption issues. Through progressive optimization stages including geometry establishment and appearance refinement, GSGEN generates high-quality 3D objects with accurate geometries and rich colors."
    },
    {
        "prompt": "summary: Federated learning facilitates the collaborative learning\nof a global model across multiple distributed medical in-\nstitutions without centralizing data. Nevertheless, the ex-\npensive cost of annotation on local clients remains an ob-\nstacle to effectively utilizing local data. To mitigate this\nissue, federated active learning methods suggest leverag-\ning local and global model predictions to select a rela-\ntively small amount of informative local data for annota-\ntion.\nHowever, existing methods mainly focus on all lo-\ncal data sampled from the same domain, making them un-\nreliable in realistic medical scenarios with domain shifts\namong different clients. In this paper, we make the first at-\ntempt to assess the informativeness of local data derived\nfrom diverse domains and propose a novel methodology\ntermed Federated Evidential Active Learning (FEAL) to\ncalibrate the data evaluation under domain shift. Specif-\nically, we introduce a Dirichlet prior distribution in both\nlocal and global models to treat the prediction as a distribu-\ntion over the probability simplex and capture both aleatoric\nand epistemic uncertainties by using the Dirichlet-based\nevidential model.\nThen we employ the epistemic uncer-\ntainty to calibrate the aleatoric uncertainty. Afterward, we\ndesign a diversity relaxation strategy to reduce data re-\ndundancy and maintain data diversity. Extensive experi-\nments and analysis on five real multi-center medical im-\nage datasets demonstrate the superiority of FEAL over the\nstate-of-the-art active learning methods in federated sce-\nnarios with domain shifts. The code will be available at\nhttps://github.com/JiayiChen815/FEAL.\n∗Equal contribution.\n†Yong Xia is the corresponding author. This\nwork was supported in part by Shenzhen Science and Technology Pro-\ngram under Grants JCYJ20220530161616036, National Natural Science\nFoundation of China under Grants 62171377 and 62271405, Ningbo Clin-\nical Research Center for Medical Imaging under Grant 2021L003 (Open\nProject: 2022LYKFZD06), and Foshan HKUST Projects under Grants\nFSUST21-HKUST10E and FSUST21-HKUST11E.\n?\n𝑈!\n𝐿!\n?\n𝑈\"\n𝐿\"\n𝑈#\n𝐿#\nAnnotate\nDistribute\nAggregate\n⋯\nClient K\nServer\nClient 1\nClient 2\nTrain\n(a) FAL scheme\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nEnergy score\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nDensity\nClient 1\nClient 2\nClient 3\nClient 4\n(b) KDE of energy score\n1\n2\n3\n4\nClient\n1\n2\n3\n4\nClient\n1.0000\n0.0001\n0.0001\n0.0001\n0.0001\n1.0000\n0.0130\n0.0001\n0.0001\n0.0130\n1.0000\n0.0031\n0.0001\n0.0001\n0.0031\n1.0000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) p-value\nFigure 1. Illustration of federated active learning (FAL) in the\npresence of domain shift. (a) FAL comprises model distribution,\nlocal training, model aggregation, and data annotation. (b) The\nKDE of energy scores depicts domain shifts across clients. (c)\nThe low p-values in cross-client KDE of energy scores indicate\nthe existence of significant domain shifts between all client pairs.\n1. Introduction\nFederated learning enables collaborative learning across\nmultiple clinical institutions (i.e., clients) to learn a uni-\nfied model on the central server through model aggregation\nwhile preserving the data privacy at each client [21, 36, 57]\n(see Fig. 1 (a)). Unfortunately, such a learning pipeline re-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n11439",
        "chosen": "This paper presents FEAL, a federated evidential active learning method for medical imaging that addresses domain shifts among clients. The method uses a Dirichlet prior distribution to capture uncertainties and a diversity relaxation strategy to maintain data diversity. Experiments on multi-center datasets demonstrate its superiority over existing methods.",
        "rejected": "FEAL introduces federated evidential active learning to calibrate data evaluation under domain shifts in medical federated learning scenarios. By leveraging Dirichlet-based evidential models, FEAL effectively selects informative local data for annotation, improving model performance across diverse domains."
    },
    {
        "prompt": "summary: Despite many attempts to leverage pre-trained text-to-\nimage models (T2I) like Stable Diffusion (SD) [25] for con-\ntrollable image editing, producing good predictable results\nremains a challenge.\nPrevious approaches have focused\non either fine-tuning pre-trained T2I models on specific\ndatasets to generate certain kinds of images (e.g., with a\nspecific object or person), or on optimizing the weights,\ntext prompts, and/or learning features for each input im-\nage in an attempt to coax the image generator to produce\nthe desired result.\nHowever, these approaches all have\nshortcomings and fail to produce good results in a pre-\ndictable and controllable manner. To address this problem,\nwe present TiNO-Edit, an SD-based method that focuses on\noptimizing the noise patterns and diffusion timesteps dur-\ning editing, something previously unexplored in the liter-\nature. With this simple change, we are able to generate\nresults that both better align with the original images and\nreflect the desired result. Furthermore, we propose a set\nof new loss functions that operate in the latent domain of\nSD, greatly speeding up the optimization when compared\nto prior losses, which operate in the pixel domain. Our\nmethod can be easily applied to variations of SD includ-\ning Textual Inversion [13] and DreamBooth [27] that en-\ncode new concepts and incorporate them into the edited re-\nsults. We present a host of image-editing capabilities en-\nabled by our approach. Our code is publicly available at\nhttps://github.com/SherryXTChen/TiNO-Edit.\n1. Introduction\nComputer-generated image synthesis has been studied\nfor decades for its wide range of applications includ-\ning content creation, marketing and advertising, visual-\nization/simulation, entertainment, and storytelling.\nRe-\n*Corresponding author email: xchen774@ucsb.edu\nPure text-guided & Reference-guided image editing\nOriginal\nRef\nw/o ref\nw/ref\n“sandwich” →“cake”\n+ “a magenta hat”\nStroke-guided image editing\nOriginal\nUser input\nResult\nOriginal\nUser input\nResult\n+ “a sunflower”\n+ “curtains”\nImage composition\nOriginal\nUser input\nResult\nOriginal\nUser input\nResult\n+ “a scarecrow”\n“white” →“yellow plaid”\nImage editing with DreamBooth (DB) or Textual Inversion (TI)\nOriginal\nDB\nResult\nOriginal\nTI\nResult\n“photo” →⟨concept⟩\n“sky” →⟨concept⟩\nFigure 1.\nOverview of capabilities enabled by TiNO-Edit.\nTiNO-Edit offers various image-editing capabilities and can be run\nwith DreamBooth (DB) [27] or Textual Inversion (TI) [13]. By\nleveraging diffusion timestep and noise optimization techniques,\nit can generate realistic and high quality outputs.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n6337",
        "chosen": "This paper introduces TiNO-Edit, a method for robust diffusion-based image editing that optimizes noise patterns and diffusion timesteps. The method also proposes new loss functions in the latent domain, improving optimization speed and enabling various image-editing capabilities.",
        "rejected": "TiNO-Edit focuses on optimizing noise patterns and diffusion timesteps during image editing using Stable Diffusion models. Introducing new latent domain loss functions, this method achieves better alignment with original images and desired edits, facilitating realistic and high-quality image synthesis."
    },
    {
        "prompt": "summary: We present a method to generate full-body selfies from\nphotographs originally taken at arms length. Because self-\ncaptured photos are typically taken close up, they have lim-\nited field of view and exaggerated perspective that distorts\nfacial shapes. We instead seek to generate the photo some\none else would take of you from a few feet away. Our ap-\nproach takes as input four selfies of your face and body, a\nbackground image, and generates a full-body selfie in a de-\nsired target pose. We introduce a novel diffusion-based ap-\nproach to combine all of this information into high-quality,\nwell-composed photos of you with the desired pose and\nbackground.\n1. Introduction\nThe prevalence of selfies has skyrocketed in recent years,\nwith an estimated 93 million taken each day. Despite their\npopularity, they suffer from multiple shortcomings: (1) they\ncapture only the upper portion of the subject, (2) the close-\nup camera viewpoint distorts faces and requires awkward\nposes (e.g., with arm reaching out), and (3) it is difficult to\ncompose a shot that optimally captures both the subject and\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n6701",
        "chosen": "This paper presents a method to generate full-body selfies from arm's length photographs using a diffusion-based approach. The method takes four selfies, a background image, and generates a well-composed full-body selfie in a desired pose.",
        "rejected": "This work presents a diffusion-based method to generate full-body selfies from close-up photographs. Utilizing multiple input selfies and a background image, it creates well-composed photos of subjects in desired poses, overcoming limitations of traditional selfies such as distorted perspectives and limited field of view."
    },
    {
        "prompt": "summary: Tumor synthesis enables the creation of artificial tumors\nin medical images, facilitating the training of AI models for\ntumor detection and segmentation. However, success in tu-\nmor synthesis hinges on creating visually realistic tumors\nthat are generalizable across multiple organs and, further-\nmore, the resulting AI models being capable of detecting\nreal tumors in images sourced from different domains (e.g.,\nhospitals). This paper made a progressive stride toward\ngeneralizable tumor synthesis by leveraging a critical ob-\nservation: early-stage tumors (< 2cm) tend to have simi-\nlar imaging characteristics in computed tomography (CT),\nwhether they originate in the liver, pancreas, or kidneys.\nWe have ascertained that generative AI models, e.g., Dif-\nfusion Models, can create realistic tumors generalized to a\nrange of organs even when trained on a limited number of\ntumor examples from only one organ. Moreover, we have\nshown that AI models trained on these synthetic tumors can\nbe generalized to detect and segment real tumors from CT\nvolumes, encompassing a broad spectrum of patient demo-\ngraphics, imaging protocols, and healthcare facilities.\n1. Introduction\nTumor synthesis enables the creation of artificial tumor ex-\namples in medical images [11, 42, 88], it is particularly\nvaluable when there is a dearth or complete absence of per-\nvoxel annotated real tumors (e.g., early-stage tumors) for\neffective AI training. Typically, to train AI models for tu-\nmor detection in multiple (N) organs, annotated real tu-\nmor examples from each of these organs are necessary, and\nideally, in substantial numbers [14, 43, 54, 55, 97]. Fur-\nthermore, AI models often fail to generalize across images\nfrom different hospitals, which may vary due to variations\n*Correspondence to Zongwei Zhou (ZZHOU82@JH.EDU)\nin imaging protocols, patient demographics, and scanner\nmanufacturers [61, 95, 96]. The challenge amplifies with\nthe need for extensive manual annotations, a task that could\ndemand up to 25 human years for annotating just one tu-\nmor type [1, 9, 83]. The task of collecting and annotating\na comprehensive dataset encompassing tumors from multi-\nple organs (N) and images from numerous hospitals (M) is\ndaunting, considering both annotation cost and complexity\n(N × M). We hypothesize that tumor synthesis could solve\nthis challenge by creating various tumor types across non-\ntumor images from multiple hospitals, even when only one\ntumor type is available, thereby simplifying the complexity\nfrom N × M to 1 × M.\nSuccess in tumor synthesis hinges on creating visually\nrealistic tumors that are generalizable across multiple or-\ngans and, furthermore, the resulting AI models being gen-\neralizable in detecting real tumors in images sourced from\ndifferent hospitals. Previous studies have introduced gen-\nerative models to create synthetic medical data (not lim-\nited to tumors) such as polyp detection from colonoscopy\nvideos [69], COVID-19 detection from Chest X-ray [26,\n56, 87], and diabetic lesion detection from retinal im-\nages [78]—refer to §5 for a comprehensive review. How-\never, these studies have primarily focused on enhancing the\ndetection and segmentation of specific tumors without fully\nexploring the wider generalizability of these models across\ndifferent organs and patient demographics.\nThis paper made a progressive stride toward generaliz-\nable tumor synthesis by leveraging a critical observation:\nearly-stage tumors (< 2cm) tend to have similar imaging\ncharacteristics in computed tomography (CT)1. Early-stage\ntumors typically present small, round, or oval shapes with\nminimal deformation and exhibit relatively simple and uni-\n1Note that, owing to the public dataset constraints, we have only ver-\nified the similarity across early hepatocellular carcinoma and intrahep-\natic cholangiocarcinoma from the liver, pancreatic ductal adenocarcinoma\nfrom the pancreas, and renal cell carcinoma from kidneys.\n1\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n11147",
        "chosen": "This paper discusses generalizable tumor synthesis in medical images using diffusion models, focusing on early-stage tumors with similar imaging characteristics across organs. The method creates realistic synthetic tumors and trains AI models for detecting real tumors across different domains.",
        "rejected": "The paper discusses tumor synthesis for training AI models in detecting and segmenting tumors across different organs and imaging domains. Successful tumor synthesis requires creating visually realistic and generalizable tumors, ensuring AI models can effectively detect real tumors in varied medical images."
    },
    {
        "prompt": "summary: Pretrained diffusion models and their outputs are widely\naccessible due to their exceptional capacity for synthe-\nsizing high-quality images and their open-source nature.\nThe users, however, may face litigation risks owing to\nthe models’ tendency to memorize and regurgitate train-\ning data during inference. To address this, we introduce\nAnti-Memorization Guidance (AMG), a novel framework\nemploying three targeted guidance strategies for the main\ncauses of memorization: image and caption duplication,\nand highly specific user prompts. Consequently, AMG en-\nsures memorization-free outputs while maintaining high im-\nage quality and text alignment, leveraging the synergy of\nits guidance methods, each indispensable in its own right.\nAMG also features an innovative automatic detection sys-\ntem for potential memorization during each step of in-\nference process, allows selective application of guidance\nstrategies, minimally interfering with the original sampling\nprocess to preserve output utility. We applied AMG to pre-\ntrained Denoising Diffusion Probabilistic Models (DDPM)\nand Stable Diffusion across various generation tasks. The\nresults demonstrate that AMG is the first approach to suc-\ncessfully eradicates all instances of memorization with no\nor marginal impacts on image quality and text-alignment,\nas evidenced by FID and CLIP scores.\n1. Introduction\nDiffusion models [12, 23, 34] have attracted substantial in-\nterest, given their superiority in terms of diversity, fidelity,\nscalability [28] and controllability [24] over previous gener-\native models including VAEs [17], normalizing flows [29],\nand GANs [10, 14–16]. With guidance techniques [7, 11],\ndiffusion models can be further improved by the strategical\ndiversity-fidelity trade-off. State-of-the-art diffusion mod-\nels trained on vast web-scale datasets are widespreadly used\nand have seen deployment at a commercial scale [1, 30, 31].\nSuch widespread adoption, however, has significantly\nheightened the litigation risks for companies using these\nmodels, particularly due to allegations that the models\nmemorize and reproduce training data during inference\nwithout informing the data owners and the users of diffusion\nFigure 1. Stable Diffusion’s capacity to memorize training data,\nmanifested as pixel-level memorization (left) and object-level\nmemorization (right).\nOur approach successfully guides pre-\ntrained diffusion models to produce memorization-free outputs.\nmodels. This potentially violates copyright laws and intro-\nduces ethical dilemmas, further complicated by the fact that\nthe extensive size of training sets impedes detailed human\nreview, leaving the intellectual property rights of the data\nsources largely undetermined. An ongoing example is that a\nlegal action contends that Stable Diffusion is a 21st-century\ncollage tool that remixes the copyrighted works of millions\nof artists whose work was used as training data [32].\nPrior studies [4, 35, 36] have observed memorization in\npretrained diffusion models, particularly during uncondi-\ntional CIFAR-10 [18] and text-conditional LAION dataset\n[33] generations. While previous research proposed strate-\ngies to reduce memorization, these often lead to only mod-\nest improvements and fail to fully eliminate the issue. The\neffectiveness often come with reduced output quality and\ntext-alignment [36], the need for retraining models [4], and\nextensive manual intervention [19]. Moreover, these strate-\ngies lack an automated way to differentiate potential mem-\norization cases for targeted mitigation. For example, [19]\nrelies on a predefined list of text prompts prone to causing\nmemorization, and [36] applies randomization mechanisms\nuniformly without distinguishing between scenarios.\nIn this paper, we undertake the following systematic ef-\nforts to address the issue of memorization. Firstly, we have\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8425",
        "chosen": "This paper introduces Anti-Memorization Guidance (AMG), a framework to eliminate memorization in diffusion models while maintaining image quality and text alignment. AMG uses three guidance strategies targeting image and caption duplication and specific user prompts. It includes an automatic detection system for potential memorization during inference. Experiments show AMG successfully removes memorization with minimal impact on quality and alignment.",
        "rejected": "Towards Memorization-Free Diffusion Models. This research proposes a novel approach to mitigate memorization in diffusion models, focusing on enhancing generalization while maintaining sample quality. By integrating specific regularization techniques, the model can achieve superior performance on diverse datasets without overfitting."
    },
    {
        "prompt": "summary: 3D pose transfer that aims to transfer the desired pose\nto a target mesh is one of the most challenging 3D gener-\nation tasks. Previous attempts rely on well-defined para-\nmetric human models or skeletal joints as driving pose\nsources.\nHowever, to obtain those clean pose sources,\ncumbersome but necessary pre-processing pipelines are in-\nevitable, hindering implementations of the real-time appli-\ncations. This work is driven by the intuition that the robust-\nness of the model can be enhanced by introducing adver-\nsarial samples into the training, leading to a more invulner-\nable model to the noisy inputs, which even can be further\nextended to directly handling the real-world data like raw\npoint clouds/scans without intermediate processing. Fur-\nthermore, we propose a novel 3D pose Masked Autoencoder\n(3D-PoseMAE), a customized MAE that effectively learns\n3D extrinsic presentations (i.e., pose). 3D-PoseMAE facil-\nitates learning from the aspect of extrinsic attributes by si-\nmultaneously generating adversarial samples that perturb\nthe model and learning the arbitrary raw noisy poses via a\nmulti-scale masking strategy. Both qualitative and quanti-\ntative studies show that the transferred meshes given by our\nnetwork result in much better quality. Besides, we demon-\nstrate the strong generalizability of our method on various\nposes, different domains, and even raw scans. Experimental\nresults also show meaningful insights that the intermediate\nadversarial samples generated in the training can success-\nfully attack the existing pose transfer models.\n1. Introduction\nAs a promising and challenging task, 3D pose transfer has\nbeen consistently drawing research attention from the com-\nputer vision community [10, 27, 37, 41]. The task aims\nat transferring a source pose to a target identity mesh and\nkeeping the intrinsic attributes (i.e., shape) of the identity\nmesh. Aside from pure research interests, transferring de-\nsired poses to target 3D models has various potential appli-\ncations in the film industry, games, AR/VR, etc [8, 11, 12].\nTo achieve data-driven learning, existing 3D pose trans-\nfer methods rely on different prerequisites to the data\nsources, which severely limits their further real-world im-\nplementations.\nFirstly, many existing 3D pose transfer\nmethods [30, 45] cannot directly be generalized to unseen\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n2295",
        "chosen": "This paper presents a 3D pose transfer method enhanced by adversarial samples and a 3D pose Masked Autoencoder (3D-PoseMAE). The method improves robustness to noisy inputs and generalizes well to raw point clouds/scans. The multi-scale masking strategy helps learn arbitrary raw noisy poses. Results show better quality transferred meshes and strong generalizability.",
        "rejected": "Towards Robust 3D Pose Transfer with Adversarial Learning. The study introduces an adversarial learning framework to improve the robustness of 3D pose transfer across different subjects. Through comprehensive experiments, it demonstrates significant improvements in accuracy and stability under various conditions."
    },
    {
        "prompt": "summary: Audio-visual segmentation (AVS) is a challenging task\nthat involves accurately segmenting sounding objects based\non audio-visual cues.\nThe effectiveness of audio-visual\nlearning critically depends on achieving accurate cross-\nmodal alignment between sound and visual objects. Suc-\ncessful audio-visual learning requires two essential compo-\nnents: 1) a challenging dataset with high-quality pixel-level\nmulti-class annotated images associated with audio files,\nand 2) a model that can establish strong links between au-\ndio information and its corresponding visual object. How-\never, these requirements are only partially addressed by cur-\nrent methods, with training sets containing biased audio-\nvisual data, and models that generalise poorly beyond this\nbiased training set. In this work, we propose a new cost-\neffective strategy to build challenging and relatively unbi-\nased high-quality audio-visual segmentation benchmarks.\nWe also propose a new informative sample mining method\nfor audio-visual supervised contrastive learning to leverage\ndiscriminative contrastive samples to enforce cross-modal\nunderstanding. We show empirical results that demonstrate\nthe effectiveness of our benchmark. Furthermore, experi-\nments conducted on existing AVS datasets and on our new\nbenchmark show that our method achieves state-of-the-art\n(SOTA) segmentation accuracy1.\n1. Introduction\nThe human nervous system exhibits multi-modal percep-\ntion [44], combining input signals from different modali-\nties to improve the detection and classification of multiple\nstimuli [44]. Such functionality has been emulated by re-\ncent papers [1–3, 5, 18, 32, 33] that aim to associate visual\nobjects with their corresponding audio sequences, in a task\nknown as audio-visual correspondence (AVC) [2, 3].\n*First two authors contributed equally to this work.\n1This work was supported by Australian Research Council through\ngrant FT190100525.\nBird\nOurs\nTPAVI\nImage\nMale\nHelic.\nNoise\nFigure 1. Current AVS datasets [54] tend to assume specific ob-\njects as consistent sound sources. Such a bias influences AVS\nmethods, like TPAVI [54] (2nd row), to favour segmenting the pre-\nsumed sound source, even when replacing the original audio with\ndifferent sound types such as a person speaking (2nd column), bird\nchirping (3rd column), or background noise (4th row). Our paper\nproposes a new cost-effective strategy to build a relatively unbi-\nased audio-visual segmentation benchmark and a supervised con-\ntrastive learning method that mines informative samples to better\nconstrain the learning of audio-visual embeddings (last row).\nA particularly interesting AVC task is the audio-visual\nsegmentation (AVS) [54, 55] that aims to segment all pix-\nels of the sounding visual objects using a fully supervised\nmodel. A major challenge in AVS is achieving cross-modal\nalignment between sound and visual objects [43]. Current\ndatasets poorly establish and evaluate this alignment, lead-\ning to undesired system behaviour and less effective eval-\nuation.\nFor instance, the dataset in [54] shows a “com-\nmonsense” bias because it assumes that certain objects are\nalways the sound source in some scenarios. Fig. 1 shows\nan example of a scene from [54] with a bias toward the\nsegmentation of the helicopter, even though other sound\nsources (e.g., person’s speech or bird’s singing) are plau-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n26497",
        "chosen": "This paper proposes a cost-effective strategy to build unbiased audio-visual segmentation benchmarks and a supervised contrastive learning method for audio-visual segmentation. The method mines informative samples to better constrain audio-visual embeddings. Experiments demonstrate state-of-the-art segmentation accuracy on existing datasets and the new benchmark.",
        "rejected": "Unraveling Instance Associations A Closer Look for Audio-Visual Segmentation. This work investigates the challenge of associating instances in audio-visual scenes. It presents an innovative method that enhances segmentation accuracy by leveraging multimodal information, achieving state-of-the-art results."
    },
    {
        "prompt": "summary: Significant progress in image deblurring has been\nachieved by deep learning methods, especially the remark-\nable performance of supervised models on paired synthetic\ndata. However, real-world quality degradation is more com-\nplex than synthetic datasets, and acquiring paired data in\nreal-world scenarios poses significant challenges. To ad-\ndress these challenges, we propose a novel unsupervised\nimage deblurring framework based on self-enhancement.\nThe framework progressively generates improved pseudo-\nsharp and blurry image pairs without the need for real paired\ndatasets, and the generated image pairs with higher qualities\ncan be used to enhance the performance of the reconstruc-\ntor. To ensure the generated blurry images are closer to\nthe real blurry images, we propose a novel re-degradation\nprincipal component consistency loss, which enforces the\nprincipal components of the generated low-quality images to\nbe similar to those of re-degraded images from the original\nsharp ones. Furthermore, we introduce the self-enhancement\nstrategy that significantly improves deblurring performance\nwithout increasing the computational complexity of network\nduring inference. Through extensive experiments on multiple\nreal-world blurry datasets, we demonstrate the superiority\nof our approach over other state-of-the-art unsupervised\nmethods.\n1. Introduction\nImage deblurring is a classical problem in the field of com-\nputer vision, which aims to recover a clear image from its\nblurred version. The image deblurring tasks can be divided\ninto blind and non-blind deblurring, where the blind image\ndeblurring with unknown degradation is more challenging in\ngeneral. Conventional model-based methods for blind image\ndeblurring typically involve two main steps: estimating the\nblur kernel, and then reconstructing the sharp image from\nthe blurred input [10, 26, 40, 42]. These methods’ perfor-\nmance is largely constrained by the accuracy of blur kernal\n*Corresponding author\nCycleGAN\nUAUD\nUSR-DA\nUIDGAN\nUSDF\nFCLGAN\nSEMGUD \n(Ours)\nGoPro\nHIDE\nRealBlur-J\nRealBlur-R\nFigure 1. Performance comparison of our proposed SEMGUD with\nother unsupervised methods [12, 23, 36, 39, 48, 50] on different\ndatasets.\nestimation. For instance, in [27], the dark channel prior is\nused to estimate the blur kernel and reconstruct the sharp\nimage. However, the blurry characteristics in real-world sce-\nnarios are quite complex, making it challenging to accurately\nestimate the optimal blur kernel. In addition, these meth-\nods often require complex iterative optimisation processes,\nwhich may lead to long inference time.\nIn recent years, with the rapid development of deep learn-\ning technology, convolutional neural networks (CNNs) have\nbeen widely used in deblurring tasks, achieving significant\nsuccess. The supervised methods [4–6, 15, 16, 19, 29, 34, 44–\n46] focus on training deep neural network models using a\nlarge number of paired sharp and blurry images. This en-\nables the network to learn the mapping from blurry images\nto sharp images without the need for blur kernel estima-\ntion, achieving end-to-end reconstruction of the blurry and\nsharp images. For example, DeepDeblur [24] proposes a\nmulti-scale CNN to implement a coarse-to-fine processing\npipeline and directly restores sharp images. However, in\nthe real world cases, for the supervised learning methods,\ncollecting paired datasets from the real world is challenging,\nand manually synthesized datasets are difficult to simulate\nthe complex real image degradation processes.\nCompared to supervised deep learning methods, unsuper-\nvised deep learning methods [3, 25, 49, 50] for real world\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25691",
        "chosen": "This paper presents an unsupervised image deblurring framework based on self-enhancement. The framework generates improved pseudo-sharp and blurry image pairs progressively. A re-degradation principal component consistency loss ensures generated blurry images resemble real ones. The self-enhancement strategy improves deblurring performance without added inference complexity.",
        "rejected": "Unsupervised Blind Image Deblurring Based on Self-Enhancement. The paper addresses the problem of blind image deblurring using unsupervised learning methods. It introduces a self-enhancement technique that iteratively refines the image, leading to clearer outcomes without prior knowledge of blur kernels."
    },
    {
        "prompt": "summary: Phase unwrapping (PU) is a technique to reconstruct\noriginal phase images from their noisy wrapped counter-\nparts, finding many applications in scientific imaging. Al-\nthough supervised learning has shown promise in PU, its\nutility is limited in ground-truth (GT) scarce scenarios.\nThis paper presents an unsupervised learning approach that\neliminates the need for GTs during end-to-end training.\nOur approach leverages the insight that both the gradients\nand wrapped gradients of wrapped phases serve as noisy\nlabels for GT phase gradients, along with sparse outliers\ninduced by the wrapping operation. A recorruption-based\nself-reconstruction loss in the gradient domain is proposed\nto mitigate the adverse effects of label noise, complemented\nwith a self-distillation loss for improved generalization. Ad-\nditionally, by unfolding a variational model of PU that uti-\nlizes wrapped gradients of wrapped phases for its data-\nfitting term, we develop a deep unrolling network that en-\ncodes physics of phase wrapping and incorporates special\ntreatments on outliers. In the experiments on three types\nof phase data, our approach outperforms existing GT-free\nmethods and competes well against the supervised ones.\n1. Introduction\nIn many imaging systems, the direct acquisition of original\nphases of a target signal is both challenging and costly. Typ-\nically, initial measurement yields a phase image wrapped in\n[−π, π). Let X ∈RM×N and Y ∈[−π, π)M×N denote\nthe true phase image and its noisy wrapped counterpart, re-\nspectively. The wrapping process can be formulated as:\nY = W(X + N),\nW : RM×N →[−π, π)M×N,\n(1)\n*Corresponding author: Yuhui Quan.\nThis work is supported by National Natural Science Foundation of\nChina (Grant No. 62372186), Natural Science Foundation of Guangdong\nProvince (Grant No. 2022A1515011755, 2023A1515012841), Fundamen-\ntal Research Funds for Central Universities (Grant No. x2jsD2230220),\nand Singapore MOE AcRF Tier 1 (Grant No. A-8000981-00-00).\nwhere W denotes the wrapping operator with its entry-wise\noperation defined as W(θ) =\n\u0000(θ + π) mod 2π\n\u0001\n−π for\nθ ∈R, and N ∈RM×N denotes the measurement noise,\ntypically assumed to be Gaussian [22]. Consequently, the\noriginal phase image, which exhibits strong smoothness, is\ndegraded to a wrapped version with both incorrect values\nand artificial discontinuities induced by the modulo opera-\ntion, which adversely affects subsequent processing.\nPhase unwrapping (PU) is about recovering the origi-\nnal unwrapped phase image X (up to an additive constant)\nfrom its noisy wrapped counterpart Y . Serving as a crit-\nical step in many optical imaging techniques for generat-\ning clear and coherent phase images, PU finds extensive\napplications in diverse domains such as quantitative phase\nimaging, magnetic resonance imaging, synthetic aperture\nradar interferometry, 3D depth sensing, phase contrast mi-\ncroscopy, fringe projection, and digital holographic inter-\nferometry; see e.g. [10, 14, 16, 23, 41, 47, 56, 57].\nThe model (1) is also usually formulated as follows:\nY = X −2πK + N ′, K ∈ZM×N,\n(2)\nwhere K ∈ZM×N denotes the map of wrap counts, indi-\ncating the number of times a phase value has been wrapped\naround by 2π, and N ′ denotes the noise dependent on both\nX and N, whose distribution can be complex even when\nN follows a simple distribution such as the normal distribu-\ntion. It can be seen that PU is a challenging ill-posed inverse\nproblem involving a continuous variable X and a discrete\nvariable K. Direct solutions by integrating wrapped phase\ndifferences are insufficient for ensuring spatial consistency\nin all directions of the unwrapped phases and may accumu-\nlate significant errors due to substantial noise.\nLimitations of existing works: Conventional approaches\nfor PU include reliability-guided methods (e.g. [3, 9, 11,\n23, 36]), filtering-based methods (e.g. [2, 45]), and model-\ndriven methods (e.g. [1, 12, 15]). These approaches are ei-\nther sensitive to measurement noise, due to their heuristic\nrules, or overly simplistic for complex phase structures, due\nto their handcrafted image priors. In contrast, deep learning\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25182",
        "chosen": "This paper introduces an unsupervised learning approach for phase unwrapping (PU) that doesn't require ground-truth during training. The approach uses gradients and wrapped gradients of wrapped phases as noisy labels, with a self-reconstruction loss and self-distillation loss. It also develops a deep unrolling network encoding physics of phase wrapping. The method outperforms existing GT-free methods and competes with supervised ones.",
        "rejected": "Unsupervised Deep Unrolling Networks for Phase Unwrapping. This paper develops an unsupervised deep unrolling network designed to solve phase unwrapping problems efficiently. The proposed method combines traditional optimization algorithms with deep learning, offering faster computation times and improved accuracy."
    },
    {
        "prompt": "summary: Recent Large Language Models (LLMs) have been en-\nhanced with vision capabilities, enabling them to compre-\nhend images, videos, and interleaved vision-language con-\ntent. However, the learning methods of these large multi-\nmodal models (LMMs) typically treat videos as predeter-\nmined clips, rendering them less effective and efficient at\nhandling streaming video inputs.\nIn this paper, we pro-\npose a novel Learning-In-Video-Stream (LIVE) framework,\nwhich enables temporally aligned, long-context, and real-\ntime dialogue within a continuous video stream. Our LIVE\nframework comprises comprehensive approaches to achieve\nvideo streaming dialogue, encompassing: (1) a training ob-\njective designed to perform language modeling for contin-\nuous streaming inputs, (2) a data generation scheme that\nconverts offline temporal annotations into a streaming di-\nalogue format, and (3) an optimized inference pipeline to\nspeed up interactive chat in real-world video streams. With\nBCorresponding Author.\nour LIVE framework, we develop a simplified model called\nVideoLLM-online and demonstrate its significant advan-\ntages in processing streaming videos.\nFor instance, our\nVideoLLM-online-7B model can operate at over 10 FPS\non an A100 GPU for a 5-minute video clip from Ego4D\nnarration.\nMoreover, VideoLLM-online also showcases\nstate-of-the-art performance on public offline video bench-\nmarks, such as recognition, captioning, and forecasting.\nThe code, model, data, and demo have been made available\nat showlab.github.io/videollm-online.\n1. Introduction\nBuilding the future of always-on contextual AI agent that\ncan promptly answer any human question, digitizing inputs\nas episodic memories and forecasting future plans given any\nquery in online continuous setting represents a “holy grail”\nmission in AI research. Powered by advancements in large\nlanguage models (LLMs) [8, 34, 60, 61, 63, 72], recent large\nmultimodal models (LMMs) have unveiled impressive ca-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n18407",
        "chosen": "This paper proposes the LIVE framework for temporally aligned, long-context, and real-time dialogue within continuous video streams. LIVE includes a training objective for streaming inputs, a data generation scheme, and an optimized inference pipeline. The VideoLLM-online model demonstrates significant advantages in processing streaming videos and shows state-of-the-art performance on public offline video benchmarks.",
        "rejected": "VideoLLM-online Online Video Large Language Model for Streaming Video. This contribution presents VideoLLM, a large language model tailored for streaming video analysis. Capable of processing online videos in real-time, it showcases remarkable performance in understanding and generating textual descriptions synchronized with video content."
    },
    {
        "prompt": "summary: Recent breakthroughs in vision-language models (VLMs)\nstart a new page in the vision community.\nThe VLMs\nprovide stronger and more generalizable feature embed-\ndings compared to those from ImageNet-pretrained models,\nthanks to the training on the large-scale Internet image-text\npairs. However, despite the amazing achievement from the\nVLMs, vanilla Vision Transformers (ViTs) remain the de-\nfault choice for the image encoder. Although pure trans-\nformer proves its effectiveness in the text encoding area,\nit remains questionable whether it is also the case for im-\nage encoding, especially considering that various types of\nnetworks are proposed on the ImageNet benchmark, which,\nunfortunately, are rarely studied in VLMs. Due to small\ndata/model scale, the original conclusions of model design\non ImageNet can be limited and biased. In this paper, we\naim at building an evaluation protocol of vision models\nin the vision-language era under the contrastive language-\nimage pretraining (CLIP) framework. We provide a com-\nprehensive way to benchmark different vision models, cov-\nering their zero-shot performance and scalability in both\nmodel and training data sizes. To this end, we introduce\nViTamin, a new vision models tailored for VLMs. ViTamin-\nL significantly outperforms ViT-L by 2.0% ImageNet zero-\nshot accuracy, when using the same publicly available\nDataComp-1B dataset and the same OpenCLIP training\nscheme.\nViTamin-L presents promising results on 60 di-\nverse benchmarks, including classification, retrieval, open-\nvocabulary detection and segmentation, and large multi-\nmodal models. When further scaling up the model size, our\nViTamin-XL with only 436M parameters attains 82.9% Im-\nageNet zero-shot accuracy, surpassing 82.0% achieved by\nEVA-E that has ten times more parameters (4.4B).\n1. Introduction\nThe past decades have witnessed significant progress in\ncomputer vision, like visual recognition tasks. The advent\nof AlexNet [53] marked a significant milestone, catalyz-\ning the extensive evolution and dominance of Convolutional\ntext model\n“a photo of cat”\ncontrastive loss\nvision model\nData Scalability\nModel Scalability\nCNN+Transformer\nFeature Resolution\ndesigning\nbenchmark modern \nbackbones in CLIP\nImageNet\navg. 38 dataset\nretrieval\n81.8\n80.3\n67.2\n64.7\n65.7\nVTAB\ndist. shift\n62.5\n72.4\n70.2\n61.8\n61.1\nLMM\n(VQAv2)\nLMM\n(LLAVA-Bench)\n78.9\n75.9\n66.1\n60.6\nOV pan. seg \n(ADE)\nOV sem. seg \n(A-150)\nOV detection\n(OV-LVIS)\n27.3\n24.6\n31.8\n35.6\n32.5\n35.6\nViTamin-L\nViT-L/14\nViTamin\n38 zero-shot \nclassification/\nretrieval tasks\n10 open-vocab dense tasks\n12 LMM tasks (e.g., VQA)\nFigure 1.\nPractices of designing scalable vision models in\nthe vision-language era.\nWe benchmark modern vision mod-\nels with various model and data scales under CLIP setting using\nDataComp-1B [30], leading to findings about data and model scal-\nability, feature resolution, and hybrid architecture, which motivate\nus to develop ViTamin for VLM. ViTamin-L achieves superior\nzero-shot performance over ViT-L/14 [60] on ImageNet [86] and\naverage 38 datasets [30], and advances a suite of 22 downstream\ntasks for Open-Vocabulary (OV) detection [111] and segmenta-\ntion [124], and Large Multi-modal Model (LMM) tasks [67].\nNeural Networks (ConvNets) [8, 32, 37, 38, 46, 55, 72, 73]\nin computer vision. More recently, with the debut of Vision\nTransformer [23, 104], a growing number of transformer-\nbased architectures [18, 71, 103, 108, 116, 121] have shown\ngreat potential to surpass the prior ConvNet counterparts.\nThe rapid advancement of neural network design in com-\nputer vision can be attributed to a combination of factors.\nAmong them, an important factor is the well-established\nbenchmarks, allowing the community to examine the devel-\nopments in a standardized way. Particularly, ImageNet [86]\nhas become the de facto testing ground for new vision\nmodels.\nIt not only sets a standard benchmark for vi-\nsual recognition, but also serves as a mature pre-training\ndataset for transferring the network backbone to a vari-\nety of downstream tasks (e.g., detection and segmenta-\ntion) [9, 10, 15, 38, 51, 66, 73, 95, 107, 117, 122, 123].\nRecently, the emergence of vision-language models\n(VLMs) [50, 82] has changed the paradigm by leveraging\nthe pre-training schedule on the extremely large scale noisy\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n12954",
        "chosen": "This paper aims to build an evaluation protocol for vision models in the vision-language era under the CLIP framework. It introduces ViTamin, a new vision model tailored for VLMs. ViTamin-L outperforms ViT-L in ImageNet zero-shot accuracy and presents promising results on diverse benchmarks. ViTamin-XL surpasses models with ten times more parameters.",
        "rejected": "ViTamin Designing Scalable Vision Models in the Vision-Language Era. Focusing on scalable vision models, this paper introduces ViTamin, which integrates advancements in vision-language models. ViTamin is designed to handle large-scale visual recognition tasks more effectively, demonstrating its superiority in multiple benchmarks."
    },
    {
        "prompt": "summary: Recent innovations on text-to-3D generation have fea-\ntured Score Distillation Sampling (SDS), which enables the\nzero-shot learning of implicit 3D models (NeRF) by directly\ndistilling prior knowledge from 2D diffusion models. How-\never, current SDS-based models still struggle with intricate\ntext prompts and commonly result in distorted 3D models\nwith unrealistic textures or cross-view inconsistency issues.\nIn this work, we introduce a novel Visual Prompt-guided\ntext-to-3D diffusion model (VP3D) that explicitly unleashes\nthe visual appearance knowledge in 2D visual prompt to\nboost text-to-3D generation. Instead of solely supervising\nSDS with text prompt, VP3D first capitalizes on 2D dif-\nfusion model to generate a high-quality image from input\ntext, which subsequently acts as visual prompt to strengthen\nSDS optimization with explicit visual appearance. Mean-\nwhile, we couple the SDS optimization with additional dif-\nferentiable reward function that encourages rendering im-\nages of 3D models to better visually align with 2D visual\nprompt and semantically match with text prompt. Through\nextensive experiments, we show that the 2D Visual Prompt\nin our VP3D significantly eases the learning of visual ap-\npearance of 3D models and thus leads to higher visual fi-\ndelity with more detailed textures. It is also appealing in\nview that when replacing the self-generating visual prompt\nwith a given reference image, VP3D is able to trigger a new\ntask of stylized text-to-3D generation. Our project page is\navailable at https://vp3d-cvpr24.github.io.\n1. Introduction\nGenerative Artificial Intelligence (especially for vision con-\ntent generation) has aroused great attention in computer\nvision field [5, 6, 20, 26], leading to impressive advance-\nments in text-to-image [30–32] and text-to-video generation\n[10, 14, 34]. These accomplishments can be attributed to\nthe availability of large-scale image-text and video-text pair\ndata [1, 33] and the emergence of robust diffusion-based\ngenerative models [12, 13, 25, 35]. Recently, researchers\nText prompt: “A florist is making a bouquet with fresh flowers”\n(a) Magic3D\n(b) ProlificDreamer\n(c) VP3D (Ours)\nVisual prompt\nFigure 1.\nExisiting text-to-3D generation techniques (e.g.,\nMagic3D [17] and ProlificDreamer [39]) often suffer from de-\ngenerated results (e.g., over-saturated appearances or inaccurate\ngeometries).\nOur VP3D novelly integrates a visual prompt to\nstrength score distillation sampling, leading to better 3D results.\nhave gone beyond text-driven image/video generation, and\nbegun exploring diffusion models for text-driven content\ncreation of 3D assets (e.g., text-to-3D generation). This di-\nrection paves a new way for practical 3D content creation\nand has a great potential impact for numerous applications\nlike virtual reality, gaming and Metaverse. Compared to\nimage generation, text-to-3D generation, however, is more\nchallenging, due to the complexities associated with intri-\ncate 3D geometry and appearance (i.e., object shapes and\ntextures). Moreover, the collection and annotation of 3D\ndata are somewhat resourcefully expensive and thus cannot\nbe easily scaled up to billion level as image data.\nTo tackle this issue, a pioneering text-to-3D work\n(DreamFusion [27]) presents the first attempt of exploit-\ning an off-the-shelf text-to-image diffusion model to gen-\nerate promising 3D assets in a zero-shot fashion. The key\ndesign behind such success is Score Distillation Sampling\n(SDS), which directly optimizes the implicit 3D model of\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n4896",
        "chosen": "This paper introduces VP3D, a Visual Prompt-guided text-to-3D diffusion model. VP3D uses a 2D visual prompt generated from input text to strengthen SDS optimization. It couples SDS with a differentiable reward function for better visual alignment. Experiments show VP3D significantly improves visual fidelity and texture details.",
        "rejected": "VP3D Unleashing 2D Visual Prompt for Text-to-3D Generation. VP3D is introduced as a method to generate 3D models from text prompts using 2D visual cues. This technique enhances the realism and detail of generated 3D objects, making significant contributions to the field of computer graphics."
    },
    {
        "prompt": "summary: Spatio-temporal grounding describes the task of local-\nizing events in space and time, e.g., in video data, based\non verbal descriptions only. Models for this task are usu-\nally trained with human-annotated sentences and bound-\ning box supervision. This work addresses this task from\na multimodal supervision perspective, proposing a frame-\nwork for spatio-temporal action grounding trained on loose\nvideo and subtitle supervision only, without human annota-\ntion. To this end, we combine local representation learn-\ning, which focuses on leveraging ﬁne-grained spatial in-\nformation, with a global representation encoding that cap-\ntures higher-level representations and incorporates both in\na joint approach. To evaluate this challenging task in a real-\nlife setting, a new benchmark dataset is proposed, provid-\ning dense spatio-temporal grounding annotations in long,\nuntrimmed, multi-action instructional videos for over 5K\nevents. We evaluate the proposed approach and other meth-\nods on the proposed and standard downstream tasks, show-\ning that our method improves over current baselines in var-\nious settings, including spatial, temporal, and untrimmed\nmulti-action spatio-temporal grounding.\n1. Introduction\nSpatio-temporal grounding (STG) describes the challenging\ntask of locating events in space and time within video data\nbased on text referential expressions. Methods in this ﬁeld\nusually rely on a combination of spatio-temporal bound-\ning box annotation, together with a human-generated cap-\ntion, describing the visual content of the bounding box\n[23, 54], which limits their generalizability beyond the\ngiven training scenario. Compared to that, as a second line\nof work, multimodal self-supervised learning tries to lever-\nage “free” data sources, such as video and automatic speech\nTask: Spatio-Temporal Grounding - Find the temporal boundary of a \nqueried action in an untrimmed video and spatially localize the action. \n\"Crack egg\"\n…\nbackground\nbackground\n…\n…\n…\nEvaluation Setup: Referential queries - \"Crack egg\", \"Mix egg\", etc.\nTraining Setup:  Unlabeled videos with narrated instructions\n\"To this we will add \none carrot chopped \ninto jong jullien …\"\n\"… you can use the \nforks to pull it a bit \nappart…\"\n\"… stir it a bit so it mixes \nwell…\"\nbackground\n\"Mix egg\"\nFigure 1. Learning Spatio-temporal grounding in untrimmed\nvideos. In training, we learn from unlabeled videos without human\nannotation. In evaluation, we perform spatio-temporal grounding\nusing an action description such as “crack egg” as a query. The\nmodel needs to localize both the action’s temporal boundary and\nspatial region in the long untrimmed video. We visualize the heat-\nmap from the annotation points as well as derived bounding boxes.\nrecognition (ASR) captions from large-scale instructional\nvideos to learn representations without human annotation\n[3, 4, 8, 35, 36]. The resulting models achieve state-of-\nthe-art performance on zero-shot tasks such as cross-modal\nvideo retrieval or classiﬁcation and also for zero-shot tem-\nporal action segmentation and detection based on free text\nqueries [8, 28, 42, 47, 66], but usually lack spatial local-\nization abilities.\nA third line of work focuses on label-\nfree spatial grounding, e.g. by training on image-caption\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n18419",
        "chosen": "This paper addresses spatio-temporal grounding from a multimodal supervision perspective, proposing a framework trained on loose video and subtitle supervision. It combines local and global representation learning. A new benchmark dataset with dense annotations in long instructional videos is proposed. The method improves over baselines in various settings.",
        "rejected": "What When and Where Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action. This study explores self-supervised learning for spatio-temporal grounding in long, untrimmed videos containing multiple actions. It proposes a framework that accurately localizes and recognizes actions within complex scenes without manual annotations."
    },
    {
        "prompt": "summary: Existing methods for synthesizing 3D human gestures\nfrom speech have shown promising results, but they do not\nexplicitly model the impact of emotions on the generated\ngestures. Instead, these methods directly output animations\nfrom speech without control over the expressed emotion. To\naddress this limitation, we present AMUSE, an emotional\nspeech-driven body animation model based on latent dif-\nfusion. Our observation is that content (i.e., gestures re-\nlated to speech rhythm and word utterances), emotion, and\npersonal style are separable. To account for this, AMUSE\nmaps the driving audio to three disentangled latent vec-\ntors: one for content, one for emotion, and one for personal\nstyle. A latent diffusion model, trained to generate gesture\nmotion sequences, is then conditioned on these latent vec-\ntors. Once trained, AMUSE synthesizes 3D human gestures\ndirectly from speech with control over the expressed emo-\ntions and style by combining the content from the driving\nspeech with the emotion and style of another speech se-\nquence. Randomly sampling the noise of the diffusion model\nfurther generates variations of the gesture with the same\nemotional expressivity. Qualitative, quantitative, and per-\nceptual evaluations demonstrate that AMUSE outputs real-\nistic gesture sequences. Compared to the state of the art, the\ngenerated gestures are better synchronized with the speech\ncontent, and better represent the emotion expressed by the\ninput speech. Our code is available at amuse.is.tue.mpg.de.\n1. Introduction\nAnimating 3D bodies from speech has a wide range of ap-\nplications, such as telepresence in AR/VR, avatar animation\nin games and movies, and to embody interactive digital as-\nsistants. While methods for speech-driven 3D body anima-\ntion have recently shown great progress [5, 7, 31, 56, 101],\nexisting methods do not adequately address one crucial fac-\ntor: the impact of emotion from the driving speech signal on\nthe generated gestures. Emotions and their expressions play\n*Now at Google.\n... choose a major that is easy to ﬁnd a good job in the future... \n... choose a major that is easy to ﬁnd a good job in the future... \nGestures from neutral speech input.\nGestures from surprise synchronized with neutral speech input.\nFigure 1. Goal. AMUSE generates realistic emotional 3D body\ngestures directly from a speech sequence (top). It provides user\ncontrol over the generated emotion by combining the driving\nspeech sequence with a different emotional audio (bottom).\na fundamental role in human communication [29, 35, 65]\nand have become an important consideration when design-\ning computer systems that interact with humans in a nat-\nural manner [78, 79]. They are of central concern when\nsynthesizing human animations for a wide variety of appli-\ncation contexts, such as Socially Interactive Agents [61].\nBecause of this, speech-driven animation systems must not\nonly align movement with the rhythm of the speech, but\nshould also be capable of generating gestures that are per-\nceived as expressing the suitable emotion.\nMany factors contribute to the perception of emotion\nand personal idiosyncrasies, such as facial expressions [19],\ngaze and eye contact [42], physiological responses [47],\ntone of voice [87], body language [66], and gestures [39].\nWhen it comes to 3D animation, the most relevant factors\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1942",
        "chosen": "This paper presents AMUSE, an emotional speech-driven body animation model based on latent diffusion. AMUSE disentangles content, emotion, and personal style in latent vectors. It synthesizes 3D human gestures from speech with control over emotion and style. Evaluations show realistic gesture sequences better synchronized with speech content and emotion.",
        "rejected": "Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion. The paper discusses a method for generating realistic 3D body animations driven by emotional speech. Using disentangled latent diffusion models, it achieves nuanced and contextually appropriate animations, significantly advancing the state of the art in non-verbal communication synthesis."
    },
    {
        "prompt": "summary: We introduce a novel approach to single image denoising\nbased on the Blind Spot Denoising principle, which we call\nMAsked and SHuffled Blind Spot Denoising (MASH). We\nfocus on the case of correlated noise, which often plagues\nreal images. MASH is the result of a careful analysis to\ndetermine the relationships between the level of blindness\n(masking) of the input and the (unknown) noise correlation.\nMoreover, we introduce a shuffling technique to weaken the\nlocal correlation of noise, which in turn yields an addi-\ntional denoising performance improvement. We evaluate\nMASH via extensive experiments on real-world noisy image\ndatasets. We demonstrate state-of-the-art results compared\nto existing self-supervised denoising methods. Website:\nhttps://hamadichihaoui.github.io/mash.\n1. Introduction\nThe removal of noise from real images, i.e., image denois-\ning, is a fundamental and still open problem in image pro-\ncessing despite having a long history of dedicated research\n(see [9] for an overview of the classic and recent methods).\nIn classic methods, the primary strategies involve manually\ndesigning image priors and optimization techniques to en-\nhance both reconstruction accuracy and speed. In contrast,\nin the context of deep learning methods, neural networks\nnaturally introduce a very powerful prior for images [24]\nand provide models that could perform denoising efficiently\nat inference time. These innate capabilities of neural net-\nworks opened the doors to a wide range of methods that\ncould not only learn to denoise image from examples of\nnoisy and clean image pairs, but, even more remarkably,\ndirectly from single noisy images [12, 15, 25, 26].\nIn this work, we push the limits of these advanced meth-\nods one step further. We focus on the family of methods\ncalled Blind Spot Denoising (BSD)[15], since it provides a\npowerful and general framework. Moreover, we consider\nthe case where only a single image is used for denoising\n(i.e., we do not rely on a supporting dataset). As also ob-\nserved by Wang et al [25], training on a dataset may not\ngeneralize well on new data, where the noise distribution is\nunknown. This is particularly true for real images, where\nnoise is often correlated. In these settings, most modern\nmethods find it challenging to handle non-iid data.\nIn our work, similar to the approach in [21], we explore\nthe more general setting of random masking beyond the sin-\ngle blind spot method introduced in [15]. In our analysis,\nwe uncover valuable connections between the performance\nof Blind Spot Denoising (BSD) methods trained with vari-\nous input masking techniques and the degree of noise cor-\nrelation. Surprisingly, we observe that models trained with\na higher masking ratio tend to perform better when dealing\nwith highly correlated noise, whereas models trained with a\nlower masking ratio excel in denoising tasks with iid noise.\nThis discovery offers two key contributions: 1) it provides a\nmethod to estimate the unknown level of noise correlation,\nand 2) it offers a strategy for achieving enhanced denoising\nperformance. Furthermore, our analysis reveals that noise\ncorrelation significantly hampers the denoising capabilities\nof BSD models. This suggests that a more radical approach\nwould be to directly eliminate the correlation in the input\ndata. An intuitive method to achieve this would involve\nrandomly permuting all pixels that correspond to the same\nclean-image color intensity. However, this presents a classic\nchicken and egg dilemma, as we would typically need the\nclean image to perform the permutation, yet the clean im-\nage is precisely what we are trying to restore. To tackle this\nchallenge, we utilize an intermediate denoised image as a\npseudo-clean image to define the permutation set. Further-\nmore, given that adjacent pixels are likely to have similar\ncolor intensities, we focus on shuffling only pixels within\nsmall neighborhoods. We incorporate these insights into a\nnovel method called MAsked and SHuffled Blind Spot De-\nnoising (MASH), which we elaborate on further in Sec. 3.\nOur contributions are summarized as follows\n• We provide an analysis of BSD, showcasing the impact of\nvarious masking ratios on correlated noise and presenting\na method for estimating the noise correlation level;\n• We introduce MASH, an enhanced version of BSD that\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n3025",
        "chosen": "This paper introduces MASH, a novel approach to single image denoising based on Blind Spot Denoising principles. MASH focuses on correlated noise in real images and introduces a shuffling technique to weaken local noise correlation. Extensive experiments demonstrate state-of-the-art results compared to existing self-supervised denoising methods.",
        "rejected": "We introduce MASH, a novel Blind Spot Denoising technique that handles correlated noise in images by utilizing masked and shuffled input data. This method improves denoising performance on real-world noisy images compared to existing self-supervised methods."
    },
    {
        "prompt": "summary: Referring Image Segmentation (RIS) is a challenging\ntask that requires an algorithm to segment objects referred\nby free-form language expressions.\nDespite significant\nprogress in recent years, most state-of-the-art (SOTA) meth-\nods still suffer from considerable language-image modality\ngap at the pixel and word level. These methods generally\n1) rely on sentence-level language features for language-\nimage alignment and 2) lack explicit training supervision\nfor fine-grained visual grounding. Consequently, they ex-\nhibit weak object-level correspondence between visual and\nlanguage features. Without well-grounded features, prior\nmethods struggle to understand complex expressions that\nrequire strong reasoning over relationships among multiple\nobjects, especially when dealing with rarely used or am-\nbiguous clauses. To tackle this challenge, we introduce a\nnovel Mask Grounding auxiliary task that significantly im-\nproves visual grounding within language features, by ex-\nplicitly teaching the model to learn fine-grained correspon-\ndence between masked textual tokens and their matching\nvisual objects. Mask Grounding can be directly used on\nprior RIS methods and consistently bring improvements.\nFurthermore, to holistically address the modality gap, we\nalso design a cross-modal alignment loss and an accom-\npanying alignment module. These additions work syner-\ngistically with Mask Grounding. With all these techniques,\nour comprehensive approach culminates in MagNet (Mask-\ngrounded Network), an architecture that significantly out-\nperforms prior arts on three key benchmarks (RefCOCO,\nRefCOCO+ and G-Ref), demonstrating our method’s effec-\ntiveness in addressing current limitations of RIS algorithms.\nOur code and pre-trained weights will be released.\n1. Introduction\nDeep learning has greatly improved the performance of vi-\nsion algorithms on many image segmentation tasks, such\nas semantic segmentation [5, 48], instance segmentation\n[2, 12, 24, 42] and panoptic segmentation [8, 36]. These\n† Project lead.\nB Corresponding author.\ndiving man\ndiving man\n(b) Fine-grained visual grounding is required to understand\nexpressions used in uncommon or ambiguous contexts.\nCorrect\nWrong\ndiving man\nCorrect\nthird remote from the left\n(a) Fine-grained visual grounding is required to reason over\ncomplicated relationships among multiple objects.\nthird remote from the left\nthird remote from the left\nCorrect\nWrong\nWrong\nFigure 1. Importance of Fine-grained Visual Grounding for RIS.\nMost RIS algorithms lack well-grounded text features. As a result,\nthey struggle in difficult cases illustrated in (a) and (b). Red mask\nare predictions of LAVT, one of the recent SOTA RIS methods.\nYellow dotted boxes are the ground truths.\ntasks require grouping of image pixels under a fixed set of\npre-defined categories and mainly differ in the granularity\nof grouping semantics required. In contrast to these uni-\nmodal segmentation tasks, Referring Image Segmentation\n(RIS) [9, 28] is a challenging multi-modal task that requires\nan algorithm to simultaneously understand fine-grained hu-\nman language expression and make correct pixel-level cor-\nrespondence to the referred object. Recently, it has gained\nwidespread research attention due to its potential to im-\nprove human-robot interaction [1], interactive image editing\n[43, 52] and advanced driver-assistance systems [29].\nThe key challenge in RIS lies in how to reduce the\nmodality gap between language and image features [14, 64,\n71]. To tackle this challenge, we need to have an effective\nalignment between a given language expression and the cor-\nresponding image pixels for highlighting the referred target.\nIdeally, with robust pixel-wise language-image alignment,\nlanguage and image features should have high feature simi-\nlarity when referring to the same object and low feature sim-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n26573",
        "chosen": "This paper presents MagNet, a mask-grounded network for referring image segmentation (RIS). The method introduces a Mask Grounding auxiliary task to improve visual grounding within language features and designs a cross-modal alignment loss and alignment module. MagNet significantly outperforms prior arts on key benchmarks.",
        "rejected": "This paper presents MagNet, an advanced referring image segmentation network incorporating Mask Grounding and cross-modal alignment loss to address language-image modality gaps, enhancing object-level correspondence between visual and language features for improved segmentation accuracy."
    },
    {
        "prompt": "summary: Unsupervised domain adaptation (UDA) for semantic\nsegmentation aims to transfer the pixel-wise knowledge\nfrom the labeled source domain to the unlabeled target do-\nmain.\nHowever, current UDA methods typically assume\na shared label space between source and target, limiting\ntheir applicability in real-world scenarios where novel cat-\negories may emerge in the target domain. In this paper, we\nintroduce Open-Set Domain Adaptation for Semantic Seg-\nmentation (OSDA-SS) for the first time, where the target\ndomain includes unknown classes. We identify two major\nproblems in the OSDA-SS scenario as follows: 1) the exist-\ning UDA methods struggle to predict the exact boundary of\nthe unknown classes, and 2) they fail to accurately predict\nthe shape of the unknown classes. To address these issues,\nwe propose Boundary and Unknown Shape-Aware open-\nset domain adaptation, coined BUS. Our BUS can accu-\nrately discern the boundaries between known and unknown\nclasses in a contrastive manner using a novel dilation-\nerosion-based contrastive loss.\nIn addition, we propose\nOpenReMix, a new domain mixing augmentation method\nthat guides our model to effectively learn domain and size-\ninvariant features for improving the shape detection of the\nknown and unknown classes.\nThrough extensive experi-\nments, we demonstrate that our proposed BUS effectively\ndetects unknown classes in the challenging OSDA-SS sce-\nnario compared to the previous methods by a large margin.\nThe code is available at https://github.com/KHU-\nAGI/BUS.\n1. Introduction\nIn semantic segmentation, a model predicts pixel-wise cat-\negory labels given an input image.\nSemantic segmen-\ntation has a lot of applications, e.g., autonomous driv-\ning [1], human-machine interaction [2], and augmented re-\n*Equal contribution\n†Corresponding authors\n(a) Ground truth.\n(b) UDA method (MIC [12]).\n(c) Confidence-based threshold.\n(d) Unknown head-expansion.\n(e) BUS (Ours).\nFigure 1. Visualization of prediction maps in the OSDA-SS sce-\nnario. The pixels detected by the white color mean the unknown\nclasses. The naive UDA method (b) is completely unaware of the\nunknown classes. Even after applying simple techniques to help\nthe UDA model recognize the unknown, it still struggles to accu-\nrately predict the shape of the unknown, as shown in (c) and (d).\nality.\nOver the past decade, there has been notable ad-\nvancement in supervised semantic segmentation driven by\ndeep neural networks [3–6]. However, supervised seman-\ntic segmentation requires pixel-level annotations, which\nare labor-intensive and costly to collect. To mitigate the\nchallenges, unsupervised domain adaptation (UDA) has\nemerged. Many studies [7–12] leverage the already-labeled\nsource data to achieve high performance on the unlabeled\ntarget data. Notably, synthetic datasets such as GTA5 [13]\nand SYNTHIA [14] which are automatically generated by\ngame engines present valuable resources for UDA research.\nUDA methods typically presume that source and target\ndomains share the same label space. Such an assumption\nis not reasonable in real-world applications. In the target\ndata, novel categories not presented in the source dataset\n(target-private categories) may emerge, leading to an Open-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n23943",
        "chosen": "This paper addresses open-set domain adaptation for semantic segmentation, where the target domain includes unknown classes. The proposed BUS method accurately discerns boundaries between known and unknown classes and improves shape detection. The method demonstrates effective unknown class detection in challenging scenarios.",
        "rejected": "The authors propose BUS, an open-set domain adaptation method for semantic segmentation capable of detecting unknown classes by using dilation-erosion-based contrastive loss and OpenReMix augmentation, significantly outperforming previous methods in identifying boundaries and shapes of unknown classes."
    },
    {
        "prompt": "summary: This paper proposes a novel direct Audio-Visual Speech\nto Audio-Visual Speech Translation (AV2AV) framework,\nwhere the input and output of the system are multimodal\n(i.e., audio and visual speech). With the proposed AV2AV,\ntwo key advantages can be brought: 1) We can perform\nreal-like conversations with individuals worldwide in a\nvirtual meeting by utilizing our own primary languages.\nIn contrast to Speech-to-Speech Translation (A2A), which\nsolely translates between audio modalities, the proposed\nAV2AV directly translates between audio-visual speech.\nThis capability enhances the dialogue experience by\npresenting synchronized lip movements along with the\ntranslated speech.\n2) We can improve the robustness of\nthe spoken language translation system. By employing the\ncomplementary information of audio-visual speech, the\nsystem can effectively translate spoken language even in\nthe presence of acoustic noise, showcasing robust perfor-\nmance. To mitigate the problem of the absence of a parallel\nAV2AV translation dataset, we propose to train our spoken\nlanguage translation system with the audio-only dataset of\nA2A. This is done by learning unified audio-visual speech\nrepresentations through self-supervised learning in ad-\nvance to train the translation system. Moreover, we propose\nan AV-Renderer that can generate raw audio and video in\nparallel. It is designed with zero-shot speaker modeling,\nthus the speaker in source audio-visual speech can be\nmaintained at the target translated audio-visual speech.\nThe effectiveness of AV2AV is evaluated with extensive ex-\nperiments in a many-to-many language translation setting.\nDemo page is available on choijeongsoo.github.io/av2av.\n∗Equal contribution.\n†Corresponding author. This work was sup-\nported by the National Research Foundation of Korea (NRF) grant funded\nby the Korea government (MSIT) (No. NRF-2022R1A2C2005529), Insti-\ntute of Information & communications Technology Planning & Evaluation\n(IITP) grant funded by the Korea government (MSIT) (No.2022-0-00124,\nDevelopment of Artificial Intelligence Technology for Self-Improving\nCompetency-Aware Learning Capabilities), and BK21 FOUR (Connected\nAI Education & Research Program for Industry and Society Innovation,\nKAIST EE, No. 4120200113769).\nIt\nMultilingual \nAV2AV\nTranslation\n…\n“J'aimerais t'aider”\n“Ich würde dir gerne helfen”\n“Me gustaría ayudarte”\n“Mi piacerebbe aiutarti”\nDe\nEn\nPt\nEs\nFr\nAV Speech Unit \n“I would like to help you”\n“Eu gostaria de ajudar você”\n…\nAV Speech Unit \nFigure 1.\nConceptual illustration of the proposed multilin-\ngual Audio-Visual Speech to Audio-Visual Speech Translation\n(AV2AV) framework. The system can directly translate between\nmultilingual AV speech without requiring any text.\nNote that\nthe proposed AV2AV can generate both audio speech and visual\nspeech in listener-oriented (i.e., translated) languages.\n1. Introduction\nIn our increasingly interconnected world, where commu-\nnication transcends linguistic boundaries, Neural Machine\nTranslation (NMT) [1–6] has played a critical role in break-\ning down barriers in multilingual interaction. Despite their\nstrong performances, NMT systems exhibit limitations in\nseamless application to virtual conferences or face-to-face\ninteractions. This is due to their reliance on human interven-\ntion for text input or speech recognition, as these systems\nprimarily operate with text modalities. Speech-to-Speech\nTranslation (A2A1) [7–11] can mitigate this problem by di-\nrectly translating spoken languages into the target language\nat the audio level. With the growth of A2A technologies\n[12], it is anticipated that individuals can effortlessly com-\nmunicate with one another using their primary languages,\nirrespective of their nationalities. However, there still exists\none unsolved problem in the aspects of multimedia, the dis-\ncrepancy between the translated speech and the visual stim-\n1Throughout this paper, we employ abbreviations for input and output\nmodalities, using A for Audio, V for Visual, and T for Text.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n27325",
        "chosen": "This paper proposes a direct Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework. The framework translates between multimodal audio-visual speech inputs and outputs, enhancing dialogue experiences with synchronized lip movements and improving translation robustness in noisy environments.",
        "rejected": "This work introduces AV2AV, a direct audio-visual speech translation framework that enhances dialogue experience by presenting synchronized lip movements along with translated speech, improving robustness against acoustic noise through self-supervised learning of unified audio-visual speech representations."
    },
    {
        "prompt": "summary: We address the problem of generalized category discov-\nery (GCD) that aims to partition a partially labeled col-\nlection of images; only a small part of the collection is la-\nbeled and the total number of target classes is unknown.\nTo address this generalized image clustering problem, we\nrevisit the mean-shift algorithm, i.e., a classic, powerful\ntechnique for mode seeking, and incorporate it into a con-\ntrastive learning framework. The proposed method, dubbed\nContrastive Mean-Shift (CMS) learning, trains an embed-\nding network to produce representations with better cluster-\ning properties by an iterative process of mean shift and con-\ntrastive update. Experiments demonstrate that our method,\nboth in settings with and without the total number of clus-\nters being known, achieves state-of-the-art performance on\nsix public GCD benchmarks without bells and whistles.\n1. Introduction\nClustering is one of the most fundamental problems in un-\nsupervised learning, which aims to partition instances of a\ndata collection into different groups [2, 15, 34, 42]. Unlike\nthe classiﬁcation problem, it does not assume either prede-\nﬁned target classes or labeled instances in its standard setup.\nHowever, in a practical scenario, some data instances may\nbe labeled for a subset of target classes so that we can lever-\nage them to cluster all the data instances while also discov-\nering the remaining unknown classes. The goal of Gen-\neralized Category Discovery (GCD) [48] is to tackle such\na semi-supervised image clustering problem given a small\namount of incomplete supervision.\nClustering is a transductive reasoning process based on\nthe neighborhood data in the given data collection. To learn\nan image embedding for this clustering purpose, we are mo-\ntivated to incorporate the neighborhood embeddings into\nlearning. We revisit mean shift [8, 11, 12, 18, 44], i.e., a\nclassic, powerful technique for mode seeking and cluster-\ning analysis. The mean-shift algorithm consists of iterative\nmode-seeking steps of updating each data point by kernel-\n!\n!!\npull\npush\n: image embedding \"\n: mean-shifted embedding #\n: augmented mean-shifted embedding #!\n: augmented image embedding \"!\n!\"\n: augmented image of !\n!!\n: different images in a batch\n!\"\npush\nFigure 1. Contrastive Mean-Shift (CMS) learning. We inte-\ngrate mean shift [11] into contrastive learning [7, 24, 59]. In train-\ning, image embeddings proceed a single-step mean shift with their\nkNNs. The contrastive learning objective pulls the mean-shifted\nembeddings of I and I+, while it pushes those from different im-\nage inputs. Colors denote different classes and k=4.\nweighted aggregation of its neighboring data points; this\nprocess is non-parametric and does not require any informa-\ntion about the target clusters, e.g., the number of clusters.\nFor GCD, we develop a GPU-friendly mean-shift variant\nand incorporate it into a contrastive representation learning\nframework [7, 24, 59].\nWe introduce Contrastive Mean-Shift (CMS) learning\nfor GCD. CMS learning aims to encode the semantic dis-\ntance between image embeddings on a mean-shifted space\nvia contrastive learning. Precisely, we perform a single-step\nmean shift for each image embedding by moving it toward\nthe mean of its neighbors in an embedding space. To per-\nform stable mean shifts in the embedding space, we use k\nnearest neighbors (kNNs) instead of typical distance-based\nneighbors [8, 11].\nIn learning, an image I and its aug-\nmented image I0 are both mean-shifted and pull each other\non the embedding space while pushing I and different im-\nages (Fig. 1). The training objective with mean-shifted em-\n1\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n23094",
        "chosen": "This paper introduces Contrastive Mean-Shift (CMS) learning for generalized category discovery (GCD). The method integrates mean shift into a contrastive learning framework, achieving state-of-the-art performance on GCD benchmarks.",
        "rejected": "The paper discusses Contrastive Mean-Shift (CMS) learning, a method for generalized category discovery that integrates mean shift into a contrastive learning framework, achieving state-of-the-art performance on multiple benchmarks without requiring total cluster number information."
    },
    {
        "prompt": "summary: Photometric stereo leverages variations in illumination\nconditions to reconstruct surface normals. Display photo-\nmetric stereo, which employs a conventional monitor as an\nillumination source, has the potential to overcome limita-\ntions often encountered in bulky and difficult-to-use conven-\ntional setups. In this paper, we present differentiable dis-\nplay photometric stereo (DDPS), addressing an often over-\nlooked challenge in display photometric stereo: the design\nof display patterns. Departing from using heuristic display\npatterns, DDPS learns the display patterns that yield accu-\nrate normal reconstruction for a target system in an end-\nto-end manner.\nTo this end, we propose a differentiable\nframework that couples basis-illumination image formation\nwith analytic photometric-stereo reconstruction. The dif-\nferentiable framework facilitates the effective learning of\ndisplay patterns via auto-differentiation. Also, for training\nsupervision, we propose to use 3D printing for creating a\nreal-world training dataset, enabling accurate reconstruc-\ntion on the target real-world setup. Finally, we exploit that\nconventional LCD monitors emit polarized light, which al-\nlows for the optical separation of diffuse and specular re-\nflections when combined with a polarization camera, lead-\ning to accurate normal reconstruction. Extensive evalua-\ntion of DDPS shows improved normal-reconstruction accu-\nracy compared to heuristic patterns and demonstrates com-\npelling properties such as robustness to pattern initializa-\ntion, calibration errors, and simplifications in image for-\nmation and reconstruction.\n1. Introduction\nReconstructing high-quality surface normals is pivotal in\ncomputer vision and graphics for 3D reconstruction [32,\n40], relighting [36, 39], and inverse rendering [45, 52].\nAmong various techniques, photometric stereo [50] lever-\nages the intensity variation of a scene point under varied\nillumination conditions to reconstruct normals. Photomet-\nric stereo finds its application in various imaging systems\nincluding light stages [29, 35, 49, 56], handheld-flash cam-\neras [3, 10, 37, 52], and display-camera systems [1, 28, 46].\nDisplay photometric stereo uses monitors and cameras as\na versatile and accessible system that can be conveniently\nplaced on a desk [1, 28, 46]. Producing diverse illumina-\ntion conditions can be simply achieved by displaying mul-\ntiple patterns using pixels on the display as programmable\npoint light sources. This convenient and intricate modu-\nlation of illumination conditions significantly enlarges the\ndesign space of illumination patterns for display photomet-\nric stereo. Nevertheless, existing approaches often rely on\nheuristic display patterns, resulting in sub-optimal recon-\nstruction quality.\nIn this paper, to exploit the large design space of illumi-\nnation patterns in display photometric stereo, we propose\ndifferentiable display photometric stereo (DDPS). The key\nidea is to learn display patterns that lead to improved recon-\nstruction of surface normals for a target system in an end-\nto-end manner. To this end, we introduce a differentiable\nframework that combines basis-illumination image forma-\ntion and an optimization-based photometric stereo method.\nThis enables effective pattern learning by directly optimiz-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n11831",
        "chosen": "This paper presents differentiable display photometric stereo (DDPS), which learns display patterns for accurate normal reconstruction in an end-to-end manner. The framework couples basis-illumination image formation with analytic photometric-stereo reconstruction, showing improved normal-reconstruction accuracy.",
        "rejected": "DDPS is presented as a differentiable display photometric stereo framework that learns optimal illumination patterns via auto-differentiation for accurate surface normal reconstruction, demonstrating superior reconstruction accuracy and robustness against various errors."
    },
    {
        "prompt": "summary: This paper presents a new approach for the detection of\nfake videos, based on the analysis of style latent vectors and\ntheir abnormal behavior in temporal changes in the gener-\nated videos. We discovered that the generated facial videos\nsuffer from the temporal distinctiveness in the temporal\nchanges of style latent vectors, which are inevitable during\nthe generation of temporally stable videos with various fa-\ncial expressions and geometric transformations. Our frame-\nwork utilizes the StyleGRU module, trained by contrastive\nlearning, to represent the dynamic properties of style latent\nvectors. Additionally, we introduce a style attention module\nthat integrates StyleGRU-generated features with content-\nbased features, enabling the detection of visual and tempo-\nral artifacts. We demonstrate our approach across various\nbenchmark scenarios in deepfake detection, showing its su-\nperiority in cross-dataset and cross-manipulation scenar-\nios. Through further analysis, we also validate the impor-\ntance of using temporal changes of style latent vectors to\nimprove the generality of deepfake video detection.\n1. Introduction\nRecent generative algorithms are capable of producing\nhigh-quality videos; while this advancement has led to so-\ncial concerns, as it becomes increasingly challenging to dis-\ntinguish between generated videos and authentic ones. Gen-\nerative models have the potential to expedite industries such\nas entertainment, gaming, fashion, design, and education.\nHowever, their misuse can have adverse effects on society.\nThe high quality of the videos intensifies the potential for\ninappropriate utilization of the technique. To resolve the\nissue, researchers are actively engaged in developing the\ndeepfake video detection algorithms [19, 29, 40].\nEarly deepfake detection research addressed spatial ar-\ntifacts such as unnatural aspects [30] and frequency-level\ncheckerboard of the generative model [33] in the genera-\n*Corresponding author\nStyle latent level\nStyle latent variance\nDFD-real\nDFD-fake\nYoutube-real      \nCeleb-Synthesis\n1    2     3     4    5     6    7     8     9   10   11   12   13  14   15  16   17   18\n1    2     3     4   5     6   7     8     9   10   11   12   13  14   15  16   17   18\n0.4\n0.2\n0.0\n0.0\n0.5\n1.0\n1.5\nFigure 1. Variance of style flow for each style latent level. The x-\naxis shows the level of style latent vectors for fine style representa-\ntions. We noticed that the level-wise differences vary across deep-\nfake domains, but the variance of style latent vectors is particularly\nlower in certain levels of the style latent vectors for fake videos\nthan in real videos. This happens due to the temporal smooth-\nness of the style latent vectors to create temporally stable deepfake\nvideos, and our results demonstrate that deepfake videos have a\ndistinct variance in style flow compared to real videos.\ntion of a single frame. While spatial artifact-based detec-\ntion methods have shown reasonable performance for sin-\ngle images, they failed in accounting for temporal artifacts\nin deepfake videos that consist of multiple frames. To ad-\ndress the limitation, recent studies [15, 41] integrate tempo-\nral cues such as flickering [14, 57] and discontinuity [16] to\nenhance the accuracy of deepfake video detection task.\nNevertheless, existing methods [28, 51] that exploit\nvisual and temporal cues of generated videos encoun-\ntered performance degradation when attempting to identify\nvideos produced by recent deepfake generation algorithms,\nwhich effectively suppress visual and temporal artifacts.\nRecent observations [36, 39] have presented a decline in\nthe effectiveness of visual artifact-based algorithms against\nnewly developed high-quality generation techniques [11,\n37]. Temporal artifact-based algorithms [16] also experi-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1133",
        "chosen": "This paper proposes a new approach for fake video detection by analyzing style latent vectors and their temporal changes. The framework utilizes a StyleGRU module and a style attention module to detect visual and temporal artifacts, demonstrating superiority in cross-dataset and cross-manipulation scenarios.",
        "rejected": "A new approach for fake video detection is introduced, focusing on the analysis of style latent vectors' abnormal behavior in temporal changes. Utilizing StyleGRU and style attention modules, this method shows superiority in cross-dataset and cross-manipulation scenarios."
    },
    {
        "prompt": "summary: Advancements in neural signed distance fields (SDFs)\nhave enabled modeling 3D surface geometry from a set of\n2D images of real-world scenes. Baking neural SDFs can\nextract explicit mesh with appearance baked into texture\nmaps as neural features. The baked meshes still have a\nlarge memory footprint and require a powerful GPU for\nreal-time rendering.\nNeural optimization of such large\nmeshes with differentiable rendering pose significant chal-\nlenges. We propose a method to produce optimized meshes\nfor large unbounded scenes with low triangle budget and\nhigh fidelity of geometry and appearance.\nWe achieve\nthis by combining advancements in baking neural SDFs\nwith classical mesh simplification techniques and proposing\na joint appearance-geometry refinement step. The visual\nquality is comparable to or better than state-of-the-art neu-\nral meshing and baking methods with high geometric ac-\ncuracy despite significant reduction in triangle count, mak-\ning the produced meshes efficient for storage, transmission,\nand rendering on mobile hardware. We validate the effec-\ntiveness of the proposed method on large unbounded scenes\nfrom mip-NeRF 360, Tanks & Temples, and Deep Blend-\ning datasets, achieving at-par rendering quality with 73×\nreduced triangles and 11× reduction in memory footprint.\n1. Introduction\nPhotorealistic reconstruction and rendering of real-world\nobjects and scenes is a longstanding problem of importance\nwith several applications in computer vision, robotics and\nAR/VR. In recent years, Neural Radiance Fields (NeRFs)\n[1, 2, 38, 39] have been quite successful at novel view syn-\nthesis. However, the scene representation is learned as an\nimplicit volumetric representation that is expensive to eval-\nuate. This is a crucial limitation for many applications, such\nas Augmented and Virtual Reality (AR/VR), often requiring\nreal-time rendering with low memory and compute power.\nFigure 1. Example results using LTM showing mesh geometry\n(top-left) and novel-view synthesis (right). Bottom right plot com-\npares performance of different methods on mip-NeRF360 outdoor\ndataset.\nOur method achieves comparable rendering quality to\nBakedSDF while reducing the mesh size considerably.\nSince modern graphics engines are highly optimized to ras-\nterize triangle meshes, researchers have started exploring\nways to extract explicit mesh representations from volumet-\nric functions.\nMobileNeRF [5] uses a polygon mesh with texture maps\nstoring feature vectors and opacity.\nAlthough their ren-\ndering quality is good, the mesh quality is far from an\nideal surface.\nWhile NeRF2Mesh [50] and NeRFMesh-\ning [44], based on volumetric density fields, excel at gen-\nerating highly detailed geometry, they often result in noisy\nand bumpy surfaces and numerous floaters, especially in\nplanar regions, which are common in large-scale scenes.\nIn contrast, neural Signed Distance Function (SDF) meth-\nods [42, 52, 54] typically excel in reconstructing accurate\nsurface geometry. However, they may not be able to capture\nsmall or detailed geometry, and their rendering quality is in-\nferior to volumetric density-field methods. BakedSDF [55]\novercomes these limitations by applying mip-NeRF 360\ntechniques to train SDF representation, though the result-\ning mesh representation has a high memory overhead.\nPrevious neural SDF methods that rely on the March-\ning Cube algorithm [34] result in meshes with high storage\noverhead. This results in a significant challenge for practi-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n5053",
        "chosen": "This paper presents LTM, a method for producing optimized meshes for large unbounded scenes with low triangle budget and high fidelity. The method combines baking neural SDFs with classical mesh simplification techniques, achieving significant reduction in triangle count and memory footprint while maintaining visual quality.",
        "rejected": "The research proposes a method for optimizing large unbounded scene meshes derived from neural SDFs with classical mesh simplification techniques, achieving high fidelity in geometry and appearance while significantly reducing triangle count for efficient storage and transmission."
    },
    {
        "prompt": "summary: Omnidirectional cameras are extensively used in various\napplications to provide a wide field of vision.\nHowever,\nthey face a challenge in synthesizing novel views due to\nthe inevitable presence of dynamic objects, including the\nphotographer, in their wide field of view.\nIn this paper,\nwe introduce a new approach called Omnidirectional Lo-\ncal Radiance Fields (OmniLocalRF) that can render static-\nonly scene views, removing and inpainting dynamic ob-\njects simultaneously. Our approach combines the princi-\nples of local radiance fields with the bidirectional optimiza-\ntion of omnidirectional rays. Our input is an omnidirec-\ntional video, and we evaluate the mutual observations of\nthe entire angle between the previous and current frames.\nTo reduce ghosting artifacts of dynamic objects and inpaint\nocclusions, we devise a multi-resolution motion mask pre-\ndiction module. Unlike existing methods that primarily sep-\narate dynamic components through the temporal domain,\nour method uses multi-resolution neural feature planes for\nprecise segmentation, which is more suitable for long 360◦\nvideos. Our experiments validate that OmniLocalRF out-\nperforms existing methods in both qualitative and quanti-\ntative metrics, especially in scenarios with complex real-\nworld scenes. In particular, our approach eliminates the\nneed for manual interaction, such as drawing motion masks\nby hand and additional pose estimation, making it a highly\neffective and efficient solution.\n1. Introduction\nOmnidirectional cameras such as Ricoh Theta or Insta360\nallow capturing panoramic 360◦views in a single shot.\nVarious applications with omnidirectional images such as\nspherical depth estimation [53, 58, 59], novel view syn-\nthesis [2, 3, 5–7, 11, 18, 31, 35] and geometry reconstruc-\ntion [3, 18] aiming at large-scale static scenes have recently\nbeen explored. In particular, synthesizing 360◦novel views\ncan provide continuous views from unobserved camera an-\ngles while maintaining its details.\nHowever, recent novel view synthesis methods struggle\nto apply to omnidirectional input for the following reasons.\n(b) OmniLocalRF (ours) \n(a) Conventional neural rendering\nInput 360°video\nFigure 1. We introduce omnidirectional local radiance fields for\nphotorealistic view synthesis of static scenery from 360◦videos.\nOur method effectively removes dynamic objects (including the\nphotographer) without manual interaction. Also, it achieves high-\nresolution details in the inpainted regions by means of bidirec-\ntional observations of omnidirectional local radiance fields. Refer\nto the supplemental video for more results.\nWhen capturing omnidirectional videos to record static en-\nvironments, dynamic objects are prone to be captured as\nan extension of the field of view, and capturing a photog-\nrapher is inevitable unless employing a dedicated hardware\nor remote controller. When synthesizing novel views, these\ncaptured objects are represented as ghosting artifacts onto\nthe rendered results [37]. Despite these problems, existing\nmethods have achieved 360◦view synthesis, relying on con-\nstrained capturing conditions, where it minimizes the ad-\nvent of dynamic objects [3, 11] or requiring dedicated hard-\nware [5–7, 31, 35], which are not suitable for casual 360◦\nphotography.\nLow-rank decomposition through robust principal com-\nponent analysis [15, 16, 57] and existing optimization-\nbased methods [13, 14] effectively eliminate dynamic ob-\njects on the image domain.\nHowever, their applicabil-\nity is limited to scenarios involving multiple images cap-\ntured from the same viewpoints.\nRecent view synthesis\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n6871",
        "chosen": "This paper introduces OmniLocalRF, a method for rendering static-only scene views from omnidirectional videos by removing and inpainting dynamic objects. The approach combines local radiance fields with bidirectional optimization of omnidirectional rays, outperforming existing methods in complex real-world scenes.",
        "rejected": "This paper introduces a novel framework for spatio-temporal action grounding trained solely on video and subtitle supervision, eliminating the need for human annotations. The method combines local representation learning to capture fine-grained spatial information with global representation encoding for higher-level representations. A new benchmark dataset is proposed, featuring dense spatio-temporal grounding annotations in long, untrimmed multi-action instructional videos. Evaluations demonstrate that the proposed approach outperforms current baselines in various settings, including spatial and temporal localization tasks."
    },
    {
        "prompt": "summary: Video Transformers have become the prevalent solution\nfor various video downstream tasks with superior expressive\npower and flexibility. However, these video transformers\nsuffer from heavy computational costs induced by the mas-\nsive number of tokens across the entire video frames, which\nhas been the major barrier to train and deploy the model.\nFurther, the patches irrelevant to the main contents, e.g.,\nbackgrounds, degrade the generalization performance of\nmodels. To tackle these issues, we propose training-free to-\nken merging for lightweight video Transformer (vid-TLDR)\nthat aims to enhance the efficiency of video Transformers by\nmerging the background tokens without additional training.\nFor vid-TLDR, we introduce a novel approach to capture the\nsalient regions in videos only with the attention map. Fur-\nther, we introduce the saliency-aware token merging strat-\n*: Equal contribution, †: Corresponding author.\negy by dropping the background tokens and sharpening the\nobject scores. Our experiments show that vid-TLDR sig-\nnificantly mitigates the computational complexity of video\nTransformers while achieving competitive performance com-\npared to the base model without vid-TLDR. Code is available\nat https://github.com/mlvlab/vid-TLDR.\n1. Introduction\nWith the success of Transformers in computer vision, e.g.,\nclassification [14, 52], object detection [10, 32, 43, 61, 75,\n77], segmentation [59, 64], a line of works [16, 33, 51, 57,\n60, 76] have proposed video Transformers to comprehend\nthe video for various downstream tasks. The attention mech-\nanism in Transformers shows the desirable characteristics for\nvideo understanding such as the ability to capture the spatial\nand temporal dependencies at the same time. Consequently,\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n18771",
        "chosen": "This paper proposes vid-TLDR, a training-free token merging method for lightweight video Transformers. The method enhances efficiency by merging background tokens using a saliency-aware strategy, significantly mitigating computational complexity while maintaining competitive performance.",
        "rejected": "AMUSE is presented as an emotional speech-driven body animation model based on latent diffusion. It addresses the limitation of existing methods by explicitly modeling the impact of emotions on generated gestures. AMUSE maps audio input to three disentangled latent vectors representing content, emotion, and personal style. These vectors condition a latent diffusion model trained to generate gesture motion sequences. AMUSE synthesizes realistic 3D human gestures from speech while allowing control over expressed emotions and style. Comprehensive evaluations show that AMUSE produces more synchronized and emotionally expressive gestures compared to state-of-the-art methods."
    },
    {
        "prompt": "summary: Utilizing multi-view inputs to synthesize novel-view im-\nages, Neural Radiance Fields (NeRF) have emerged as a\npopular research topic in 3D vision. In this work, we in-\ntroduce a Generalizable Semantic Neural Radiance Fields\n(GSNeRF), which uniquely takes image semantics into the\nsynthesis process so that both novel view image and the as-\nsociated semantic maps can be produced for unseen scenes.\nOur GSNeRF is composed of two stages: Semantic Geo-\nReasoning and Depth-Guided Visual rendering. The former\nis able to observe multi-view image inputs to extract se-\nmantic and geometry features from a scene. Guided by the\nresulting image geometry information, the latter performs\nboth image and semantic rendering with improved perfor-\nmances. Our experiments not only confirm that GSNeRF\nperforms favorably against prior works on both novel-view\nimage and semantic segmentation synthesis but the effec-\ntiveness of our sampling strategy for visual rendering is fur-\nther verified.\n1. Introduction\n3D scene understanding plays a pivotal role in many vision-\nrelated tasks, including 3D reconstruction and 3D reason-\ning. The former domain focuses on low-level understand-\ning, including developing efficient models that interpret\nand reconstruct observations from various data sources,\nsuch as RGB, RGBD, and physical sensors. In contrast,\nthe latter emphasizes high-level understanding, like encod-\ning, segmentation, and common sense knowledge learning.\nFor real-world applications like robotic navigation or aug-\nmented reality, both these facets - reconstruction and rea-\nsoning - are crucial to enhance interactions with the real\nworld [10].\nAmong the challenges in the reconstruction\ndomain, novel view synthesis has consistently been a chal-\nlenging objective. Being able to generate a novel view im-\nage based on a few existing views shows that a model com-\n*Equal Contribution\nprehends the scene’s geometry and possesses a foundational\nunderstanding akin to reconstruction.\nThe Neural Radiance Field (NeRF) [1, 4, 8, 19–22, 28,\n35] has recently risen as an exciting research area, provid-\ning a novel way to tackle the task of novel view synthesis.\nBy encoding the density and emitted radiance at each spatial\nlocation, NeRF is able to compress a scene into a learnable\nmodel given several images and the corresponding camera\nposes of the scene. By incorporating a volumetric rendering\nskill, images of unseen camera views can be generated with\nconvincing quality. However, NeRF primarily focuses on\nreconstructing the color information of novel views and un-\nderstanding the associated high-level semantic information\n(e.g., semantic segmentation or object detection) remains a\nsignificant challenge.\nRecent works [9, 15, 25, 30, 31, 37] aim to amplify the\nhigh-level scene understanding ability of NeRF by integrat-\ning NeRF with semantic segmentation. By sharing the in-\nformation between the semantic object class and their cor-\nresponding appearances, these two tasks are capable of ben-\nefiting from each other’s insights [17]. Nevertheless, these\nsemantic understanding methods stick closely to the orig-\ninal NeRF paradigm, focusing on exploiting the model to\nrepresent a specific scene. As mentioned in [17], such a\nstrategy requires additional annotations of semantic seg-\nmentation maps when applied to a new scene, limiting real-\nworld applicability and generalizability.\nTo tackle this problem, generalizable NeRFs [3, 14, 18,\n27, 32, 33, 36] have emerged as a promising solution. They\nadopt an on-the-fly approach for building a neural radiance\nfield conditioned on extracted features from input images of\ndifferent scenes rather than encoding the scene representa-\ntion directly into the model. For instance, PixelNeRF [36]\nintroduces the idea of conditioning a NeRF model with\nmulti-view images across multiple scenes. This innovation\nboosts NeRF’s ability to generalize to unseen scenes and\navoids retraining for each individual scene. Yet, it leaves\nuncertainties in its application to semantic understanding.\nAs a pioneer in integrating a generalizable NeRF\nwith semantic segmentation capabilities, Semantic-Ray (S-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n20806",
        "chosen": "This paper introduces GSNeRF, a generalizable semantic neural radiance field that produces novel-view images and associated semantic maps for unseen scenes. GSNeRF consists of Semantic Geo-Reasoning and Depth-Guided Visual Rendering stages. Experiments confirm GSNeRF's effectiveness in novel-view synthesis and semantic segmentation.",
        "rejected": "Generalizable Semantic Neural Radiance Fields (GSNeRF) integrates semantic information into novel view synthesis, enabling both image and semantic map generation. It uses a two-stage process: Semantic Geo-Reasoning extracts features from multi-view images, while Depth-Guided Visual rendering generates improved images and semantic renderings. GSNeRF demonstrates superior performance in novel view synthesis and semantic segmentation tasks compared to prior methods."
    },
    {
        "prompt": "summary: Deep Neural Networks (DNNs) are widely used for vi-\nsual classiﬁcation tasks, but their complex computation pro-\ncess and black-box nature hinder decision transparency and\ninterpretability. Class activation maps (CAMs) and recent\nvariants provide ways to visually explain the DNN decision-\nmaking process by displaying ‘attention’ heatmaps of the\nDNNs. Nevertheless, the CAM explanation only offers rela-\ntive attention information, that is, on an attention heatmap,\nwe can interpret which image region is more or less im-\nportant than the others.\nHowever, these regions cannot\nbe meaningfully compared across classes, and the contri-\nbution of each region to the model’s class prediction is\nnot revealed. To address these challenges that ultimately\nlead to better DNN Interpretation, in this paper, we pro-\npose CAPE, a novel reformulation of CAM that provides a\nuniﬁed and probabilistically meaningful assessment of the\ncontributions of image regions. We quantitatively and qual-\nitatively compare CAPE with state-of-the-art CAM methods\non CUB and ImageNet benchmark datasets to demonstrate\nenhanced interpretability. We also test on a cytology imag-\ning dataset depicting a challenging Chronic Myelomono-\ncytic Leukemia (CMML) diagnosis problem. Code is avail-\nable at: https://github.com/AIML-MED/CAPE.\n1. Introduction\nDeep neural networks (DNNs), despite achieving supe-\nrior performance on various tasks such as computer vision\nand natural language processing, are known to be black\nboxes [23] that lack the ability to explain their decision-\nmaking process.\nThe black-box nature is commonly re-\ngarded as a result of the complex model structure charac-\nterized by stacked computation layers, involving non-linear\nfunctions and many model parameters. Explainable DNN\ndecisions are crucial to many life-critical scenarios [26]\n†Corresponding author.\nsuch as AI-powered autonomous driving and medical diag-\nnostics. Taking the example of healthcare applications [2],\ndecision transparency is critical for doctors to understand\nand trust AI analysis, and to use AI to make insightful and\naccurate diagnoses or decisions.\nDNN interpretability is an emerging and actively stud-\nied research ﬁeld. For visual classiﬁcation tasks, a com-\nmon type of DNN interpretability analysis is to explain\nDNN outputs via ﬁnding and displaying model attention\non the input image, i.e., identifying which image regions\nthe model focused on during the decision-making process.\nThis type of visual explanation can be achieved via methods\nof gradient-based attention visualization [25], perturbation-\nbased input manipulation [6, 21], and class activation map\n(CAM)-based visualization [11, 24]. In particular, CAM\nis an inherent intermediate step of DNN prediction which\nrepresents the actual region relevance produced by the net-\nwork. CAM stands out due to its efﬁcient feedforward pro-\ncess, yet its attention values can not directly explain and\ncompose model outcomes. Speciﬁcally, CAM values are\nclass-wise relative probabilities. They only represent the\nrelative region importance compared to the highest attention\nvalue within each class map. Thus, CAM values provide a\nlimited explanation within the context of one target class.\nThis means that they are incomparable between classes, and\ncannot explain image-level predictions. Take the CAM vi-\nsualization in Fig. 1 as an example, CAM assigns similar\nattention values to two dog breed classes Siberian Husky\nand Alaskan Malamute. Differencing the two CAM maps\nbetween the breeds fails to yield meaningful comparisons.\nThe limited analytical capability of current CAM-based\napproaches hinders their use in many downstream applica-\ntions. For example, ﬁne-grained classiﬁcation analysis re-\nquires the model’s ability to discriminate regions between\nclosely related concepts.\nIn addition, for tasks such as\nweakly supervised semantic segmentation, CAM threshold-\ning is employed to initialize a segmentation method [13] but\nthe threshold choice is often arbitrary without any semantic\nmeaning attached.\nIn this paper, we reformulate CAM as a Probabilistic\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n11072",
        "chosen": "This paper proposes CAPE, a reformulation of class activation maps (CAM) that provides a unified and probabilistically meaningful assessment of image region contributions. CAPE enhances DNN interpretation by revealing region contributions to class predictions, demonstrated on benchmark datasets and a medical imaging task.",
        "rejected": "CAPE reformulates Class Activation Maps (CAM) to offer a unified and probabilistically meaningful assessment of image region contributions. Unlike traditional CAMs that provide relative attention values, CAPE enables cross-class comparisons and reveals each region's contribution to class prediction. Experiments on CUB, ImageNet, and a cytology dataset demonstrate enhanced interpretability and diagnostic accuracy, making DNN decisions more transparent."
    },
    {
        "prompt": "summary: Music is a universal language that can communicate\nemotions and feelings.\nIt forms an essential part of the\nwhole spectrum of creative media, ranging from movies to\nsocial media posts. Machine learning models that can syn-\nthesize music are predominantly conditioned on textual de-\nscriptions of it. Inspired by how musicians compose music\nnot just from a movie script, but also through visualizations,\nwe propose MELFUSION, a model that can effectively use\ncues from a textual description and the corresponding im-\nage to synthesize music. MELFUSION is a text-to-music\ndiffusion model with a novel “visual synapse”, which effec-\ntively infuses the semantics from the visual modality into the\ngenerated music. To facilitate research in this area, we in-\ntroduce a new dataset MeLBench, and propose a new eval-\nuation metric IMSM. Our exhaustive experimental evalua-\ntion suggests that adding visual information to the music\nsynthesis pipeline significantly improves the quality of gen-\nerated music, measured both objectively and subjectively,\nwith a relative gain of up to 67.98% on the FAD score. We\nhope that our work will gather attention to this pragmatic,\nyet relatively under-explored research area.\n1. Introduction\nMusic is an essential tool for creative professionals and con-\ntent creators. It can complement and set the mood for an\naccompanying still image, animation, video, or even text\ndescriptions while creating a social media post. Finding\nmusic that matches a specific setting, can indeed be an ar-\nduous task. A conditional music generation approach, that\ncan synthesize a music track by analyzing the visual content\nand the textual description can find a wide range of practical\n∗Equal contribution.\n†Work done during internship at Adobe Research.\nMELFUSION\n(Ours)\nLong Caption\nA tranquil scenery is captured with the view of the\nnight sky just before sunrise. The sky consists of\ndynamic spiralling clouds which symbolizes\nmovement and aliveness. The stars are bright and\nprominent with strokes of yellows and whites\nrepresent a vivid yet peaceful moment. The village\nin the scene has houses whose windows emit warm\nand glowing light, giving a contrast to the cool,\ncelestial tones of sky depicting pleasant emotions.\nOVL: 83.86\nCaption\nGenerator\nA soft musical track of folk\nacoustic genre played on violin.\nOVL: 88.45\nWe introduce a new\nproblem setting and an\napproach to synthesize\nmusic, conditioned on both\nvisual and textual\nmodalities.\nA soft musical track of folk acoustic genre played\non violin.\nAn\nAlternate\nApproach\nText-to-Music\nFigure 1.\nWe present MELFUSION, a music diffusion model\nequipped with a novel “visual synapse”, that can effectively in-\nfuse image semantics into a text-to-music diffusion model. This\ntask indeed requires a detailed understanding of the concepts in\nthe image. An alternate approach like using a caption generator to\nconvert image to text space to be further used with existing text-to-\nmusic methods leads to a sub-optimal overall audio quality (OVL)\nscore. Our approach can knit together complementary information\nfrom both modalities to synthesize high-quality music.\napplications in various fields including social media.\nInspired by the progress in generative modeling of im-\nages, music generation has also garnered significant atten-\ntion from the community [1, 30, 42]. Recently, Agostinelli\net al. [1], Copet et al. [4] proposed conditioning in the form\nof melody or humming. While Sheffer and Adi [43] pursue\nimage-guided audio generation. Despite these efforts, mu-\nsic generation conditioned on multiple modalities like text\nand image, is largely uncharted.\nImages are more expressive [13] than text-only infor-\nmation and capture more fine-grained semantic information\nabout various visual aspects. For example, as depicted in\nFig 1, to generate a musical track that goes well with a\ngiven image, without indeed using it, one has to make the\ntedious effort of producing long, descriptive captions (either\ngenerated by an image captioning model or human annota-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n26826",
        "chosen": "This paper presents MELFUSION, a text-to-music diffusion model with a \"visual synapse\" that infuses visual semantics into music synthesis. The model improves music quality using visual and textual cues, validated on a new dataset and evaluation metric.",
        "rejected": "MELFUSION is a text-to-music diffusion model incorporating visual cues for music synthesis. By integrating a \"visual synapse,\" it effectively blends textual descriptions with corresponding images to produce high-quality music. An evaluation on MeLBench shows significant improvements in generated music quality, achieving up to a 67.98% gain on the FAD score. This approach highlights the potential of multimodal inputs in creative applications."
    },
    {
        "prompt": "summary: Open-vocabulary semantic segmentation presents the\nchallenge of labeling each pixel within an image based on a\nwide range of text descriptions. In this work, we introduce a\nnovel cost-based approach to adapt vision-language foun-\ndation models, notably CLIP, for the intricate task of se-\nmantic segmentation. Through aggregating the cosine sim-\nilarity score, i.e., the cost volume between image and text\nembeddings, our method potently adapts CLIP for segment-\ning seen and unseen classes by fine-tuning its encoders, ad-\ndressing the challenges faced by existing methods in han-\ndling unseen classes. Building upon this, we explore meth-\nods to effectively aggregate the cost volume considering its\nmulti-modal nature of being established between image and\ntext embeddings. Furthermore, we examine various meth-\nods for efficiently fine-tuning CLIP.\n1. Introduction\nOpen-vocabulary semantic segmentation aims to assign\neach pixel in an image to a class label from an unbounded\nrange, defined by text descriptions.\nTo handle the chal-\nlenge of associating an image with a wide variety of text\ndescriptions, pre-trained vision-language foundation mod-\nels, e.g., CLIP [43] and ALIGN [22], have drawn attention\nas they exerted strong open-vocabulary recognition capa-\nbilities achieved through training on extensive image-text\ndatasets.\nNonetheless, these foundation models primar-\nily receive image-level supervision during training, which\nintroduces a notable disparity when applying them to the\npixel-level segmentation tasks [66].\nTo address this gap, recent works [9, 14, 30, 55–57, 60]\nhave reformulated the task into a region-level problem by\nutilizing mask proposal generators.\nWhile this partially\nbridges the discrepancy between the pre-training and the\n∗Equal contribution. †Corresponding authors.\n0\n10\n20\n30\n40\nmIoU on ADE20K\n🔥\n❄\n🔥\n❄\nFeature Agg.\nCost Agg.\n(a) mIoU of seen classes\n0\n10\n20\n30\n40\nmIoU on ADE20K\n🔥\n❄\n🔥\n❄\nFeature Agg.\nCost Agg.\n🔥: Fine-tuned CLIP\n❄: Frozen CLIP\n(b) mIoU of unseen classes\nFigure 1. Comparison between feature and cost aggregation\nfor open-vocabulary semantic segmentation task. In contrast\nto feature aggregation suffering severe overfitting to seen classes,\ncost aggregation can generalize to unseen classes and achieve sig-\nnificant performance improvements upon fine-tuning of CLIP.\ndownstream task, a discernible gap persists between the\nconceptualization of regions and the entire image for CLIP.\nIn this work, we investigate methods to transfer the holis-\ntic understanding capability of images to the pixel-level\ntask of segmentation.\nWhile a straightforward approach\nwould be to fine-tune the encoders of CLIP, existing meth-\nods struggle in such attempt [57, 60, 66] as they encounter\nsignificant overfitting problems to the seen classes. This\nresults in the misalignment of the joint embedding space\nfor unseen classes, as the CLIP features undergo decoder\nmodules for aggregating them into segmentation masks,\nhence losing their alignment. Consequently, most meth-\nods [9, 14, 30, 55–57, 60] opt for freezing the encoders of\nCLIP instead, remaining the challenge underexplored.\nIn this regard, we extend the exploration of adapting\nCLIP for open-vocabulary semantic segmentation and in-\ntroduce a novel cost-based framework. We propose to ag-\ngregate the cosine similarity between image and text em-\nbeddings of CLIP, i.e., the matching cost, drawing parallels\nto the visual correspondence literature [26]. Surprisingly,\nwe find that fine-tuning CLIP upon this framework effec-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n4113",
        "chosen": "This paper introduces a cost-based framework for open-vocabulary semantic segmentation, adapting CLIP through cost volume aggregation. The method effectively segments seen and unseen classes, with fine-tuning improving performance on unseen classes.",
        "rejected": "A cost-based approach adapts vision-language models like CLIP for open-vocabulary semantic segmentation by aggregating cosine similarity scores between image and text embeddings. Fine-tuning CLIP encoders improves handling of seen and unseen classes, addressing limitations in existing methods. Cost aggregation proves effective in generalizing to unseen classes, significantly enhancing performance metrics over feature aggregation methods."
    },
    {
        "prompt": "summary: We present a new open-vocabulary detection framework.\nOur framework uses both image-level labels and detailed\ndetection annotations when available. Our framework pro-\nceeds in three steps. We first train a language-conditioned\nobject detector on fully-supervised detection data. This de-\ntector gets to see the presence or absence of ground truth\nclasses during training, and conditions prediction on the\nset of present classes.\nWe use this detector to pseudo-\nlabel images with image-level labels. Our detector pro-\nvides much more accurate pseudo-labels than prior ap-\nproaches with its conditioning mechanism.\nFinally, we\ntrain an unconditioned open-vocabulary detector on the\npseudo-annotated images. The resulting detector, named\nDECOLA, shows strong zero-shot performance in open-\nvocabulary LVIS benchmark as well as direct zero-shot trans-\nfer benchmarks on LVIS, COCO, Object365, and OpenIm-\nages. DECOLA outperforms the prior arts by 17.1 APrare\nand 9.4 mAP on zero-shot LVIS benchmark.\nDECOLA\nachieves state-of-the-art results in various model sizes, ar-\nchitectures, and datasets by only training on open-sourced\ndata and academic-scale computing. Code is available at\nhttps://github.com/janghyuncho/DECOLA.\n1. Introduction\nObject detection has seen immense progress over the past\ndecade. Classical object detectors reason over datasets of\nfixed predefined classes. This simplifies the design, train-\ning, and evaluation of new methods, and allows for rapid\nprototyping [2–5, 17, 18, 25, 37, 42, 60, 70, 72, 75]. How-\never, it complicates deployment to downstream applica-\ntions too. A classical detector requires a new dataset to\nfurther finetune for every new concept it encounters. Col-\nlecting sufficient data for every new concept is not scal-\nable [20].\nOpen-vocabulary detection offers an alterna-\ntive [19, 43, 61, 65, 66, 73]. Open-vocabulary detectors\nreason about any arbitrary concept with free-form text,\nusing the generalization ability of vision-language mod-\nels. Yet, common open-vocabulary detectors reuse clas-\nsical detectors and either replace the last classification layer\nDECOLA\nLanguage-conditioned Detections\nImage-level tags: \n“cat”, “cola”, “mentos”\ntext encoder\nconditioned\nStandard \nOV Detector\nimage-text dataset of size N\nfiltering & argmax\nselect scores\ntraining data with pseudo-labels\nargmax\nx N\ntraining data with pseudo-labels\nimage-text dataset of size N\nx N\nx N\nx N\n(a) Standard self-training\n(b) DECOLA self-training\nFigure 1. An illustration of how standard open-vocabulary detec-\ntors and DECOLA generate pseudo-labels using image-level data.\nStandard detectors use image-level information later in the pipeline\nafter initial box proposals, which may result in low coverage of\nunseen classes (e.g., “mentos” and “cola”). DECOLA adjusts the\nprediction to the information and ensures sufficient coverage.\nwith [19, 43, 66, 73], or fuse box feature with [31, 65] text\nrepresentation from pretrained vision-language model. The\ninner workings of the detector remain unchanged.\nIn this paper, we introduce a transformer-based object\ndetector that adjusts its inner workings to any arbitrary set\nof concepts represented in language. The detector considers\nonly the queried set of concepts as foreground and disre-\ngards any other objects as background. It learns to adapt\ndetection to the language embedding of queried concepts at\nrun-time. Specifically, the detector conditions proposal gen-\neration with respect to the text embedding of each queried\nconcept and refines the conditioned proposals into predic-\ntions. Our detection transformer conditioned on language\n(DECOLA) offers a powerful alternative to classical architec-\ntures in open-vocabulary detection. Adapting the detector to\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n16593",
        "chosen": "This paper presents DECOLA, an open-vocabulary detection framework using language-conditioned object detectors. The framework generates accurate pseudo-labels and trains an unconditioned detector, showing strong zero-shot performance on multiple benchmarks.",
        "rejected": "DECOLA, a language-conditioned object detector, leverages image-level labels and detection annotations to generate accurate pseudo-labels for training. It outperforms previous methods in zero-shot detection benchmarks on LVIS, COCO, Object365, and OpenImages datasets. DECOLA achieves state-of-the-art results across various model sizes and architectures, demonstrating strong generalization capabilities in open-vocabulary settings."
    },
    {
        "prompt": "summary: While GAN-based models have been successful in image\nstylization tasks, they often struggle with structure preser-\nvation while stylizing a wide range of input images. Re-\ncently, diffusion models have been adopted for image styl-\nization but still lack the capability to maintain the original\nquality of input images. Building on this, we propose OS-\nASIS: a novel one-shot stylization method that is robust in\nstructure preservation. We show that OSASIS is able to ef-\nfectively disentangle the semantics from the structure of an\nimage, allowing it to control the level of content and style\nimplemented to a given input. We apply OSASIS to var-\nious experimental settings, including stylization with out-\nof-domain reference images and stylization with text-driven\nmanipulation. Results show that OSASIS outperforms other\nstylization methods, especially for input images that were\nrarely encountered during training, providing a promising\nsolution to stylization via diffusion models. The source code\ncan be found at https://github.com/hansam95/OSASIS.\n*Work done during an internship at NAVER Cloud.\n†Corresponding author.\n1. Introduction\nIn the literature of generative models, image stylization\nrefers to training a model in order to transfer the style of\na reference image to various input images during infer-\nence [21, 22, 28]. However, collecting a sufficient num-\nber of images that share a particular style for training\ncan be difficult.\nConsequently, one-shot stylization has\nemerged as a viable and practical solution, with genera-\ntive adversarial networks (GANs) showing promising re-\nsults [2, 16, 36, 38, 39].\nDespite significant advancements in GAN-based styliza-\ntion techniques, the accurate preservation of an input im-\nage’s structure continues to pose a significant challenge.\nThis difficulty is particularly pronounced for input images\nthat contain elements infrequently encountered during train-\ning, often characterized by complex structural nuances that\ndiverge from those observed in more commonly presented\nimages. Figure 1(a) illustrates this challenge, where entities\nsuch as hands and microphones, when processed through\nGAN-based stylization, diverge considerably from their\noriginal structural integrity. In addition, GAN-based styl-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8302",
        "chosen": "This paper proposes OSASIS, a one-shot stylization method using diffusion models. OSASIS disentangles image semantics and structure, offering control over content and style levels. It outperforms other methods, especially on rarely encountered inputs.",
        "rejected": "OSASIS introduces a one-shot stylization method using diffusion models, focusing on robust structure preservation. It disentangles semantics from structure, allowing controlled content and style implementation. OSASIS excels particularly in handling rare input images, offering promising solutions for stylization via diffusion models. The approach surpasses other stylization methods in preserving structural integrity."
    },
    {
        "prompt": "summary: Video Frame Interpolation (VFI), which aims at gener-\nating high-frame-rate videos from low-frame-rate inputs, is\na highly challenging task. The emergence of bio-inspired\nsensors known as event cameras, which boast microsecond-\nlevel temporal resolution, has ushered in a transformative\nera for VFI. Nonetheless, the application of event-based\nVFI techniques in domains with distinct environments from\nthe training data can be problematic. This is mainly be-\ncause event camera data distribution can undergo substan-\ntial variations based on camera settings and scene condi-\ntions, presenting challenges for effective adaptation. In this\npaper, we propose a test-time adaptation method for event-\nbased VFI to address the gap between the source and target\ndomains. Our approach enables sequential learning in an\nonline manner on the target domain, which only provides\nlow-frame-rate videos. We present an approach that lever-\nages confident pixels as pseudo ground-truths, enabling\nstable and accurate online learning from low-frame-rate\nvideos. Furthermore, to prevent overfitting during the con-\ntinuous online process where the same scene is encountered\nrepeatedly, we propose a method of blending historical sam-\nples with current scenes. Extensive experiments validate the\neffectiveness of our method, both in cross-domain and con-\ntinuous domain shifting setups. The code is available at\nhttps://github.com/Chohoonhee/TTA-EVF.\n1. Introduction\nVideo Frame Interpolation (VFI) is a well-established prob-\nlem in the field of computer vision, aiming to enhance\nthe temporal resolution of videos. In recent times, deep\nlearning-based VFI approaches [2, 3, 15, 20–22, 33, 35, 37,\n38, 50, 60] have achieved remarkable performances across\nvarious benchmark datasets. However, accurate motion es-\ntimation becomes challenging in scenes with complex and\nnon-linear motions due to the lack of information, often re-\nsulting in the generation of erroneous inter frames.\nTime\nPre-trained\nTTA-EVF\nFrame 297\nFrame 441\nFrame 596\nNo Adaptation\nTest-Time Adaptation\nFrame 297\nFrame 297\nFrame 441\nFrame 596\nFigure 1. TTA-EVF efficiently produces high-quality results by\nadapting the network to the target domain in an online manner, al-\nleviating the need for offline data supply and addressing the perfor-\nmance degradation observed when applying a well-trained event-\nbased VFI network to a domain with a different distribution.\nTo address the inherent lack of intermediate information\nin frame cameras, recent researches [16, 24, 51, 52, 56] have\nexplored the potential of event cameras as a promising so-\nlution for VFI, especially in scenarios involving complex\nmotion. Event cameras sense dynamic changes in pixel in-\ntensity, triggering an event when the change surpasses a pre-\ndefined threshold. The incorporation of event cameras as\nadditional devices in VFI mitigates the challenges associ-\nated with modeling complex motion.\nTypically,\nframe-based VFI methods [17, 33, 38]\nare evaluated on test datasets (e.g., SNU-FILM [12],\nX4K1000FPS [43]) with distributions different from their\ntraining dataset (e.g., Vimeo90K [61]). Frame-based VFI\ndemonstrates a certain level of generalization ability even\nwithout additional modules, providing evidence that it can\nbe applied to real scenario applications, such as dealing with\nchanges in camera devices. These device and environment\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25701",
        "chosen": "This paper proposes a test-time adaptation method for event-based video frame interpolation. The approach uses confident pixels as pseudo ground-truths for stable online learning, blending historical samples to prevent overfitting. Experiments validate its effectiveness in cross-domain and continuous domain shifting setups.",
        "rejected": "TTA-EVF proposes a test-time adaptation method for event-based Video Frame Interpolation (VFI). It leverages confident pixels as pseudo ground-truths for online learning from low-frame-rate videos, blending historical samples to prevent overfitting. Extensive experiments validate its effectiveness in cross-domain and continuous domain shifting setups, enhancing VFI performance without offline data supply."
    },
    {
        "prompt": "summary: Each photo in an image burst can be considered a sam-\nple of a complex 3D scene: the product of parallax, diffuse\nand specular materials, scene motion, and illuminant vari-\nation. While decomposing all of these effects from a stack\nof misaligned images is a highly ill-conditioned task, the\nconventional align-and-merge burst pipeline takes the other\nextreme: blending them into a single image. In this work,\nwe propose a versatile intermediate representation: a two-\nlayer alpha-composited image plus flow model constructed\nwith neural spline fields – networks trained to map input\ncoordinates to spline control points. Our method is able\nto, during test-time optimization, jointly fuse a burst image\ncapture into one high-resolution reconstruction and decom-\npose it into transmission and obstruction layers. Then, by\ndiscarding the obstruction layer, we can perform a range\nof tasks including seeing through occlusions, reflection sup-\npression, and shadow removal. Tested on complex in-the-\nwild captures we find that, with no post-processing steps\nor learned priors, our generalizable model is able to out-\nperform existing dedicated single-image and multi-view ob-\nstruction removal approaches.\n1. Introduction\nOver the last decade, as digital photos have increasingly\nbeen produced by smartphones, smartphone photos have in-\ncreasingly been produced by burst fusion. To compensate\nfor less-than-ideal camera hardware – typically restricted\nto a footprint of less than 1cm3 [6] – smartphones rely on\ntheir advanced compute hardware to process and fuse mul-\ntiple lower-quality images into a high-fidelity photo [10].\nThis proves particularly important in low-light and high-\ndynamic-range settings [22,39], where a single image must\ncompromise between noise and motion blur, but multi-\nple images afford the opportunity to minimize both [26].\nBut even as mobile night- and astro-photography applica-\ntions [16, 17] use increasingly long sequences of photos\nas input, their output remains a static single-plane image.\nGiven the typically non-static and non-planar nature of the\nreal world, a core problem in burst image pipelines is thus\nInput Scene\nBurst Capture\nNeural Spline Field\nLayer Separation\nRecovered Obstruction + Transmission\nFigure 1. Fitting our two-layer neural spline field model to a stack\nof images we’re able to directly estimate and separate even severe,\nout-of-focus obstructions to recover hidden scene content.\nthe alignment [32,45] and aggregation [5,63] of pixels into\nan image array – referred to as the align-and-merge process.\nWhile existing approaches treat pixel motion as a source\nof noise and artifacts, a parallel direction of work [9,20,69]\nattempts to extract useful parallax cues from this pixel mo-\ntion to estimate the geometry of the scene. Recent work by\nChugunov et al. [8] finds that maximizing the photometric\nconsistency of an RGB plus depth neural field model of an\nimage sequence is enough to distill dense depth estimates\nof the scene. While this method is able to jointly estimate\nhigh-quality camera motion parameters, it does not perform\nhigh-quality image reconstruction, and rather treats its im-\nage model as “a vehicle for depth optimization” [8]. In con-\ntrast, work by Nam et al. [50] proposes a neural field fit-\nting approach for multi-image fusion and layer separation\nwhich focuses on the quality of the reconstructed “canon-\nical view”. By swapping in different motion models, they\ncan separate and remove layers such as occlusions, reflec-\ntions, and moir´e patterns during image reconstruction – as\nopposed to in a separate post-processing step [19,54]. This\napproach, however, does not make use of a realistic cam-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25763",
        "chosen": "This paper introduces neural spline fields for burst image fusion and layer separation. The method constructs a two-layer alpha-composited image plus flow model, enabling tasks like occlusion removal and reflection suppression.",
        "rejected": "This work introduces a neural spline fields-based intermediate representation for burst photography, creating a two-layer alpha-composited image plus flow model. During test-time optimization, it fuses burst captures into a high-resolution reconstruction and decomposes them into transmission and obstruction layers. Discarding the obstruction layer facilitates tasks such as seeing through occlusions, reflection suppression, and shadow removal, outperforming existing single-image and multi-view approaches."
    },
    {
        "prompt": "summary: Recent advancements in Spatial Transcriptomics (ST)\ntechnology have facilitated detailed gene expression anal-\nysis within tissue contexts. However, the high costs and\nmethodological limitations of ST necessitate a more ro-\nbust predictive model. In response, this paper introduces\nTRIPLEX, a novel deep learning framework designed to\npredict spatial gene expression from Whole Slide Images\n(WSIs). TRIPLEX uniquely harnesses multi-resolution fea-\ntures, capturing cellular morphology at individual spots,\nthe local context around these spots, and the global tissue\norganization. By integrating these features through an ef-\nfective fusion strategy, TRIPLEX achieves accurate gene ex-\npression prediction. Our comprehensive benchmark study,\nconducted on three public ST datasets and supplemented\nwith Visium data from 10X Genomics, demonstrates that\nTRIPLEX outperforms current state-of-the-art models in\nMean Squared Error (MSE), Mean Absolute Error (MAE),\nand Pearson Correlation Coefficient (PCC). The model’s\npredictions align closely with ground truth gene expression\nprofiles and tumor annotations, underscoring TRIPLEX’s\npotential in advancing cancer diagnosis and treatment.\n1. Introduction\nThe emergence of large-scale Spatial Transcriptomics\n(ST) technology has facilitated the quantification of mRNA\nexpression across a multitude of genes within the spatial\ncontext of tissue samples [22]. ST technology segments\ncentimeter-scale Whole Slide Images (WSIs) into hundreds\nof thousands of small spots, each providing its gene ex-\npression profile.\nConsidering the substantial cost asso-\nciated with ST sequencing technology, coupled with the\nwidespread availability of WSIs, a pressing question is how\nto best predict spatial gene expression based on WSIs using\nrapidly evolving computer vision techniques.\nA number of studies have endeavored to address this\n*Corresponding author.\nchallenge [7, 18, 24–26]. Approaches vary, with some pre-\ndicting gene expression strictly from the tissue image con-\nfined within the spot’s boundaries [7], while others also\ntake into account spatial dependencies between spot im-\nages [18, 26], or consider similarities to reference spots\n[24, 25]. However, we have noted several limitations inher-\nent to these existing methodologies. Firstly, current meth-\nods primarily focus on spot images, neglecting the wealth of\nbiological information available in the wider image context.\nBy integrating both the specific spot and its surrounding en-\nvironment, along with the holistic view of the entire histol-\nogy image, we can access richer information, encompass-\ning varied biological contexts. Secondly, models that con-\nsider interactions between spots [18, 26] face a limitation\nin processing the embedding of all patches in a WSI simul-\ntaneously. This approach, common in handling hundreds\nto thousands of patches within a WSI, limits the scalability\nof the patch embedding model due to resource constraints.\nSuch limitations significantly impede the extraction of fine-\ngrained, rich representations from each spot, thereby affect-\ning the model’s ability to perform detailed analysis of WSIs.\nThirdly, model performance is frequently overestimated be-\ncause of inadequate validation, such as using the limited\nsize of dataset [7] sometimes without cross-validation [24]\nand training/testing with replicates from the same patient\n[18, 25, 26]. The limited size of ST datasets means that\nexclusive reliance on a single dataset for model evaluation\ncan hinder an accurate assessment of the model’s capabili-\nties, thereby emphasizing the necessity for cross-validation.\nThe issue is compounded when replicate data from the same\npatient, often featuring nearly identical image-gene expres-\nsion pairs, are used in both training and testing phases. This\ncan lead to an inflated perception of a model’s effective-\nness, as it may not accurately reflect the model’s ability to\ngeneralize to new, unseen data. Lastly, the use of disparate\ndatasets, diverse normalization methods, and varied evalua-\ntion techniques in existing research studies compounds the\nchallenge of conducting fair comparisons of the models.\nAddressing these limitations, we present TRIPLEX, an\ninnovative deep learning framework designed to leverage\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n11591",
        "chosen": "This paper presents TRIPLEX, a deep learning framework predicting spatial gene expression from whole slide images. TRIPLEX uses multi-resolution features and outperforms state-of-the-art models in prediction accuracy.",
        "rejected": "TRIPLEX introduces a deep learning framework for predicting spatial gene expression from Whole Slide Images (WSIs). By integrating multi-resolution features, it captures cellular morphology at individual spots, local context around these spots, and global tissue organization. TRIPLEX employs an effective fusion strategy to achieve accurate predictions, outperforming state-of-the-art models in MSE, MAE, and PCC on three public ST datasets and Visium data. The model's predictions closely align with ground truth gene expressions and tumor annotations, highlighting its potential for advancing cancer diagnosis and treatment."
    },
    {
        "prompt": "summary: Existing research based on deep learning has extensively\nexplored the problem of daytime image dehazing. However,\nfew studies have considered the characteristics of nighttime\nhazy scenes. There are two distinctions between nighttime\nand daytime haze. First, there may be multiple active col-\nored light sources with lower illumination intensity in night-\ntime scenes, which may cause haze, glow and noise with\nlocalized, coupled and frequency inconsistent characteris-\ntics. Second, due to the domain discrepancy between simu-\nlated and real-world data, unrealistic brightness may occur\nwhen applying a dehazing model trained on simulated data\nto real-world data. To address the above two issues, we\npropose a semi-supervised model for real-world nighttime\ndehazing. First, the spatial attention and frequency spec-\ntrum ﬁltering are implemented as a spatial-frequency do-\nmain information interaction module to handle the ﬁrst is-\nsue. Second, a pseudo-label-based retraining strategy and\na local window-based brightness loss for semi-supervised\ntraining process is designed to suppress haze and glow\nwhile achieving realistic brightness. Experiments on pub-\nlic benchmarks validate the effectiveness of the proposed\nmethod and its superiority over state-of-the-art methods.\nThe source code and Supplementary Materials are placed\nin the https://github.com/Xiaofeng-life/SFSNiD.\n1. Introduction\nNighttime and daytime images may contain hazy effects,\nwhich may cause their quality to be degraded [7, 15, 39,\n50]. Therefore, two valuable research ﬁelds are proposed,\nwhich are daytime single image dehazing (DaSID) [2, 38,\n54] and nighttime single image dehazing (NiSID) [14, 19,\n31], respectively. Compared with the daytime hazy image,\nthe imaging of the nighttime hazy image is more complex\n[28, 49]. Currently, NiSID is still a challenging problem.\nExisting research on DaSID [20, 25, 27, 34, 43, 48, 52,\n*Corresponding author\n(a) Hazy\n(b) IM-YellowHaze [26]\n(c) IM-NightHaze [26]\n(d) IM-NHR [51]\n(e) GE-UNREAL-NH [31]\n(f) Ours\nFigure 1. Visualization of real-world dehazed images, where the\n“IM-” and “GE-” denote the dehazed results obtained by training\non imaging model (IM) and game engine (GE) simulated datasets,\nrespectively.\nThe curve ﬁgure represents the pixel histogram,\nwhere the x and y coordinates represent the pixel values and cor-\nresponding numbers, respectively. The x and y coordinates of the\nbar ﬁgure represent the color channel and the corresponding aver-\nage pixel value, respectively.\n53] have achieved impressive performance. Various effec-\ntive DaSID algorithms have been proposed and veriﬁed on\nbenchmark daytime datasets [21]. However, these DaSID\nalgorithms are designed for the properties of daytime hazy\nand haze-free images, without taking into account the char-\nacteristics of nighttime hazy and haze-free images.\nCurrently, NiSID research is divided into two types,\nnamely non-deep learning-based NiSID and deep learning-\nbased NiSID. On the one hand, the prior hypotheses and\nstatistical laws are explored [50, 51]. The maximum re-\nﬂectance prior to estimate the varying ambient illumina-\ntion is proposed by [50]. The illumination estimation, color\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n2631",
        "chosen": "This paper proposes a semi-supervised model for real-world nighttime dehazing. The method includes a spatial-frequency domain information interaction module and strategies to handle nighttime haze characteristics and domain discrepancies.",
        "rejected": "A novel semi-supervised model is proposed for real-world nighttime dehazing, addressing the unique challenges of nighttime hazy scenes such as localized glow, noise, and frequency inconsistencies due to multiple active light sources. The approach includes a spatial-frequency domain information interaction module and a pseudo-label-based retraining strategy with a local window-based brightness loss to ensure realistic brightness while suppressing haze and glow. Experiments on public benchmarks demonstrate the method's effectiveness and superiority over existing techniques, emphasizing its capability to handle complex nighttime imaging conditions effectively."
    },
    {
        "prompt": "summary: Recent advancements have shown the potential of lever-\naging both point clouds and images to localize anomalies.\nNevertheless, their applicability in industrial manufactur-\ning is often constrained by significant drawbacks, such as\nthe use of memory banks, which leads to a substantial in-\ncrease in terms of memory footprint and inference times.\nWe propose a novel light and fast framework that learns\nto map features from one modality to the other on nominal\nsamples and detect anomalies by pinpointing inconsisten-\ncies between observed and mapped features. Extensive ex-\nperiments show that our approach achieves state-of-the-art\ndetection and segmentation performance in both the stan-\ndard and few-shot settings on the MVTec 3D-AD dataset\nwhile achieving faster inference and occupying less memory\nthan previous multimodal AD methods. Furthermore, we\npropose a layer pruning technique to improve memory and\ntime efficiency with a marginal sacrifice in performance.\n1. Introduction\nIndustrial Anomaly Detection (AD) aims to identify un-\nusual characteristics or defects in products, serving as a vital\ncomponent within quality inspection processes. Collecting\ndata to exemplify anomalies is challenging due to their rar-\nity and unpredictability. Therefore, most works focus on\nunsupervised approaches, i.e., algorithms trained only on\nsamples without defects, also referred to as nominal sam-\nples. Currently, most existing AD methods are geared to-\nward analyzing RGB images. However, in many industrial\nsettings, anomalies are hard to recognize effectively based\nsolely on colour images, e.g., due to varying light conditions\nconducive to false detection and surface deviations that may\nnot appear as unlikely colours. Deploying colour images\nand surface information acquired by 3D sensors can tackle\nthe above issues and substantially improve AD.\nRecently, researchers have started to explore novel av-\nenues thanks to the introduction of benchmark datasets for\n*These authors contributed equally to this work.\nFigure 1. Performance, speed and memory occupancy of Mul-\ntimodal Anomaly Detection methods. The chart reports defect\nsegmentation performance (AUPRO@30%) vs inference speed\n(Frame Rate on an NVIDIA 4090 GPU).\n3D anomaly detection, such as MVTec 3D-AD [5] and\nEyecandies [6]. Indeed, both provide RGB images along-\nside pixel-registered 3D information for all data samples,\nthereby fostering the development of new, multimodal AD\napproaches [17, 34, 39].\nUnsupervised multimodal AD\nmethods like BTF [17] and M3DM [39] rely on large mem-\nory banks of multimodal features.\nThey achieve excel-\nlent performance (AUPRO@30% metric in Fig. 1) at the\ncost of extensive memory requirements and slow inference\n(Fig. 1). In particular, M3DM outperforms BTF by leverag-\ning frozen feature extractors trained by self-supervision on\nlarge datasets, i.e., ImageNet and Shapenet, for 2D and 3D\nfeatures, respectively. Another recent multimodal method,\nAST [34], follows a teacher-student paradigm conducive to\na faster architecture (Fig. 1). Yet, AST does not exploit\nthe spatial structure of the 3D data but employs this infor-\nmation just as an additional input channel in a 2D network\narchitecture. This results in inferior performance compared\nto M3DM and BTF (Fig. 1).\nIn this paper, we propose a novel paradigm to exploit\nthe relationship between features extracted from different\nmodalities and improve multimodal AD. The core idea be-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n17234",
        "chosen": "This paper presents a multimodal industrial anomaly detection framework that maps features from one modality to another on nominal samples. The approach detects anomalies by identifying inconsistencies between observed and mapped features. Experiments show state-of-the-art performance on the MVTec 3D-AD dataset with faster inference and lower memory usage than previous methods.",
        "rejected": "Recent research proposes a lightweight framework for anomaly detection in industrial settings that maps features between point clouds and images, detecting inconsistencies to identify anomalies. This method outperforms existing techniques in accuracy and speed on the MVTec 3D-AD dataset while using less memory."
    },
    {
        "prompt": "summary: Lifelong Person Re-identiﬁcation (L-ReID) aims to learn\nfrom sequentially collected data to match a person across\ndifferent scenes. Once an L-ReID model is updated using\nnew data, all historical images in the gallery are required to\nbe re-calculated to obtain new features for testing, known as\n“re-indexing”. However, it is infeasible when raw images\nin the gallery are unavailable due to data privacy concerns,\nresulting in incompatible retrieval between the query and\nthe gallery features calculated by different models, which\ncauses signiﬁcant performance degradation.\nIn this pa-\nper, we focus on a new task called Re-indexing Free Life-\nlong Person Re-identiﬁcation (RFL-ReID), which requires\nachieving effective L-ReID without re-indexing raw images\nin the gallery. To this end, we propose a Continual Com-\npatible Representation (C2R) method, which facilitates the\nquery feature calculated by the continuously updated model\nto effectively retrieve the gallery feature calculated by the\nold model in a compatible manner. Speciﬁcally, we design\na Continual Compatible Transfer (CCT) network to con-\ntinuously transfer and consolidate the old gallery feature\ninto the new feature space. Besides, a Balanced Compati-\nble Distillation module is introduced to achieve compatibil-\nity by aligning the transferred feature space with the new\nfeature space. Finally, a Balanced Anti-forgetting Distilla-\ntion module is proposed to eliminate the accumulated for-\ngetting of old knowledge during the continual compatible\ntransfer. Extensive experiments on several benchmark L-\nReID datasets demonstrate the effectiveness of our method\nagainst state-of-the-art methods for both RFL-ReID and L-\nReID tasks. The source code of this paper is available at\nhttps://github.com/PKU-ICST-MIPL/C2R CVPR2024.\n1. Introduction\nPerson re-identiﬁcation (ReID) aims to identify the same\nperson across different areas at different times [42]. Exist-\n*Corresponding author.\nQuery Images\nQuery Features\n(a) Traditionial Lifelong ReID\nID:1\nRe-indexed\nGallery Features\nID:5\nID:1      ID:2\nOld Gallery Images\nReID w/ Re-indexing\nReID w/o Re-indexing\nɌ\nɁ\nQuery Features\n(b) Re-indexing Free Lifelong ReID\nOld Gallery Features\nRe-indexing\nNew Model\nQuery Images\nID:1      ID:2\nOld Gallery Images\nID:3\nID:5\nɁ\nɁ\nData\nPrivacy×\nNew Model\nOld Gallery Features\nFeature Replacing\nFigure 1. Comparison between (a) the traditional Lifelong Per-\nson Re-identiﬁcation (L-ReID) task and (b) the Re-indexing Free\nLifelong Person Re-identiﬁcation (RFL-ReID) task.\ning methods [26, 32, 44] have made remarkable progress\nbased on deep learning methods [37] and large-scale\ndatasets [34, 41, 43]. However, their performance is often\nlimited when training data are continuously collected from\na series of different scenarios due to the well-known catas-\ntrophic forgetting challenge [3].\nRecently, Lifelong person ReID (L-ReID) has aroused\ngreat concerns involving acquiring knowledge from stream-\ning data and performing well across all data [5, 18, 19, 38].\nIts challenge is to balance the anti-forgetting of old knowl-\nedge with the acquisition of new knowledge. To this end,\nexisting L-ReID methods usually adopt the exemplar re-\nplay [38] and the knowledge distillation [27] to preserve\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n16614",
        "chosen": "This paper introduces the Re-indexing Free Lifelong Person Re-identification (RFL-ReID) task and proposes the Continual Compatible Representation (C2R) method. C2R facilitates effective retrieval without re-indexing raw images in the gallery, using a Continual Compatible Transfer (CCT) network and Balanced Compatible Distillation module. Experiments demonstrate effectiveness on benchmark datasets.",
        "rejected": "This paper introduces Re-indexing Free Lifelong Person Re-identification (RFL-ReID), aiming to eliminate the need for re-indexing gallery images during model updates. It presents a Continual Compatible Representation (C2R) approach to maintain retrieval compatibility across model updates without accessing original images, showing improved performance on benchmark datasets."
    },
    {
        "prompt": "summary: Recent advances in instruction tuning have led to the\ndevelopment of State-of-the-Art Large Multimodal Models\n(LMMs).\nGiven the novelty of these models, the impact\nof visual adversarial attacks on LMMs has not been thor-\noughly examined. We conduct a comprehensive study of the\nrobustness of various LMMs against different adversarial\nattacks, evaluated across tasks including image classifica-\ntion, image captioning, and Visual Question Answer (VQA).\nWe find that in general LMMs are not robust to visual ad-\nversarial inputs. However, our findings suggest that context\nprovided to the model via prompts—such as questions in a\nQA pair—helps to mitigate the effects of visual adversarial\ninputs. Notably, the LMMs evaluated demonstrated remark-\nable resilience to such attacks on the ScienceQA task with\nonly an 8.10% drop in performance compared to their vi-\nsual counterparts which dropped 99.73%. We also propose\na new approach to real-world image classification which\nwe term query decomposition. By incorporating existence\nqueries into our input prompt we observe diminished at-\ntack effectiveness and improvements in image classification\naccuracy. This research highlights a previously under ex-\nplored facet of LMM robustness and sets the stage for future\nwork aimed at strengthening the resilience of multimodal\nsystems in adversarial environments.\n1. Introduction\nLarge Multi-modal Models (LMMs) have demonstrated re-\nmarkable abilities in a range of applications, from image\nclassification and Visual Question Answering (VQA) to im-\nage captioning and semantic segmentation [1, 13, 22, 23,\n28]. These models excel in generalizing to new domains\nwith data-efficient solution, a feat attributed to advance-\nments in Instruction Tuning [42].\nSuch techniques, tra-\nditionally applied to text-only models, have now been ex-\ntended to multi-modal models, opening new avenues for ef-\nficient fine-tuning with significantly less data [13, 28].\n†University of Central Florida\nLLaVA(adv): A group of people are sitting in a tub, with\none person holding a toothbrush. ✗\nQuery: Is the photo taken indoor or outdoor?\nLLaVA(adv): Outdoor. ✓\nQuery: What is this image about?\nQuery: Is there a tree in the image?\nLLaVA(adv): Yes. ✓\nQuery: Sheep have a head with two large, curved horns, with a\nwoolly coat that can vary in color..\nQuestion: What animal is in the image?\nLLaVA(adv): Sheep. ✓\nQuery: What is in the background of the image?\nLLaVA(adv): Trees. ✓\nQuery: What animal is in the image?\nLLaVA(adv): None. ✗\nLLaVA: Two sheep are standing on a ledge, looking\nover a wall.. ✓\nLLaVA: Sheep. ✓\nFigure 1. QA pairs for LLaVA [28] given an adversarial image.\n“LLaVA” and “LLaVA(adv)” refer to LLaVA’s response to the\nuser query with clean and adversarial image, respectively. For the\nreaders, there are two sheep in the scene, and the adversarial attack\nwas based on maximizing the distance between the image and the\ntext “a photo of a sheep”. In the first two QA pairs, we can see\nthat LLaVA(adv)’s answer is completely wrong. However, it can\nstill answer the following questions correctly, because they are not\npertinent to the object being attacked (sheep). Also note the con-\ntrast between the second and last QA pairs. LLaVA(adv) answers\nthe question correctly after additional context has been provided.\nThese observations help drove some of the findings in this paper.\nSource: COCO 2014 [26]\nDespite the recent advancements in LMMs, the impact\nof adversarial examples still remains under explored. Typi-\ncally adversarial examples are generated end-to-end, target-\ning the final loss of the whole model, and focusing on a sin-\ngle modality. However, in the era of combining different\npre-trained models with additional projectors or adaptors\n[8, 28, 44], it is imperative to reevaluate the effectiveness\nof these adversarial approaches. For example, let’s consider\nLLaVA [28] which uses CLIP as its visual component and\nLLAMA as text component (with some additional projector\nto bridge the gap), will an attack on one of the two compo-\nnents compromise its overall performance?\nFrom a practical perspective, given the substantial size\nof LMMs, attacking the entire model is often prohibitively\nexpensive [7], making the above question an increasingly\nimportant one to answer since traditional adversarial attacks\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n24625",
        "chosen": "This paper evaluates the robustness of large multimodal models (LMMs) against visual adversarial attacks across tasks like image classification and VQA. Findings indicate LMMs are not robust, but context via prompts mitigates effects. A query decomposition approach for image classification is proposed.",
        "rejected": "The study examines the robustness of Large Multimodal Models (LMMs) against visual adversarial attacks, finding that LMMs are generally not robust but can be mitigated with contextual prompts. The proposed query decomposition method enhances image classification accuracy under adversarial conditions."
    },
    {
        "prompt": "summary: Acquiring large-scale, well-annotated datasets is essen-\ntial for training robust scene text detectors, yet the pro-\ncess is often resource-intensive and time-consuming. While\nsome efforts have been made to explore the synthesis of\nscene text images, a notable gap remains between syn-\nthetic and authentic data. In this paper, we introduce a\nnovel method that utilizes Neural Radiance Fields (NeRF)\nto model real-world scenes and emulate the data collec-\ntion process by rendering images from diverse camera per-\nspectives, enriching the variability and realism of the syn-\nthesized data.\nA semi-supervised learning framework is\nproposed to categorize semantic regions within 3D scenes,\nensuring consistent labeling of text regions across vari-\nous viewpoints.\nOur method also models the pose, and\nview-dependent appearance of text regions, thereby offer-\ning precise control over camera poses and significantly\n*Corresponding author. E-mail: lianzhouhui@pku.edu.cn\nThis work was supported by National Natural Science Foundation of China\n(Grant No.: 62372015), Center For Chinese Font Design and Research,\nand Key Laboratory of Intelligent Press Media Technology.\nimproving the realism of text insertion and editing within\nscenes. Employing our technique on real-world scenes has\nled to the creation of a novel scene text image dataset\n(https://github.com/cuijl-ai/TextNeRF). Compared to other\nexisting benchmarks, the proposed dataset is distinctive in\nproviding not only standard annotations such as bounding\nboxes and transcriptions but also the information of 3D\npose attributes for text regions, enabling a more detailed\nevaluation of the robustness of text detection algorithms.\nThrough extensive experiments, we demonstrate the effec-\ntiveness of our proposed method in enhancing the perfor-\nmance of scene text detectors.\n1. Introduction\nThe detection of text in natural images are pivotal in ad-\nvancing numerous computer vision applications, includ-\ning industrial automation, image retrieval, robot navigation,\nand autonomous driving. Despite the remarkable progress\nin scene text detection, the inherent complexity of natural\nscenes and the diverse manifestations of text present are still\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n22272",
        "chosen": "This paper proposes TextNeRF, a method using Neural Radiance Fields to synthesize scene text images by modeling real-world scenes and rendering images from diverse perspectives. A semi-supervised learning framework ensures consistent labeling of text regions. The method improves realism of text insertion and editing within scenes.",
        "rejected": "A novel method is introduced for synthesizing scene text images using Neural Radiance Fields (NeRF), providing enriched variability and realism. The technique includes a semi-supervised learning framework for consistent labeling across viewpoints, enhancing the training of scene text detectors through a new dataset."
    },
    {
        "prompt": "summary: The lifting of a 3D structure and camera from 2D land-\nmarks is at the cornerstone of the discipline of computer\nvision.\nTraditional methods have been confined to spe-\ncific rigid objects, such as those in Perspective-n-Point\n(PnP) problems, but deep learning has expanded our ca-\npability to reconstruct a wide range of object classes (e.g.\nC3DPO [18] and PAUL [24]) with resilience to noise, oc-\nclusions, and perspective distortions. However, all these\ntechniques have been limited by the fundamental need to\nestablish correspondences across the 3D training data, sig-\nnificantly limiting their utility to applications where one\nhas an abundance of “in-correspondence” 3D data. Our\napproach harnesses the inherent permutation equivariance\nof transformers to manage varying numbers of points per\n3D data instance, withstands occlusions, and generalizes\n*Both authors advised equally.\nto unseen categories. We demonstrate state-of-the-art per-\nformance across 2D-3D lifting task benchmarks. Since our\napproach can be trained across such a broad class of struc-\ntures, we refer to it simply as a 3D Lifting Foundation Model\n(3D-LFM) – the first of its kind.\n1. Introduction\nLifting 2D landmarks from a single-view RGB image into\n3D has long posed a complex challenge in the field of com-\nputer vision because of the ill-posed nature of the problem.\nThis task is important for a range of applications from aug-\nmented reality to robotics, and requires an understanding\nof non-rigid spatial geometry and accurate object descrip-\ntions [2, 11, 25]. Historically, efforts in single-frame 2D-\n3D lifting have encountered significant hurdles: reliance on\nobject-specific models, poor scalability, and limited adapt-\nability to diverse and complex object categories. Traditional\nmethods, while advancing in specific domains like human\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n10466",
        "chosen": "This paper introduces 3D-LFM, a 3D lifting foundation model leveraging transformer permutation equivariance for 2D-3D lifting tasks. The model handles varying points per 3D data instance, withstands occlusions, and generalizes to unseen categories, showing state-of-the-art performance.",
        "rejected": "This work develops a transformer-based approach called 3D Lifting Foundation Model (3D-LFM) for lifting 2D landmarks into 3D structures, demonstrating state-of-the-art performance across various benchmarks. The model's capability to handle occlusions and unseen categories marks a significant advancement in the field."
    },
    {
        "prompt": "summary: Advanced Audio-Visual Speech Recognition (AVSR) sys-\ntems have been observed to be sensitive to missing video\nframes, performing even worse than single-modality mod-\nels. While applying the common dropout techniques to the\nvideo modality enhances robustness to missing frames, it\nsimultaneously results in a performance loss when deal-\ning with complete data input. In this study, we delve into\nthis contrasting phenomenon through the lens of modality\nbias and uncover that an excessive modality bias towards\nthe audio modality induced by dropout constitutes the fun-\ndamental cause. Next, we present the Modality Bias Hy-\npothesis (MBH) to systematically describe the relationship\nbetween the modality bias and the robustness against miss-\ning modality in multimodal systems. Building on these find-\nings, we propose a novel Multimodal Distribution Approxi-\nmation with Knowledge Distillation (MDA-KD) framework\nto reduce over-reliance on the audio modality, maintain-\ning performance and robustness simultaneously. Finally,\nto address an entirely missing modality, we adopt adapters\nto dynamically switch decision strategies.\nThe effective-\nness of our proposed approach is evaluated through com-\nprehensive experiments on the MISP2021 and MISP2022\ndatasets. Our code is available at https://github.\ncom/dalision/ModalBiasAVSR.\n1. Introduction\nAudio-Visual Speech Recognition (AVSR) is a multimodal\napplication inspired by human speech perception. It out-\nperforms single-modality models by incorporating noise-\ninvariant complementary information from visual cues, es-\npecially in noisy environments.\nDriven by increasingly\nlarge open-source datasets and models [1–4], AVSR has\nachieved significant advancements across various bench-\n*Corresponding author. This work was supported by the National Nat-\nural Science Foundation of China under Grant No. 62171427.\nmarks with a simple end-to-end design [5, 6].\nRecent research on AVSR focuses on more challenging\nreal-life scenarios. Techniques such as reinforcement learn-\ning [7] and carefully designed fusion architecture [8–10] are\nused to accommodate varying noise levels and overlapping\nspeech. Self-supervised learning [11] and automatic label-\ning techniques [12] are applied facing insufficient audio-\nvisual pairs. Meanwhile, various synchronization modules\nhave been developed for audio-visual alignment.[13–15].\nHowever, restricted to the open-source datasets [1, 2, 16],\nmost studies often assume that each video is recorded in\nrelatively high quality, without blurring, corruption, or loss.\nMoreover, there is growing evidence to suggest that current\nadvanced AVSR systems are highly susceptible to pertur-\nbations in video modality [17, 18], resulting in significant\nperformance degradation even perform worse than single-\nmodality models [19, 20].\nMissing video modality is a crucial and common prob-\nlem for AVSR applied in real-life scenarios [1, 17, 19, 20].\nIt arises from various causes, including losses induced by\nnetwork latency or hardware limitations, as well as errors in\nlip movement tracking due to occlusion and side-face. Most\nresearchers utilize dropout techniques 1 on video training\ndata to improve robustness against missing modalities [19–\n23]. It has been demonstrated to effectively mitigate the\nout-of-distribution (OOD) issue and alleviate performance\ndegradation without additional inference consumption or\ncomplex modules. However, it leads to new challenges on\nreal-life scenarios with low-quality input. In our early ex-\nperiments on MISP datasets [24, 25], a contradictory phe-\nnomenon could be observed in Figure 1: while applying the\ndropout strategy to video training data enhance the robust-\nness against missing video modality, it also leads to perfor-\nmance degradation when dealing with complete data input.\n1Distinguished from the classic dropout that randomly deactivates\nnodes during neural network training, dropout in this paper specifically\nrefers to a data augmentation technique that partially or entirely replaces\noriginal video frames with padding frames.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n27445",
        "chosen": "This paper studies dropout-induced modality bias in Audio-Visual Speech Recognition (AVSR) systems. It presents the Modality Bias Hypothesis (MBH) and proposes a Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on audio, maintaining performance and robustness.",
        "rejected": "The paper explores modality bias in Audio-Visual Speech Recognition (AVSR) systems, proposing a Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on audio modality, ensuring robustness against missing video frames without compromising performance on complete data."
    },
    {
        "prompt": "summary: Retrieval tasks play central roles in real-world machine\nlearning systems such as search engine, recommender sys-\ntem, and retrieval-augmented generation (RAG). Achieving\ndecent performance in these tasks often requires fine-tuning\nvarious pretrained models on specific datasets and select-\ning the best candidate, a process that can be both time and\nresource consuming. To tackle the problem, we introduce a\nnovel and efficient method, called RetMMD, that leverages\nMaximum Mean Discrepancy (MMD) and kernel methods to\nassess the transferability of pretrained models in retrieval\ntasks. RetMMD is calculated on pretrained model and tar-\nget dataset without any fine-tuning involved. Specifically,\ngiven some query, we quantify the distribution discrepancy\nbetween relevant and irrelevant document embeddings, by\nestimating the similarities within their mappings in the fine-\ntuned embedding space through kernel method. This discrep-\nancy is averaged over multiple queries, taking into account\nthe distribution characteristics of the target dataset. Ex-\nperiments suggest that the proposed metric calculated on\npretrained models closely aligns with retrieval performance\npost fine-tuning. The observation holds across a variety of\ndatasets, including image, text, and multi-modal domains,\nindicating the potential of using MMD and kernel meth-\nods for transfer learning evaluation in retrieval scenarios.\nIn addition, we also design a way of evaluating dataset\ntransferability for retrieval tasks, with experimental results\ndemonstrating the effectiveness of the proposed approach.\n1. Introduction\nDeveloping transfer learning evaluation metrics for retrieval\ntasks is of great importance in machine learning and infor-\nmation retrieval. Reliable metrics enable assessing the effec-\ntiveness of transfer learning models in retrieval-based appli-\ncations, such as search engines, recommendation systems,\nand Retrieval Augmented Generation (RAG) with Large\nLanguage Models (LLMs) [1, 24, 39, 53]. For example, in-\n*Partial work done at Salesforce.\ntegrating retrieval mechanisms into LLMs allows them to ac-\ncess and leverage external knowledge sources, significantly\nenhancing their ability to provide accurate, up-to-date and\ncontextually relevant responses. In these contexts, the goal is\noften to retrieve the most relevant items from a large dataset\ngiven a specific query or user profile. A well-designed trans-\nfer learning evaluation metric can help assess whether the\nlearned representations from a pretrained model are effec-\ntively capturing the underlying semantics of the data and\nimproving retrieval performance. Additionally, such a met-\nric can facilitate comparison between different models or\ndifferent fine-tuning strategies, thus guiding researchers and\npractitioners in model selection and further optimization.\nExisting transfer learning evaluation metrics primarily\nfocus on classification tasks [3, 37, 40, 50, 54]. Although\nclassification and retrieval tasks are often used together and\ncan be treated as complementary tasks, there exists funda-\nmental differences between them. For instance, classification\nassigns data items to predefined categories or classes, while\nretrieval is the process of searching relevant information\nfrom a collection of items, often using similarity measures\ncalculated from their embeddings. Another distinguishing\nfactor is the asymmetric distribution of item embeddings\nin retrieval tasks. This asymmetric distribution can often\nresult in a complex and nuanced performance landscape for\nretrieval models. Furthermore, the discrepancy between dis-\ntributions of relevant and irrelevant documents in retrieval\ntasks is often dependent on the specific query. This means\nthat any transfer learning evaluation metric must take into\naccount this query-dependent variability to provide an accu-\nrate measure of model performance. In addition, one also\nneeds to take account into the importance of each examined\nquery, to be able to provide less biased predictions regarding\noverall retrieval performance in downstream tasks.\nGiven these unique challenges, we propose a method de-\nsigned specifically to assess model transferability in retrieval\ntasks. Our approach utilizes the Maximum Mean Discrep-\nancy (MMD) [15] to quantify the discrepancy between dis-\ntributions of relevant and irrelevant document embeddings\nin embedding spaces. We employ a kernel-based method to\nestimate these discrepancies, which are then averaged over\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n22390",
        "chosen": "This paper introduces RetMMD, a method using Maximum Mean Discrepancy (MMD) and kernel methods to assess pretrained model transferability in retrieval tasks. Experiments show the metric aligns with retrieval performance post fine-tuning across various datasets.",
        "rejected": "A novel metric named RetMMD is introduced for evaluating transferability of pretrained models in retrieval tasks, leveraging Maximum Mean Discrepancy (MMD) and kernel methods. Experiments demonstrate that RetMMD aligns closely with retrieval performance post fine-tuning, offering a promising evaluation approach across various domains."
    },
    {
        "prompt": "summary: It is especially challenging to achieve real-time human\nmotion tracking on a standalone VR Head-Mounted Dis-\nplay (HMD) such as Meta Quest and PICO. In this pa-\nper, we propose HMD-Poser, the first unified approach to\nrecover full-body motions using scalable sparse observa-\ntions from HMD and body-worn IMUs. In particular, it\ncan support a variety of input scenarios, such as HMD,\nHMD+2IMUs, HMD+3IMUs, etc. The scalability of in-\nputs may accommodate users’ choices for both high track-\ning accuracy and easy-to-wear.\nA lightweight temporal-\nspatial feature learning network is proposed in HMD-Poser\nto guarantee that the model runs in real-time on HMDs.\nFurthermore, HMD-Poser presents online body shape es-\ntimation to improve the position accuracy of body joints.\nExtensive experimental results on the challenging AMASS\ndataset show that HMD-Poser achieves new state-of-the-\nart results in both accuracy and real-time performance. We\nalso build a new free-dancing motion dataset to evaluate\nHMD-Poser’s on-device performance and investigate the\nperformance gap between synthetic data and real-captured\nsensor data. Finally, we demonstrate our HMD-Poser with\na real-time Avatar-driving application on a commercial\nHMD. Our code and free-dancing motion dataset are avail-\nable here.\n1. Introduction\nHuman motion tracking (HMT), which aims at estimating\nthe orientations and positions of body joints in 3D space, is\nhighly demanded in various VR applications, such as gam-\ning and social interaction. However, it is quite challeng-\ning to achieve both accurate and real-time HMT on HMDs.\nThere are two main reasons. First, since only the user’s\nhead and hands are tracked by HMD (including hand con-\ntrollers) in the typical VR setting, estimating the user’s full-\n!\"#$%&'\n!(#$%&')*+&,-\n!.#$%&')/+&,-\n .\"0\"(01 +2345  .12\"678-\n%&'$!59611$:';<-#\n .\"0\"(01  3\"6-1 ;(-16=\"5782-\n>1\"057?1$&8@10 +2A1612.1$\n82$%&'$'1=7.1\n'67=72B \"2$C=\"5\"6 82$%&' 72 >1\"0D57?1\nC$=\"67\"(01$24?(16$8A\n/';<$+&,-$!;35782\"0#\n$\n$\nFigure 1. HMD-Poser can handle scalable input scenarios, includ-\ning (a) HMD, (b) HMD+2IMUs wherein two IMUs are worn on\nthe lower legs, (c) HMD+3IMUs wherein a third IMU is added to\nthe pelvis, etc. HMD-Poser runs on HMD and outputs full-body\nmotion data to drive an Avatar in real-time.\nbody motions, especially lower-body motions, is inherently\nan under-constrained problem with such sparse tracking sig-\nnals. Second, computing resources are usually highly re-\nstricted in portable HMDs, which makes deploying a real-\ntime HMT model on HMDs even harder.\nPrior works have focused on improving the accuracy of\nfull-body tracking. One category of methods utilizes three\n6DOFs (degrees of freedom) from HMD to estimate full-\nbody motions, and they could be roughly classified into\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n874",
        "chosen": "This paper proposes HMD-Poser, a unified approach for real-time full-body motion tracking on VR HMDs using sparse observations from HMD and body-worn IMUs. The approach supports various input scenarios and includes a lightweight network and online body shape estimation.",
        "rejected": "HMD-Poser is presented as the first unified approach for real-time full-body motion tracking using sparse observations from HMD and body-worn IMUs. The lightweight temporal-spatial feature learning network ensures real-time operation on standalone VR headsets, achieving state-of-the-art results in both accuracy and performance."
    },
    {
        "prompt": "summary: Generative models have been very popular in the recent\nyears for their image generation capabilities. GAN-based\nmodels are highly regarded for their disentangled latent\nspace, which is a key feature contributing to their success\nin controlled image editing. On the other hand, diffusion\nmodels have emerged as powerful tools for generating high-\nquality images. However, the latent space of diffusion mod-\nels is not as thoroughly explored or understood. Existing\nmethods that aim to explore the latent space of diffusion\nmodels usually relies on text prompts to pinpoint specific\nsemantics. However, this approach may be restrictive in ar-\neas such as art, fashion, or specialized fields like medicine,\nwhere suitable text prompts might not be available or easy\nto conceive thus limiting the scope of existing work. In this\npaper, we propose an unsupervised method to discover la-\ntent semantics in text-to-image diffusion models without re-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n24209",
        "chosen": "This paper presents NoiseCLR, a contrastive learning approach for unsupervised discovery of interpretable semantics in the latent space of diffusion models without relying on text prompts.",
        "rejected": "This paper explores the latent space of text-to-image diffusion models, aiming to discover latent semantics without relying on restrictive text prompts. Existing methods often depend on text inputs to pinpoint specific semantics, which can be limiting in specialized fields like art, fashion, or medicine where suitable text prompts may not be readily available. The proposed unsupervised method aims to address this limitation by discovering meaningful latent variables directly from the data, enhancing the applicability and flexibility of diffusion models in generating high-quality images across various domains."
    },
    {
        "prompt": "summary: We introduce a new family of minimal problems for re-\nconstruction from multiple views. Our primary focus is a\nnovel approach to autocalibration, a long-standing prob-\nlem in computer vision.\nTraditional approaches to this\nproblem, such as those based on Kruppa’s equations or\nthe modulus constraint, rely explicitly on the knowledge of\nmultiple fundamental matrices or a projective reconstruc-\ntion. In contrast, we consider a novel formulation involv-\ning constraints on image points, the unknown depths of\n3D points, and a partially specified calibration matrix K.\nFor 2 and 3 views, we present a comprehensive taxonomy\nof minimal autocalibration problems obtained by relaxing\nsome of these constraints. These problems are organized\ninto classes according to the number of views and any as-\nsumed prior knowledge of K. Within each class, we deter-\nmine problems with the fewest—or a relatively small num-\nber of—solutions. From this zoo of problems, we devise\nthree practical solvers. Experiments with synthetic and real\ndata and interfacing our solvers with COLMAP demon-\nstrate that we achieve superior accuracy compared to state-\nof-the-art calibration methods.\nThe code is available at\ngithub.com/andreadalcin/MinimalPerspectiveAutocalibration.\n1. Introduction\nAutocalibration is the fundamental process of determin-\ning intrinsic camera parameters using only point correspon-\ndences, without external calibration objects or known scene\ngeometry [11–14, 23, 28, 33, 34, 36, 38, 47, 51].\n1.1. Contribution\nThis paper presents a comprehensive characterization of\ntwo- and three-view minimal autocalibration problems in\nthe case of a perspective camera with constant intrinsics.\nWe introduce practical and efficient solvers for minimal au-\ntocalibration by introducing a novel formulation that ex-\ntends the minimal Euclidean reconstruction problem of four\npoints in three calibrated views [24, 40] to the uncalibrated\ncase. Our approach jointly estimates camera intrinsics, en-\ncoded in the calibration matrix K, and unknown 3D point\ndepths, and seamlessly integrates any partial knowledge\nof the camera intrinsics.\nThis gives rise to a variety of\ntwo- and three-view minimal autocalibration problems, for\nwhich we provide a complete taxonomy in Tab. 1. We de-\nvelop a general theory of minimal relaxations to address\ncases where our formulation leads to an over-constrained\nproblem. These minimal relaxations of our depth formu-\nlation can be completely enumerated, and each instance of\na specific autocalibration problem can be solved offline by\napplying numerical homotopy continuation (HC) methods\nto one such relaxation. Crucially, the offline analysis with\nHC methods also enables us to identify the most efficiently\nsolvable minimal relaxations.\nOur practical contributions include implementing a nu-\nmerical solver for full camera calibration, i.e., calibration of\nall 5 unknown parameters of a perspective camera. We also\nconsider common assumptions—namely, zero-skew and\nsquare pixels—and design fast solvers for specialized prob-\nlems with a partially calibrated camera. These solvers can\nbe fast enough for many online calibration applications, and\ncan also bootstrap solutions using RANSAC-based frame-\nworks with high accuracy in offline calibration settings.\nAmong the strengths of our approach, we avoid well-known\ndegeneracies of Kruppa’s equations [48] and recover K di-\nrectly instead of relying on estimates of the dual image\nof the absolute conic (DIAC), which may not be positive-\nsemidefinite. Experiments show that our solvers outperform\nexisting autocalibration methods in terms of accuracy in\nboth synthetic and real image sequences despite increased\nruntime. Interfacing our solvers with COLMAP [44] further\nhighlights the applicability of our approach.\nC1\n<latexit sha1_base64=\"2XSB6vctFt2ibIztuHwmJKhq67w=\">ACEnicdVA9TwJBFNzDL8Qv1NJmIzGxIncGoyWRxh\nKjIAlcyLvlgRv29i670wI4SfYGf0vdsbWP+BfsfI4KR0qsnMvGTeBLGSlz3w8ktLa+sruXCxubW9s7xd29po0SI7AhIhWZVgAWldTYIEkKW7FBCAOFt8GwNvVv79FYGekbGsXohzDQsi8FUCpd17pet1hy24Gvki8GSmxGerd4menF4kRE1C\ngbVtz43JH4MhKROCp3EYgxiCANsp1RDiNYfZ1Un/CixQBGP0XCpeCbiz4sxhNaOwiBNhkB3dt7rR5rsf+ZU/MtrJ9Q/98dSxwmhFtMWJBVmLawMh0IeU8aJILpW8il5gIMEKGRHIRIxSRdrJCO5c1Ps0iaJ2WvUj69qpSqF7PZ8uyAHbJj5rEzVmW\nXrM4aTLABe2BP7Nl5dF6cV+ftO5pzZjf7Bec9y97/p3q</latexit>\nxiq\n<latexit sha1_base64=\"ZDWfS4W5wCjdMfUNT7krdrNOck=\">ACFXicdVC7TsNAEDyHVwivACXNiQiJKrJREJQ\nRNJRBIg8psaLzZROnM/mbo2IrPwDHYJ/oUO01PwKFbZxAQlMNZqZlWbHC6UwaNsfVmFhcWl5pbhaWlvf2Nwqb+0TBpDk0eyEB3PGZACgVNFCihE2pgvieh7Y3PU79B9qIQF3hJATXZyMlhoIzTKTWfT8Wt9N+uWJX7Qx0njg5qZAcjX7\n5szcIeOSDQi6ZMV3HDtGNmUbBJUxLvchAyPiYjaCbUMV8MG6ctZ3Sg8gwDGgImgpJMxF+XsTMN2bie0nSZ3htZr1hoND8Z6biX143wuGpGwsVRgiKpy1QSMhaGK5FshHQgdCAyNK3gApFOdMEbSgjPNEjJLRSslYzuw086R1VHVq1ePLWqV\n+ls9WJHtknxwSh5yQOrkgDdIknNyQB/JEnq1H68V6td6+owUrv9klv2C9fwEC25/e</latexit>\nXp\n<latexit sha1_base64=\"B3Quf9Hiy5SE6Dkge+HzD7gqzs=\">ACEnicdVC7SgNBFJ2NrxhfUubwSBYhV2JaBm0sY\nxoHpAsYXZyE4fMzg4zd4UQ8gl2ov9iJ7b+gL9i5e6hSZ6qsM58K5J9BSWHTdD6ewtLyulZcL21sbm3vlHf3WjaKDYcmj2RkOgGzIWCJgqU0NEGWBhIaAfjy9Rv34OxIlK3ONHgh2ykxFBwhol0+nrfrniVt0MdJF4OamQHI1+bM3iHgcgkIu\nmbVdz9XoT5lBwSXMSr3YgmZ8zEbQTahiIVh/mlWd0aPYMoyoBkOFpJkIPy+mLR2EgZJMmR4Z+e9YaTQ/mem4l9eN8bhuT8VSscIiqctUEjIWlhuRDIQ0IEwgMjSt4AKRTkzDBGMoIzRIyTxUrJWN78NIukdVL1atXT61qlfpHPViQH5JAcE4+ckTq\n5Ig3SJyMyAN5Is/Oo/PivDpv39GCk9/sk19w3r8ACKePg=</latexit>\nCi\n<latexit sha1_base64=\"Sw+bqk05SmHaZ5o12fyk3S+/h/E=\">ACEnicdVA9TwJBFHyHX4hfqKXNRmJiRe4MRksijS\nVGQRK4kL3lgRv29i670wI4SfYGf0vdsbWP+BfsfI4KR0qsnMvGTeBLGSlz3w8ktLa+sruXCxubW9s7xd29po0SI7AhIhWZVsAtKqmxQZIUtmKDPAwU3gbD2tS/vUdjZaRvaBSjH/KBln0pOKXSda0ru8WSW3YzsEXizUgJZqh3i5+dXiSEDUJ\nxa1te25M/pgbkLhpNBJLMZcDPkA2ynVPETrj7OqE3aUWE4Ri9EwqVgm4s+LMQ+tHYVBmgw53dl5rx9psv+ZU/Evr51Q/9wfSx0nhFpMW5BUmLWwsh0IGQ9aZCIT9CJjUT3HAiNJxIVIxSRcrpGN589MskuZJ2auUT68qperFbLY8HMAhHIMHZ1C\nFS6hDAwQM4AGe4Nl5dF6cV+ftO5pzZjf78AvO+xfZnp4i</latexit>\nCM\n<latexit sha1_base64=\"dYU0D6NtPg3R3NjgwHeJ2ItpBow=\">ACEnicdVDLSsNAFJ3UV62vqks3g0VwVRKp6LYjR\nuhon1AG8pkeluHTiZh5kYoZ/gTvRf3Ilbf8BfcWUSs9BWz+pwzrlw7vFCKQza9odVWFpeWV0rpc2Nre2d8q7e20TRJpDiwcy0F2PGZBCQsFSuiGpjvSeh4k0bqd+5BGxGoW5yG4PpsrMRIcIaJdNMYXA3KFbtqZ6CLxMlJheRoDsqf/WHAIx8U\ncsmM6Tl2iG7MNAouYVbqRwZCxidsDL2EKuaDceOs6oweRYZhQEPQVEiaifDzIma+MVPfS5I+wzsz740CheY/MxX/8noRjs7dWKgwQlA8bYFCQtbCcC2SgYAOhQZElr4FVCjKmWaIoAVlnCdilCxWSsZy5qdZJO2TqlOrnl7XKvWLfLYiOSCH5Jg45Iz\nUySVpkhbhZEweyBN5th6tF+vVevuOFqz8Zp/8gvX+BarOngY=</latexit>\nXq\n<latexit sha1_base64=\"/Wl+QO+p+fKb+w0keoeVATkLEVs=\">ACEnicdVC7TsNAEDyHVwivACXNiQiJKrIRCMoIGs\nogyENKrGh92YRTzmdzt0aKonwCHYJ/oUO0/AC/QoUdUkACU41mZqXZCWIlLbnuh5NbWFxaXsmvFtbWNza3its7dRslRmBNRCoyzQAsKqmxRpIUNmODEAYKG8HgIvMb92isjPQNDWP0Q+hr2ZMCKJWum527TrHklt0J+DzxpqTEpqh2ip/tbiSEDUJ\nBda2PDcmfwSGpFA4LrQTizGIAfSxlVINIVp/NKk65geJBYp4jIZLxSci/rwYQWjtMAzSZAh0a2e9XqTJ/mdm4l9eK6HemT+SOk4ItchakFQ4aWGFkelAyLvSIBFkbyGXmgswQIRGchAiFZN0sUI6ljc7zTypH5W94/LJ1XGpcj6dLc/2D47ZB47ZRV\n2yaqsxgTrswf2xJ6dR+fFeXevqM5Z3qzy37Bef8ClOePw=</latexit>\nK−1xip\n<latexit sha1_base64=\"N1nOjePq3T8BaE3KbCkBXKuc4U=\">ACG3icdVA9SwNBFNzM8avqKXNYhBsDHcS0TJoI9hEMB\n+QnGFv8xKX7O0tu+/EcORv2In+FzuxtfCvWHk5U2iUw0z82DeBFoKi674czNLywuLedW8qtr6xubha3tuo1iw6HGIxmZsAsSKGghgIlNLUBFgYSGsHgfOw37sBYEalrHGrwQ9ZXoic4w1RqX94kh97ovpMIPeoUim7JzUBniTchRTJBtVP4bHcjHoegkEtmb\nctzNfoJMyi4hFG+HVvQjA9YH1opVSwE6ydZ5xHdjy3DiGowVEiaifDzImGhtcMwSJMhw1s7fUihfY/cyz+5bVi7J36iVA6RlB83AKFhKyF5UakSwHtCgOIbPwWUKEoZ4YhghGUcZ6KcTpdPh3Lm5mltSPSl65dHxVLlbOJrPlyC7ZIwfEIyekQi5IldQIJ5o8\nkCfy7Dw6L86r8/YdnXMmNzvkF5z3L0dFohg=</latexit>\nxip\n<latexit sha1_base64=\"Rom81qI/rQ7PWgLYA0G6G21RiME=\">ACFXicdVA9SwNBFNyLXzF+RS1tFoNgFe5E0TJoYx\nnBfEASwt7mJa7Z21t234nhyH+wE/0vdmJr7V+x8u68QhOdapiZB/PG1JYdN0Pp7CwuLS8Ulwtra1vbG6Vt3eaNowMhwYPZWjaPrMghYIGCpTQ1gZY4Eto+eOL1G/dgbEiVNc40dAL2EiJoeAME6l534+FnvbLFbfqZqDzxMtJheSo98uf3UHIowAUc\nsms7Xiuxl7MDAouYVrqRhY042M2gk5CFQvA9uKs7ZQeRJZhSDUYKiTNRPh5EbPA2kngJ8mA4Y2d9YahQvufmYp/eZ0Ih2e9WCgdISietkAhIWthuRHJRkAHwgAiS98CKhTlzDBEMIyzhMxSkYrJWN5s9PMk+ZR1TunlwdV2rn+WxFskf2ySHxyCmp\nkUtSJw3CyS15IE/k2Xl0XpxX5+07WnDym13yC87FwEun90=</latexit>\nFigure 1. Illustrating the setup of equations (1) and (4).\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n5064",
        "chosen": "This paper introduces a new family of minimal problems for reconstruction from multiple views, focusing on autocalibration. The approach involves constraints on image points, unknown depths, and a partially specified calibration matrix K, with practical solvers developed for minimal autocalibration problems.",
        "rejected": "The research introduces a novel family of minimal problems for autocalibration in multi-view reconstruction, focusing on a new approach that relaxes traditional constraints such as those based on Kruppa’s equations or modulus constraint. By formulating the problem with constraints on image points, unknown depths of 3D points, and partially specified calibration matrices, the study presents a comprehensive taxonomy of minimal autocalibration problems for 2 and 3 views. Practical solvers developed from this framework demonstrate superior accuracy compared to state-of-the-art methods when tested on both synthetic and real datasets. The approach also integrates seamlessly with existing tools like COLMAP, showcasing its practical utility and robustness."
    },
    {
        "prompt": "summary: In this work, we tackle the problem of domain gener-\nalization for object detection, specifically focusing on the\nscenario where only a single source domain is available.\nWe propose an effective approach that involves two key\nsteps: diversifying the source domain and aligning detec-\ntions based on class prediction confidence and localiza-\ntion. Firstly, we demonstrate that by carefully selecting a\nset of augmentations, a base detector can outperform ex-\nisting methods for single domain generalization by a good\nmargin. This highlights the importance of domain diversi-\nfication in improving the performance of object detectors.\nSecondly, we introduce a method to align detections from\nmultiple views, considering both classification and localiza-\ntion outputs. This alignment procedure leads to better gen-\neralized and well-calibrated object detector models, which\nare crucial for accurate decision-making in safety-critical\napplications. Our approach is detector-agnostic and can\nbe seamlessly applied to both single-stage and two-stage\ndetectors.\nTo validate the effectiveness of our proposed\nmethods, we conduct extensive experiments and ablations\non challenging domain-shift scenarios.\nThe results con-\nsistently demonstrate the superiority of our approach com-\npared to existing methods. Our code and models are avail-\nable at: https://github.com/msohaildanish/DivAlign.\n1. Introduction\nIn recent years, we have witnessed remarkable performance\nimprovements in supervised object detection [39, 47, 48,\n53].\nThe success of these methods rely on the assump-\ntion that the training and testing data are sampled from the\nsame distribution. However, in many real-world applica-\ntions, such as autonomous driving, this assumption is of-\nten violated and these object detectors usually show de-\ngraded performance due to a phenomenon called domain-\nshift [3, 5, 49]. Shifts in real-world domains are typically\ncaused by environmental alterations, like varying weather\nand time conditions.\nThese changes manifest in diverse\ncontrasts, brightness levels, and textures among others.\nA prominent line of research that attempts to alleviate the\nimpact of domain-shift is known as unsupervised domain\nadaptation (UDA) [20, 24, 42, 49, 63, 68, 72]. Given the\nlabeled data from the source domain and unlabelled data\nfrom the target domain, the aim of UDA methods is to align\nthe source and target data distributions so that the trained\nmodel can generalize well to the target domain [67]. An\nobvious limitation of UDA methods is that they require pre-\ncollecting data and re-training the model for different target\ndomains. Collecting data, even without annotation, for all\npossible domain shifts and training the model when shift\nhappens is difficult and sometime not possible.\nTo cope with the domain-shift problem, a more real-\nistic albeit challenging problem is domain generalization\n[2, 11, 23, 26, 32, 34, 35, 40, 58].\nThe goal of do-\nmain generalization (DG) is to learn a generalizable model\ntypically from multiple source domains, available during\ntraining, that can perform well on the unseen target do-\nmain. A dominant class of DG methods attempts to learn\ndomain-invariant feature space across multiple source do-\nmains [1, 11, 33, 35].\nThe performance of these meth-\nods is sensitive to the number of available source domains\n[7, 70]. In many realistic scenarios, acquiring labeled data\nfrom multiple source domains is often costly and time-\nconsuming, eventually restricting the potential utilization\nof such methods. For these reasons, generalizing from a\nsingle-source domain is a more practical setting.\nRecent surveys [29, 70] reveal that very little work has\nbeen done on DG for object detection [38, 60], despite the\nfact that object detectors occupy an important position in\nmany safety-critical and security-sensitive applications e.g.,\nautonomous driving. An object detector should be able to\nprovide accurate as well as calibrated detections in different\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n17732",
        "chosen": "This paper proposes a method to improve domain generalization for object detection from a single source domain. The approach involves diversifying the source domain through careful augmentation selection and aligning detections based on class prediction confidence and localization. Experiments show superior performance compared to existing methods.",
        "rejected": "In this work, the authors tackle domain generalization for object detection with a single source domain. They introduce diversification through augmentations and align detections based on class prediction confidence and localization, leading to better-generalized models. The approach is validated on challenging domain-shift scenarios, showing superior performance."
    },
    {
        "prompt": "summary: The existing facial datasets, while having plentiful im-\nages at near frontal views, lack images with extreme\nhead poses, leading to the downgraded performance of\ndeep learning models when dealing with proﬁle or pitched\nfaces. This work aims to address this gap by introducing\na novel dataset named Extreme Pose Face High-Quality\nDataset (EFHQ), which includes a maximum of 450k high-\nquality images of faces at extreme poses.\nTo produce\nsuch a massive dataset, we utilize a novel and meticulous\ndataset processing pipeline to curate two publicly avail-\nable datasets, VFHQ and CelebV-HQ, which contain many\nhigh-resolution face videos captured in various settings.\nOur dataset can complement existing datasets on various\nfacial-related tasks, such as facial synthesis with 2D/3D-\naware GAN, diffusion-based text-to-image face generation,\nand face reenactment.\nSpeciﬁcally, training with EFHQ\nhelps models generalize well across diverse poses, signif-\nicantly improving performance in scenarios involving ex-\ntreme views, conﬁrmed by extensive experiments. Addition-\nally, we utilize EFHQ to deﬁne a challenging cross-view\nface veriﬁcation benchmark, in which the performance of\nSOTA face recognition models drops 5-37% compared to\nfrontal-to-frontal scenarios, aiming to stimulate studies on\nface recognition under severe pose conditions in the wild.\n1. Introduction\nThe human face is a central focus in computer vision, with\nextensive research dedicated to tasks like detection, recog-\nnition, generation, and manipulation [39, 42]. Numerous\nlarge-scale facial datasets[9, 12, 14, 24, 28, 30, 45, 48, 54,\n55] have facilitated the face-related studies.\nAlong with\ndeep learning techniques, many tasks have made a consid-\nerable leap in performance in recent years. For instance,\nface recognition systems can recognize near-frontal faces in\nthe wild with near-perfect accuracy [13, 18]. Facial gen-\nerative models, such as [5, 15, 49], can synthesize realistic\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n22605",
        "chosen": "This paper introduces EFHQ, a large-scale dataset of extreme pose face images. Created by curating existing datasets, EFHQ helps improve model performance on profile and pitched faces. The dataset also defines a challenging cross-view face verification benchmark.",
        "rejected": "This paper introduces EFHQ, a dataset containing 450k high-quality images of faces at extreme poses, curated from existing datasets. Training with EFHQ improves model generalizability across diverse poses, especially in extreme view scenarios. Additionally, a cross-view face verification benchmark using EFHQ reveals significant drops in SOTA models' performance compared to frontal-face scenarios."
    },
    {
        "prompt": "summary: Modeling and visualizing relationships between tasks\nor datasets is an important step towards solving various\nmeta-tasks such as dataset discovery, multi-tasking, and\ntransfer learning. However, many relationships, such as\ncontainment and transferability, are naturally asymmetric\nand current approaches for representation and visualiza-\ntion (e.g., t-SNE [44]) do not readily support this.\nWe\npropose TASK2BOX, an approach to represent tasks us-\ning box embeddings—axis-aligned hyperrectangles in low\ndimensional spaces—that can capture asymmetric relation-\nships between them through volumetric overlaps. We show\nthat TASK2BOX accurately predicts unseen hierarchical\nrelationships between nodes in ImageNet and iNaturalist\ndatasets, as well as transferability between tasks in the\nTaskonomy benchmark.\nWe also show that box embed-\ndings estimated from task representations (e.g., CLIP [36],\nTask2Vec [4], or attribute based [15]) can be used to pre-\ndict relationships between unseen tasks more accurately\nthan classifiers trained on the same representations, as well\nas handcrafted asymmetric distances (e.g., KL divergence).\nThis suggests that low-dimensional box embeddings can\neffectively capture these task relationships and have the\nadded advantage of being interpretable. We use the ap-\nproach to visualize relationships among publicly available\nimage classification datasets on popular dataset hosting\nplatform called Hugging Face.\n1. Introduction\nThe success of deep learning has led to the proliferation of\ndatasets for solving a wide range of computer vision prob-\nlems. Yet, there are few tools available to enable practition-\ners to find datasets related to the task at hand, and to solve\nvarious meta-tasks related to it. We present TASK2BOX, a\nmethod to represent tasks using axis-aligned hyperrectan-\ngles (or box embeddings). TASK2BOX is framed as a learn-\nable mapping from dataset representation to boxes, and can\nbe trained to predict various relationships between novel\ntasks such as transferability, hierarchy, and overlap.\nBox embeddings [48] extend order embeddings [46] by\nusing volumetric relationships between axis-aligned hyper-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n28827",
        "chosen": "This paper presents TASK2BOX, an approach using box embeddings to model asymmetric relationships between tasks. The method accurately predicts hierarchical relationships and transferability between tasks, offering an interpretable way to visualize task relationships.",
        "rejected": "TASK2BOX proposes representing tasks as box embeddings to capture asymmetric relationships between them. This method accurately predicts unseen hierarchical and transferability relationships among tasks, outperforming classifiers trained on the same representations. Visualization of image classification datasets on Hugging Face demonstrates its effectiveness."
    },
    {
        "prompt": "summary: Reconstructing dynamic objects from monocular videos\nis a severely underconstrained and challenging problem,\nand recent work has approached it in various directions.\nHowever, owing to the ill-posed nature of this problem,\nthere has been no solution that can provide consistent, high-\nquality novel views from camera positions that are signifi-\ncantly different from the training views. In this work, we\nintroduce Neural Parametric Gaussians (NPGs) to take on\nthis challenge by imposing a two-stage approach: first, we\nfit a low-rank neural deformation model, which then is used\nas regularization for non-rigid reconstruction in the second\nstage. The first stage learns the object’s deformations such\nthat it preserves consistency in novel views. The second\nstage obtains high reconstruction quality by optimizing 3D\nGaussians that are driven by the coarse model. To this end,\nwe introduce a local 3D Gaussian representation, where\ntemporally shared Gaussians are anchored in and deformed\nby local oriented volumes. The resulting combined model\ncan be rendered as radiance fields, resulting in high-quality\nphoto-realistic reconstructions of the non-rigidly deforming\nobjects. We demonstrate that NPGs achieve superior results\ncompared to previous works, especially in challenging sce-\nnarios with few multi-view cues.1\n1Project Page: https://geometric-rl.mpi-inf.mpg.de/npg/\n1. Introduction\nReconstructing 3D objects from 2D observations is a core\nproblem in computer vision with numerous applications in\nseveral industries, such as the movie and game industry,\nAR/VR, and robotics. Tremendous progress has been seen\nin static scene reconstruction during the last few years. The\nreal world is, however, dynamic, and most recorded scenes\nare captured in a casual setting, with sparse coverage from\na single camera. Thus, addressing these two aspects during\nreconstruction is of fundamental importance.\nThe success of neural approaches on static scenes has en-\ncouraged their use for dynamic scene reconstruction from\nmonocular videos, both in its classical [24, 40, 45] and\nhybrid [4, 10, 28] forms.\nThese methods either learn a\nper-frame scene representation with limited time consis-\ntency [10, 24] or utilize a time-invariant canonical space,\nwhich is used to track the observations at each timestep [41,\n45]. However, as pointed out in Gao et al. [12], they are\nevaluated on datasets that contain multi-view signals, such\nas camera teleportation—i.e., alternating samples from dif-\nferent cameras to construct a temporal sequence—and lim-\nited object motion, and their performance suffers drastically\nwhen evaluated on more realistic monocular sequences.\nSuch sequences usually contain faster object motion com-\npared to camera motion. Strong regularization is required in\norder to propagate information between different timesteps\nwith the correct data association. As we will demonstrate\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n10715",
        "chosen": "This paper proposes Neural Parametric Gaussians (NPGs) for reconstructing dynamic objects from monocular videos. The two-stage approach first fits a deformation model, then refines it with 3D Gaussians, achieving high-quality reconstructions of non-rigidly deforming objects.",
        "rejected": "NPGs address dynamic object reconstruction from monocular videos by imposing a two-stage approach: fitting a low-rank neural deformation model followed by optimizing 3D Gaussians. This method achieves high-quality photo-realistic reconstructions, demonstrating superiority in scenarios with few multi-view cues."
    },
    {
        "prompt": "summary: Even the best current algorithms for estimating body\n3D shape and pose yield results that include body self-\nintersections.\nIn this paper, we present CLOAF, which\nexploits the diffeomorphic nature of Ordinary Differential\nEquations to eliminate such self-intersections while still im-\nposing body shape constraints. We show that, unlike earlier\napproaches to addressing this issue, ours completely elim-\ninates the self-intersections without compromising the ac-\ncuracy of the reconstructions. Being differentiable, CLOAF\ncan be used to ﬁne-tune pose and shape estimation base-\nlines to improve their overall performance and eliminate\nself-intersections in their predictions.\nFurthermore, we\ndemonstrate how our CLOAF strategy can be applied to\npractically any motion ﬁeld induced by the user. CLOAF\nalso makes it possible to edit motion to interact with the\nenvironment without worrying about potential collision or\nloss of body-shape prior.\n1. Introduction\nFeed-forward approaches to estimating human body 3D\nshape and pose from a single image have become remark-\nably effective [6, 25, 35]. The very recent transformer-based\narchitecture of [10] embodies the current state-of-the-art.\nIt is pre-trained on 300 million images and ﬁne-tuned on\nmost SMPL data sets in existence. However, as good as\nthese methods have become, they can still produce unreal-\nistic poses with substantial self-intersections of body parts,\nas illustrated by Fig. 1. This is a serious issue if video-based\nmotion capture is to be used in ﬁelds, such as robotics or re-\nalistic animation, where preventing self-intersections is of\nutmost importance.\nMost current approaches to addressing this issue [4, 13,\n24, 28] are iterative. They penalize self-intersections explic-\nitly by minimizing an interpenetration loss. This requires\nexplicitly detecting self-intersections and then performing\n*This work was supported in part by the Swiss National Science Foun-\ndation\nCollision Rate (%)\nHMR2.0\nOptim\nCLOAF\nMethods\n39.2%\n9.2%\n0.0%\nFigure 1.\nSelf-intersections in SOTA methods.\nTop rows.\nHMR2.0 [10] (ﬁrst row) and PARE [19] (second row), two of the\nbest current methods, produce bodies shown in blue with self-\nintersections.\nCLOAF removes them and generates the results\nshown in gold. Bottom row. HMR 2.0 [10] recovers bodies with\nself-intersections in 39.2% of frames of the 3DPW-test set. A\nrecent post-processing method such as [28] brings this down to\n9.2%. CLOAF drops this number all the way to zero.\na separate optimization step, which makes the whole pro-\ncess non-differentiable and precludes its use during train-\ning. Another approach is to eliminate self-intersections in\nthe training databases [26]. While all these methods help,\nthey do not guarantee the absence of self-collisions at infer-\nence time.\nIn this paper, we propose a different approach. It pre-\nvents self-intersections in a differentiable manner and with-\nout an explicit detection step. To this end, we rely on the\nfact that if the scene ﬂow from one body to another is the\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1176",
        "chosen": "This paper presents CLOAF, a method using Ordinary Differential Equations to eliminate self-intersections in human body 3D shape and pose estimation. The differentiable approach improves reconstruction accuracy without explicit detection steps.",
        "rejected": "CLOAF uses differential equations to eliminate body self-intersections while preserving shape constraints during 3D human pose estimation. Unlike iterative methods that minimize interpenetration loss, CLOAF ensures no self-intersections without compromising accuracy, making it suitable for realistic animation and robotics applications."
    },
    {
        "prompt": "summary: We present 3D Paintbrush, a technique for automati-\ncally texturing local semantic regions on meshes via text\ndescriptions. Our method is designed to operate directly\non meshes, producing texture maps which seamlessly inte-\ngrate into standard graphics pipelines. We opt to simultane-\nously produce a localization map (to specify the edit region)\nand a texture map which conforms to it. This approach im-\nproves the quality of both the localization and the styliza-\ntion. To enhance the details and resolution of the textured\narea, we leverage multiple stages of a cascaded diffusion\nmodel to supervise our local editing technique with gen-\nerative priors learned from images at different resolutions.\nOur technique, referred to as Cascaded Score Distillation\n(CSD), simultaneously distills scores at multiple resolutions\nin a cascaded fashion, enabling control over both the gran-\nularity and global understanding of the supervision. We\ndemonstrate the effectiveness of 3D Paintbrush to locally\ntexture different semantic regions on a variety of shapes.\nProject page: https://threedle.github.io/3d-\npaintbrush\n1. Introduction\nThe ability to edit existing high-quality 3D assets is a funda-\nmental capability in 3D modeling workflows. Recent works\nhave shown exceptional results for text-driven 3D data cre-\nation [32, 38, 48, 53, 58, 59], but focus on making global\nedits. While some progress has been made on local edit-\ning using an explicit localization of the edit region [49, 67],\nthese regions are often coarse and lack fine-grained detail.\nHighly-detailed and accurate localizations are important for\nconstraining the edits to be within a specific region, prevent-\ning changes unrelated to the target edit. Furthermore, while\nmeshes with texture maps are the de facto standard in graph-\nics pipelines, existing local editing work does not natively\noperate on meshes nor produce texture maps for them.\nIn this work we develop 3D Paintbrush, a method for\nautomatically texturing local semantic regions on meshes\nvia text descriptions. Our method is designed to operate\ndirectly on meshes, producing texture maps which seam-\nlessly integrate into standard graphics pipelines. 3D Paint-\nbrush is controlled via intuitive, free-form text input, allow-\ning users to describe their edits using open vocabulary on\na wide range of meshes. Specifically, given an input mesh\nand a text prompt, 3D Paintbrush produces the correspond-\ning high-quality texture map and a localization region to\nconfine it. To enhance the details and resolution of the lo-\ncally textured area, we introduce Cascaded Score Distilla-\ntion (CSD) which leverages multiple stages of a cascaded\ndiffusion model.\nOur explicit localization masks can be\nused to layer our edit texture onto existing textures.\nWe opt to represent both our localization map and tex-\nture map as neural fields encoded by multi-layer percep-\ntions. Our method synthesizes both a fine-grained localiza-\ntion mask and high-quality texture in tandem. Simultane-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n4473",
        "chosen": "This paper introduces 3D Paintbrush, a technique for texturing local semantic regions on meshes via text descriptions. Using cascaded diffusion models, the method produces high-quality texture maps that integrate into standard graphics pipelines.",
        "rejected": "3D Paintbrush presents a technique for texturing local semantic regions on meshes via text descriptions. By leveraging cascaded diffusion models, it simultaneously produces localization and texture maps, enhancing detail and resolution. The method allows for intuitive control over edits, seamlessly integrating into standard graphics pipelines."
    },
    {
        "prompt": "summary: When building classification systems with demographic\nfairness considerations, there are two objectives to satisfy:\n1) maximizing utility for the specific task and 2) ensuring\nfairness w.r.t. a known demographic attribute. These ob-\njectives often compete, so optimizing both can lead to a\ntrade-off between utility and fairness. While existing works\nacknowledge the trade-offs and study their limits, two ques-\ntions remain unanswered: 1) What are the optimal trade-\noffs between utility and fairness? and 2) How can we nu-\nmerically quantify these trade-offs from data for a desired\nprediction task and demographic attribute of interest? This\npaper addresses these questions. We introduce two utility-\nfairness trade-offs: the Data-Space and Label-Space Trade-\noff. The trade-offs reveal three regions within the utility-\nfairness plane, delineating what is fully and partially pos-\nsible and impossible. We propose U-FaTE, a method to nu-\nmerically quantify the trade-offs for a given prediction task\nand group fairness definition from data samples. Based on\nthe trade-offs, we introduce a new scheme for evaluating\nrepresentations. An extensive evaluation of fair representa-\ntion learning methods and representations from over 1000\npre-trained models revealed that most current approaches\nare far from the estimated and achievable fairness-utility\ntrade-offs across multiple datasets and prediction tasks.\n1. Introduction\nAs learning-based systems are increasingly being deployed\nin high-stakes applications, there is a dire need to ensure\nthat they do not propagate or amplify any discriminative\ntendencies inherent in the training datasets. An ideal so-\nlution would impart fairness to prediction models while re-\ntaining the performance of the same model when learned\nwithout fairness considerations.\nRealizing this goal necessitates optimizing two objec-\ntives: maximizing utility in predicting a label Y for a tar-\nget task (e.g., face identity) while minimizing the unfair-\nness w.r.t. a demographic attribute S (e.g., age or gender).\nHowever, when the statistical dependence between Y and S\nPossible\nImpossible\nPossible with Extra Data\nDST\nLST\nUtility (Y)\nUnfairness (S)\n(a)\n0\n10\n20\n30\nEOD (%)\n30\n40\n50\n60\n70\n80\n90\n100\nAccuracy (%)\nDST\nLST\nZero-Shot\nSupervised\n(b)\nFigure 1. The utility-fairness trade-offs. (a) Classification sys-\ntems can be evaluated by their utility (e.g., accuracy) w.r.t. a target\nlabel Y and their unfairness w.r.t. a demographic label S. We\nintroduce two trade-offs, Data Space Trade-Off (DST) and Label\nSpace Trade-Off (LST). (b) We empirically estimate DST and LST\non CelebA and evaluate the utility (high cheekbones) and fairness\n(gender & age) of over 100 zero-shot and 900 supervised models.\nis not negligible, learning with fairness considerations will\nnecessarily degrade the performance of the target predictor,\ni.e., a trade-off will exist between utility and fairness.\nThe existence of a utility-fairness trade-off has been well\nestablished, theoretically [11, 22, 29, 36, 37] and empir-\nically [29], in multiple prior works. However, the focus\nof this body of work has been limited in multiple respects.\nFirst, prior work [29, 36] focused on just one type of trade-\noff, ignoring other possible trade-offs between utility and\nfairness. Second, prior work [36, 37] focused on establish-\ning bounds or identifying the end-points of the trade-off of\ninterest rather than attempting its precise characterization.\nThird, the majority of the prior work [11, 29, 36, 37] has\ninvestigated the utility-fairness trade-offs for one definition\nof group fairness, namely, demographic parity (DP). There\nare multiple fairness definitions [2], including those more\npractically relevant than DP, such as Equalized Opportunity\n(EO), for which the trade-offs have not been studied.\nDespite these attempts, several questions related to the\nutility-fairness trade-offs remain outstanding.\n1. What are the optimal utility-fairness trade-offs?\n2. For a given prediction task and a demographic at-\ntribute, we wish to be fair w.r.t., how can we empiri-\ncally estimate the trade-offs from data?\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n12037",
        "chosen": "This paper addresses the utility-fairness trade-offs in classification systems with demographic fairness considerations. The authors introduce methods to quantify these trade-offs and evaluate fair representation learning methods.",
        "rejected": "The paper discusses utility-fairness trade-offs in classification systems considering demographic fairness. It introduces Data-Space and Label-Space Trade-offs, delineating what is possible, partially possible, and impossible. U-FaTE quantifies these trade-offs for given tasks and demographic attributes, revealing that most current approaches fall short of achievable fairness-utility balances."
    },
    {
        "prompt": "summary: Estimating large, extreme inter-image rotations is crit-\nical for numerous computer vision domains involving im-\nages related by limited or non-overlapping fields of view.\nIn this work, we propose an attention-based approach with\na pipeline of novel algorithmic components. First, as ro-\ntation estimation pertains to image pairs, we introduce an\ninter-image distillation scheme using Decoders to improve\nembeddings. Second, whereas contemporary methods com-\npute a 4D correlation volume (4DCV) encoding inter-image\nrelationships, we propose an Encoder-based cross-attention\napproach between activation maps to compute an enhanced\nequivalent of the 4DCV. Finally, we present a cascaded\nDecoder-based technique for alternately refining the cross-\nattention and the rotation query. Our approach outperforms\ncurrent state-of-the-art methods on extreme rotation estima-\ntion. We make our code publicly available1.\n1. Introduction\nEstimating the relative pose between a pair of images is a\ncrucial task in computer vision, which is used in various ap-\nplications such as indoor navigation, augmented reality, au-\ntonomous driving, 3D reconstruction [40, 44], camera local-\nization [5, 45, 47], simultaneous localization and mapping\n[12, 38], and novel view synthesis [35, 42]. The current\napproach to image registration involves extracting features,\nmatching them, and establishing correspondence between\nthem. However, this approach is ineffective for input pairs\nwith little or no overlap, making it difficult to establish suf-\nficient feature correspondences for matching, such as in the\nimages shown in Fig. 1.\nNumerous applications [1, 32, 49] necessitate precise es-\ntimation of inter-image rotations. The prevalent approach\nfor extreme 3D rotation estimation between images with\nlimited or no overlap, as in Fig.\n1, relates to the semi-\nnal work of Coughlan and Yuille [10]. They introduced a\ntechnique premised on linear structures within an image,\nprimarily arising from three mutually orthogonal directions\nFigure 1. The estimation of extreme 3D image rotations. First row:\nImages pair with a small overlap. Second row: non-overlapping\nimage pairs. The proposed scheme estimates the relative rotation\nbetween image pairs.\n- one vertical (building walls) and two horizontal (ground\npavements, roads, etc.). Similarly, ”Single View Metrol-\nogy” by Criminisi et al. [11] and extensions [26, 41, 61] uti-\nlize parallel image lines and corresponding vanishing points\n[19] for camera calibration. Furthermore, relative camera\nrotation can be estimated via illumination cues [2], by ana-\nlyzing lighting and shadow directions.\nIn this work, we propose a deep-learning approach for\nestimating significant, extreme inter-image rotations. Un-\nlike classical formulations [10, 11] that explicitly detect\nhand-crafted cues such as lines, shadows, and vanishing\npoints, our method directly regresses the relative rotation\nfrom input images through a deep neural network. Inspired\nby recent successful applications of Transformers [53] in\ncomputer vision tasks including object detection [8] and im-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n2588",
        "chosen": "This paper proposes an attention-based approach with novel components for estimating extreme 3D image rotations. The method uses inter-image distillation and cross-attention to outperform state-of-the-art rotation estimation techniques.",
        "rejected": "An attention-based approach for estimating large inter-image rotations is proposed, involving inter-image distillation and Encoder-based cross-attention to enhance 4D correlation volume computation. A cascaded Decoder-based refinement technique alternately refines cross-attention and rotation query, outperforming state-of-the-art methods on extreme rotation estimation."
    },
    {
        "prompt": "summary: Existing 3D scene understanding methods are heavily fo-\ncused on 3D semantic and instance segmentation. How-\never, identifying objects and their parts only constitutes an\nintermediate step towards a more ﬁne-grained goal, which\nis effectively interacting with the functional interactive ele-\nments (e.g., handles, knobs, buttons) in the scene to accom-\nplish diverse tasks. To this end, we introduce SceneFun3D,\na large-scale dataset with more than 14.8k highly accu-\nrate interaction annotations for 710 high-resolution real-\nworld 3D indoor scenes. We accompany the annotations\nwith motion parameter information, describing how to in-\nteract with these elements, and a diverse set of natural lan-\nguage descriptions of tasks that involve manipulating them\nin the scene context. To showcase the value of our dataset,\nwe introduce three novel tasks, namely functionality seg-\nmentation, task-driven affordance grounding and 3D mo-\ntion estimation, and adapt existing state-of-the-art methods\nto tackle them. Our experiments show that solving these\ntasks in real 3D scenes remains challenging despite recent\nprogress in closed-set and open-set 3D scene understanding\nmethods.\n1. Introduction\nDatasets of 3D indoor environments have been extensively\nused for computer vision, robotics, embodied AI and mixed\nreality [5, 7, 11]. To perceive 3D environments, 3D instance\nsegmentation has served as a fundamental task to provide\nobject-level knowledge to agents enabling scene interaction.\nGoing a step further, a line of work [46, 49] has studied the\ntask of part-object segmentation focusing on the lower-level\nobject parts, e.g., the drawers of a cabinet. However, these\ntwo tasks serve only as a proxy since in the real-world set-\nting, agents need to successfully detect and interact with the\nfunctional interactive elements (e.g., knobs, handles, but-\ntons) of the objects in the scene. Detecting these elements\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n14531",
        "chosen": "This paper introduces SceneFun3D, a dataset with detailed interaction annotations for 3D indoor scenes. The dataset supports tasks like functionality segmentation and 3D motion estimation, highlighting challenges in real-world scene understanding.",
        "rejected": "SceneFun3D introduces a dataset with interaction annotations for real-world 3D indoor scenes, including motion parameters and natural language task descriptions. Novel tasks such as functionality segmentation and task-driven affordance grounding are introduced, showcasing the complexity of interacting with functional elements in 3D environments despite recent progress in scene understanding."
    },
    {
        "prompt": "summary: Recently, building on the foundation of neural radiance\nﬁeld, various techniques have emerged to learn unsigned\ndistance ﬁelds (UDF) to reconstruct 3D non-watertight\nmodels from multi-view images. Yet, a central challenge\nin UDF-based volume rendering is formulating a proper\nway to convert unsigned distance values into volume den-\nsity, ensuring that the resulting weight function remains un-\nbiased and sensitive to occlusions. Falling short on these\nrequirements often results in incorrect topology or large\nreconstruction errors in resulting models. This paper ad-\ndresses this challenge by presenting a novel two-stage al-\ngorithm, 2S-UDF, for learning a high-quality UDF from\nmulti-view images. Initially, the method applies an easily\ntrainable density function that, while slightly biased and\ntransparent, aids in coarse reconstruction. The subsequent\nstage then reﬁnes the geometry and appearance of the ob-\nject to achieve a high-quality reconstruction by directly ad-\njusting the weight function used in volume rendering to en-\nsure that it is unbiased and occlusion-aware. Decoupling\ndensity and weight in two stages makes our training sta-\nble and robust, distinguishing our technique from existing\nUDF learning approaches. Evaluations on the DeepFash-\nion3D, DTU, and BlendedMVS datasets validate the ro-\nbustness and effectiveness of our proposed approach. In\nboth quantitative metrics and visual quality, the results in-\ndicate our superior performance over other UDF learn-\ning techniques in reconstructing 3D non-watertight models\nfrom multi-view images. Our code is available at https:\n//bitbucket.org/jkdeng/2sudf/.\n1. Introduction\nAs the success of neural radiance ﬁeld (NeRF) [29], numer-\nous volume rendering based 3D modeling methods are pro-\n*Corresponding author\nGT\nOurs\nNeuralUDF\nNeUDF\nFigure 1.\nWe learn a UDF from multiview images for non-\nwatertight model reconstruction. As illustrated in the cross sec-\ntions of learned UDFs, our learned UDF approximates to the\nground truth. In contrast, the learned UDF of NeuralUDF [25]\nis choppy leading to signiﬁcant artifacts, e.g., unexpected pit. The\nlearned UDF of NeUDF [23] is almost closed struggling to gener-\nate open surface.\nposed to learn signed distance ﬁelds (SDF) for 3D model re-\nconstruction from multi-view images [7, 34, 36, 40]. These\napproaches map signed distance value to a density function,\nthereby enabling the use of volume rendering to learn an\nimplicit SDF representation. To calculate pixel colors, they\ncompute the weighted sum of radiances along each light\nray. Achieving an accurate surface depiction requires the\ndensity function to meet three essential criteria. Firstly, the\nweights, which are derived from the density function, must\nreach their maximum value when the distance is zero, ensur-\ning unbiasedness. Secondly, as a ray traverses through the\nsurface, the accumulated density should tend towards inﬁn-\nity, rendering the surface opaque — a property referred to as\nocclusion-awareness. Finally, the density function should\nbe bounded to prevent numerical issues. The popular SDF\napproaches, such as NeuS [34] and VolSDF [40], adopt an\nS-shaped density function that meets all these requirements.\nWhile SDF-based methods excel at reconstructing wa-\ntertight models, they have limitations in representing open\nmodels. This is due to the intrinsic nature of SDF, which\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n5084",
        "chosen": "This paper presents 2S-UDF, a two-stage algorithm for learning unsigned distance fields from multi-view images. The method improves reconstruction of non-watertight models by decoupling density and weight functions, ensuring unbiased and occlusion-aware results.",
        "rejected": "Building on neural radiance fields, this work explores unsigned distance fields (UDFs) for reconstructing non-watertight 3D models. Challenges in converting unsigned distances to volume density are addressed to ensure unbiased weight functions sensitive to occlusions, aiming to improve topology correctness and reduce reconstruction errors."
    }
]