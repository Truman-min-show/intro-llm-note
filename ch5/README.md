## 5.1 微调训练的一些概念

### 问题：

1. 平时在使用AIGC时不断用prompt给出要求来使得模型能输出更符合要求的内容是否也算是指令微调？还是说因为没有使用微调数据集去训练就不算是微调，只能叫做"基于prompt提供上下文"？

   * 不属于微调，而是属于情景学习，或者说就是"基于prompt提供临时的上下文"，因为其是暂时的。

2. 许多公司的大模型在提供使用时都会设置一个用户可以为回答进行好评和差评的功能，这些数据被收集后是用来进行指令微调吗？还是用于强化学习？

   * 用于RLHF，赞和踩可以用于对指定的prompt的回答的人类偏好排序，然后用于训练PPO中的奖励模型。

### 思考：

与预训练不同，指令微调所需数据集不用太多，只要少量的富有多样性的高质量微调数据集就可以可观地提升大模型对各类型问题的回答质量。

## 5.2 LoRA微调

### 问题：

1. LoRA与SVD的关系是什么？为何使用一个很小的r作为对ΔW的低维映射是有效的？

   * SVD 给出“最佳的低秩近似”理论（保留大奇异值方向）。
   * LoRA 用 BA 直接参数化并学习这类低秩更新，在训练过程中自动“找到”对应的左右奇异方向与强度。
   * 由于 ΔW 的奇异值往往快速衰减，小的 r 就能逼近全量微调的效果，从而在几乎不掉点的前提下把显存/算力花费降到个位数百分比。

2. 在我的印象中，CV领域的微调任务往往只需要微调最后几层的参数就能获得不错的成效，那么放在transformer架构中，所有层都需要LoRA微调吗？不同层的LoRA是否可以作用在不同的模块上，例如对于低层次的transformer块LoRA可以只作用在attention块中的q和v上，而高层次时可以作用在FFN块上或者兼而有之？

   * 类比 CV 的“只调最后几层”：在 CV 任务里，底层卷积往往学到的是通用特征（边缘、纹理），只有高层与具体任务强相关，所以只调最后几层就够。
   * Transformer 中的 LoRA 更灵活：底层的 Transformer block 学习到的是通用语言结构（词序、基本语法），上层则更和语义、任务关联。因此，并不一定需要在所有层都加 LoRA，通常只在部分层插入即可。
   * 换句话说：LoRA 的插入位置是可调节的超参数，可以按任务和资源灵活选择，不是必须所有层/所有模块都加

3. LoRA微调的原理是假定了增量参数矩阵ΔW是一个低秩矩阵，那么如果该矩阵的秩越低是否就意味着使用LoRA微调的效果就越接近全量微调？

   * ΔW 秩越低 → LoRA 越容易用小 r 学到 → 效果越接近全量微调。但秩本身是任务固有的性质，不是我们能随意改变的。我们能控制的是 LoRA 的秩 r：

     * 如果 r≥rank(ΔW)，就能完美表示；
     * 如果 r<rank(ΔW)，就是近似。

## 5.3 上下文窗口拓展

### tips：

当前的顶级模型在上下文的窗口长度上都表现得十分优异，几乎已经足够完成绝大多数任务了。以Gemini-2.5-Pro为例，其单个prompt输入长度限制在100k token而上下文更是达到了惊人的1M token，可以将一整本《三国演义》丢进去问答。

## 5.4 DeepSpeed-chat SFT

尝试了使用Qwen3-0.6B模型在我之前总结的小批量的科研文献摘要数据集上进行了微调，具体的结果见code中的readme

