## 读文献后相比于看完书和网课的新想法：

* muti-head的机制有点类似于cnn的多通道卷积，设置不同的头来学习不同的k，q，v能在多个表征空间进行attention，提取多种patten信息，提高泛化能力
* attention机制可以相比于CNN和RNN的感受野更大，对长距离的信息依赖可以做到更有效的特征提取但却需要更大量的数据来训练，进而开启了“大模型”时代，大即使是好。当然也会带来计算量的增加，所以之后便有了稀疏transformer之类的改进。
* 看书时还不太明白为什么Scaled Dot-Product Attention要多除一个根号d，实际是因为当维度很大时，点乘之后的结果数据量级很大。对于softmax函数而言，一旦输入很大，会进入函数的饱和区，梯度值很小。因而需要scaled。如果输入q和k各自都是均值为0，方差为1的分布，q与k的点积将是均值为0，方差为dk，为了将其方差控制在1，需要除以dk的平方根。

论文在最后提到了之后可以将transformer运用到除了文本之外的地方，可以说是预言了今天的整个人工智能的格局，transformer架构在无论是LLM还是CV亦或是音频和视频领域都有着相当不错的效果，不可谓不是近十年人工智能领域最伟大的论文呢

